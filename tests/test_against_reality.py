"""
Validation testing for the Devon Air Ambulance Discrete Event Simulation (DES) Model.

These check that the results generated by the model when fed with 'current' parameters are within
acceptable range that sufficiently mirrors real world.

Unlike some other testing, we wouldn't expect perfect matches here due to inherent variability.
Therefore, functions like pytest.approx() will be used for testing values fall within a reasonable
margin of the real-world figures.

Planned tests are listed below with a [].
Implemented tests are listed below with a [x].

## Arrivals

[x] Total number of calls over period
[x] Distribution of number of calls received per day
[] Pattern of calls across the course of a day
[] Pattern of calls across seasons
[] Split of calls across AMPDS cards

## Response Times

[] Distribution of response times

## Utilisation

[] Total utilisation
[] Utilisation split across different resources
[] Average jobs per day per vehicle

## Job durations

[] Average total job durations by vehicle type
[x] Distribution of total job durations by vehicle type
[] Average total job stage durations by vehicle type
[] Distribution of job stage durations by vehicle type


## HEMS result (things like stand down en route, treated by not conveyed) proportions

[] HEMS result proportions reflect reality overall
[] HEMS result proportions by resource reflect reality overall

"""

import numpy as np
from scipy import stats
import pandas as pd
from datetime import datetime
import gc
import os
import textwrap

import pytest
import warnings

from helpers import warn_with_message, fail_with_message, calculate_chi_squared_and_cramers

# Workaround to deal with relative import issues
# https://discuss.streamlit.io/t/importing-modules-in-pages/26853/2
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))

from des_parallel_process import parallelProcessJoblib, collateRunResults, runSim, removeExistingResults

##############################################################################
# Begin tests                                                                #
##############################################################################

##################################
# Calls in period                #
##################################
#-------------------------------------------#
# Average daily calls                       #
#-------------------------------------------#
@pytest.mark.calls
def test_average_daily_calls_in_period(simulation_results):
    try:
        event_df = simulation_results # defined in conftest.py

        arrivals = event_df[event_df["time_type"] == "arrival"].copy()
        # Check we have one row per patient before proceeding
        assert len(arrivals) == len(arrivals.drop_duplicates(['P_ID', 'run_number']))

        arrivals["timestamp_dt"] = pd.to_datetime(arrivals["timestamp_dt"])

        arrivals["month"] = arrivals["timestamp_dt"].dt.strftime('%Y-%m-01')
        monthly_jobs_per_run = arrivals[['P_ID', 'run_number', 'month']].groupby(['run_number','month']).count()

        average_monthly_jobs = monthly_jobs_per_run.groupby('month').mean().round(3).reset_index()

        # Pull in historical data
        historical_monthly_jobs = pd.read_csv("historical_data/historical_monthly_totals_all_calls.csv")

        # Pull out daily number of calls across simulation and reality
        sim_calls = np.array(average_monthly_jobs['P_ID'])  # simulated data
        real_calls = np.array(historical_monthly_jobs['inc_date'])  # real data

        # Welch’s t-test (does not assume equal variances)
        t_stat, p_value = stats.ttest_ind(sim_calls, real_calls, equal_var=False)

        # Mean difference and effect size
        mean_diff = np.mean(sim_calls) - np.mean(real_calls)
        pooled_std = np.sqrt((np.std(sim_calls, ddof=1) ** 2 + np.std(real_calls, ddof=1) ** 2) / 2)
        cohen_d = mean_diff / pooled_std

        # Thresholds
        p_thresh = 0.05
        warn_effect = 0.2
        fail_effect = 0.5

        # Output for debugging

        # Decision logic
        # Will only fail if significance threshold is met and cohen's D is sufficiently large
        if p_value < p_thresh and abs(cohen_d) > fail_effect:
            fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] **Mean monthly calls** significantly different between
                        simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                        Sim mean: {np.mean(sim_calls):.2f}, Real mean: {np.mean(real_calls):.2f}.
                        Mean diff: {mean_diff:.2f}.""")
        # Else will provide appropriate warning
        elif p_value < p_thresh and abs(cohen_d) > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in **mean monthly calls**
                          between simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                          Sim mean: {np.mean(sim_calls):.2f}, Real mean: {np.mean(real_calls):.2f}.
                          Mean diff: {mean_diff:.2f}.""")
        elif abs(cohen_d) > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                          difference in **mean monthly calls** between simulation and reality
                          (p={p_value:.4f}, Cohen's d={cohen_d:.2f}) but did not meet the p-value
                          threshold for significance.
                          Sim mean: {np.mean(sim_calls):.2f}, Real mean: {np.mean(real_calls):.2f}.
                          Mean diff: {mean_diff:.2f}.""")

    finally:
        del event_df, arrivals
        gc.collect()



#-------------------------------------------#
# Distribution of Calls Received per Day    #
#-------------------------------------------#

@pytest.mark.calls
def test_distribution_daily_calls(simulation_results):
    try:
        event_df = simulation_results # defined in conftest.py

        # Read simulation results
        event_df = pd.read_csv("data/run_results.csv")

        arrivals = event_df[event_df["time_type"] == "arrival"].copy()
        # Check we have one row per patient before proceeding
        assert len(arrivals) == len(arrivals.drop_duplicates(['P_ID', 'run_number']))

        arrivals["timestamp_dt"] = pd.to_datetime(arrivals["timestamp_dt"])
        arrivals["date"] = arrivals["timestamp_dt"].dt.strftime('%Y-%m-%d')
        sim_daily_call_counts = arrivals.groupby(['run_number', 'date'])[['P_ID']].count().reset_index().rename(columns={'P_ID':'calls_in_day'})
        historical_daily_calls = pd.read_csv("historical_data/historical_daily_calls_breakdown.csv")

        assert len(sim_daily_call_counts) > 0, "No simulated daily calls found"
        assert len(historical_daily_calls) > 0, "No historical data found"

        statistic, p_value = stats.ks_2samp(sim_daily_call_counts['calls_in_day'],
                                        historical_daily_calls["calls_in_day"])

        # Thresholds
        p_thresh = 0.05
        warn_effect = 0.1 #  A KS statistic of 0.1 implies up to a 10% difference between CDFs — reasonable as a caution threshold.
        fail_effect = 0.2 # A 20% difference netweem CDFs is substantial — reasonable for failure.

        if p_value < p_thresh and statistic > fail_effect:
            fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] Significant and large difference in distribution of
                        **daily calls** between simulation and reality
                        (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
        # Else will provide appropriate warning
        elif p_value < p_thresh and statistic > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in distribution of
                          **daily calls** between simulation and reality
                          (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
        elif statistic > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                          difference in distribution of **daily calls** between simulation
                          and reality (p={p_value:.4f}, KS statistic={statistic:.2f}) but did not
                          meet the p-value threshold for significance.""")

    finally:
        del event_df, arrivals
        gc.collect()

##################################
# Total Job Durations            #
##################################

#------------------------------------------------------------#
# Average Total Job Durations (by vehicle type)              #
#------------------------------------------------------------#
@pytest.mark.jobdurations
def test_average_total_job_durations(simulation_results):
    try:
        event_df = simulation_results # defined in conftest.py

        # Read simulation results
        event_df = pd.read_csv("data/run_results.csv")

        simulated_job_time_df = event_df[event_df['event_type'].isin(['resource_use', 'resource_use_end'])].copy()
        simulated_job_time_df['timestamp_dt'] = pd.to_datetime(simulated_job_time_df['timestamp_dt'])
        simulated_job_time_df = simulated_job_time_df[['P_ID', 'run_number', 'event_type', 'timestamp_dt', 'vehicle_type']].pivot(index=["P_ID", "run_number", "vehicle_type"], columns="event_type", values="timestamp_dt").reset_index()

        assert simulated_job_time_df['resource_use'].notna().all(), "Missing 'resource_use' times."
        assert simulated_job_time_df['resource_use_end'].notna().all(), "Missing 'resource_use_end' times."

        simulated_job_time_df['resource_use_duration'] = simulated_job_time_df['resource_use_end'] - simulated_job_time_df['resource_use']
        simulated_job_time_df['resource_use_duration_minutes'] = (simulated_job_time_df['resource_use_duration'].dt.total_seconds()) / 60

        historical_time_df = pd.read_csv("historical_data/historical_job_durations_breakdown.csv")

        for vehicle in ["helicopter", "car"]:
            # Pull out daily number of calls across simulation and reality
            sim_durations = np.array(simulated_job_time_df[simulated_job_time_df["vehicle_type"]==vehicle]['resource_use_duration_minutes'])  # simulated data
            real_durations = np.array(historical_time_df[historical_time_df["vehicle_type"]==vehicle]['value'])  # real data

            assert len(sim_durations) > 10, f"Too few simulated jobs for {vehicle} to perform a meaningful test."
            assert len(real_durations) > 10, f"Too few real jobs for {vehicle} to perform a meaningful test."

            # Welch’s t-test (does not assume equal variances)
            t_stat, p_value = stats.ttest_ind(sim_durations, real_durations, equal_var=False)

            # Mean difference and effect size
            sim_mean = np.mean(sim_durations)
            real_mean = np.mean(real_durations)
            mean_diff = sim_mean - real_mean
            pooled_std = np.sqrt((np.std(sim_durations, ddof=1) ** 2 + np.std(real_durations, ddof=1) ** 2) / 2)
            cohen_d = mean_diff / pooled_std


            # Thresholds
            p_thresh = 0.05
            warn_effect = 0.2
            fail_effect = 0.5

            # Output for debugging

            # Decision logic
            # Will only fail if significance threshold is met and cohen's D is sufficiently large
            if p_value < p_thresh and abs(cohen_d) > fail_effect:
                fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] **Average total job durations** for {vehicle}s significantly different between
                            simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                            Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                            Mean diff: {mean_diff:.2f}.""")
            # Else will provide appropriate warning
            elif p_value < p_thresh and abs(cohen_d) > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in **Average total job durations** for {vehicle}s
                            between simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                            Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                            Mean diff: {mean_diff:.2f}.""")
            elif abs(cohen_d) > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                            difference in **Average total job durations** for {vehicle}s between simulation and reality
                            (p={p_value:.4f}, Cohen's d={cohen_d:.2f}) but did not meet the p-value
                            threshold for significance.
                            Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                            Mean diff: {mean_diff:.2f}.""")

    finally:
        del event_df, simulated_job_time_df, historical_time_df
        gc.collect()


#------------------------------------------------------------#
# Distribution of Total Job Durations (by vehicle type)      #
#------------------------------------------------------------#
@pytest.mark.jobdurations
def test_distribution_total_job_durations(simulation_results):
    try:
        event_df = simulation_results # defined in conftest.py

        # Read simulation results
        event_df = pd.read_csv("data/run_results.csv")

        simulated_job_time_df = event_df[event_df['event_type'].isin(['resource_use', 'resource_use_end'])].copy()
        simulated_job_time_df['timestamp_dt'] = pd.to_datetime(simulated_job_time_df['timestamp_dt'])
        simulated_job_time_df = simulated_job_time_df[['P_ID', 'run_number', 'event_type', 'timestamp_dt', 'vehicle_type']].pivot(index=["P_ID", "run_number", "vehicle_type"], columns="event_type", values="timestamp_dt").reset_index()

        assert simulated_job_time_df['resource_use'].notna().all(), "Missing 'resource_use' times."
        assert simulated_job_time_df['resource_use_end'].notna().all(), "Missing 'resource_use_end' times."

        simulated_job_time_df['resource_use_duration'] = simulated_job_time_df['resource_use_end'] - simulated_job_time_df['resource_use']
        simulated_job_time_df['resource_use_duration_minutes'] = (simulated_job_time_df['resource_use_duration'].dt.total_seconds()) / 60

        historical_time_df = pd.read_csv("historical_data/historical_job_durations_breakdown.csv")

        # Thresholds
        p_thresh = 0.05
        warn_effect = 0.1 #  A KS statistic of 0.1 implies up to a 10% difference between CDFs — reasonable as a caution threshold.
        fail_effect = 0.2 # A 20% difference netweem CDFs is substantial — reasonable for failure.

        ##########################
        # CHECK HELO DISTRIBUTION
        ##########################
        historical_time_df_helos_only = historical_time_df[historical_time_df["vehicle_type"] == "helicopter"]
        simulated_job_time_df_helos_only = simulated_job_time_df[simulated_job_time_df["vehicle_type"] == "helicopter"]

        statistic_helo, p_value_helo = stats.ks_2samp(
            historical_time_df_helos_only['value'],
            simulated_job_time_df_helos_only['resource_use_duration_minutes']
        )

        def check_output(what, p_value, statistic, p_thresh=p_thresh, fail_effect=fail_effect, warn_effect=warn_effect):
            if p_value < p_thresh and statistic > fail_effect:
                fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] Significant and large difference in distribution of
                        {what} between simulation and reality
                        (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
            # Else will provide appropriate warning
            elif p_value < p_thresh and statistic > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in distribution of
                             {what} between simulation and reality
                            (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
            elif statistic > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                            difference in distribution of {what} between simulation
                            and reality (p={p_value:.4f}, KS statistic={statistic:.2f}) but did not
                            meet the p-value threshold for significance.""")

        check_output(
            what="**HELO TOTAL JOB DURATION DISTRIBUTION**",
            p_value=p_value_helo,
            statistic=statistic_helo
        )

        ##########################
        # CHECK CAR DISTRIBUTION
        ##########################
        historical_time_df_cars_only = historical_time_df[historical_time_df["vehicle_type"] == "car"]
        simulated_job_time_df_cars_only = simulated_job_time_df[simulated_job_time_df["vehicle_type"] == "car"]

        statistic_car, p_value_car = stats.ks_2samp(
            historical_time_df_cars_only['value'],
            simulated_job_time_df_cars_only['resource_use_duration_minutes']
        )

        check_output(
            what="**CAR TOTAL JOB DURATION DISTRIBUTION**",
            p_value=p_value_car,
            statistic=statistic_car
        )

    finally:
        del event_df, simulated_job_time_df, historical_time_df
        gc.collect()



##############################################################
# Split between callsign groups                              #
##############################################################

@pytest.mark.callsigngroup
def test_proportions_callsigngroup_allocations(simulation_results):
    # Calculate proportion of jobs allocated to each callsign group in the simulation
    callsign_group_counts_simulated = simulation_results[simulation_results["event_type"]=="resource_use"]["callsign_group"].value_counts().reset_index(name="count_simulated")
    # callsign_group_counts["proportion_simulated"] = callsign_group_counts["count_simulated"].apply(lambda x: x/callsign_group_counts["count_simulated"].sum())
    callsign_group_counts_simulated["callsign_group"] = callsign_group_counts_simulated["callsign_group"].astype('int')

    # Read in the proportion of jobs allocated to each callsign group in historical data
    callsign_group_counts_historic = pd.read_csv("historical_data/historical_monthly_totals_by_callsign.csv").drop(columns="month").sum().reset_index(name="count_historic")
    callsign_group_counts_historic["callsign_group"] = callsign_group_counts_historic["index"].str.extract("(\d+)")
    callsign_group_counts_historic = callsign_group_counts_historic.drop(columns='index').groupby('callsign_group').sum().reset_index()
    callsign_group_counts_historic["callsign_group"] = callsign_group_counts_historic["callsign_group"].astype('int')

    callsign_group_counts = callsign_group_counts_simulated.merge(callsign_group_counts_historic, on="callsign_group")

    # Calculate
    calculate_chi_squared_and_cramers(callsign_group_counts, what="callsign group")


##############################################################
# Split between callsigns                                    #
##############################################################

@pytest.mark.callsign
def test_proportions_callsign_allocations(simulation_results):
    # Calculate proportion of jobs allocated to each callsign in the simulation
    callsign_counts_simulated = simulation_results[simulation_results["event_type"]=="resource_use"]["callsign"].value_counts().reset_index(name="count_simulated")
    # callsign_counts["proportion_simulated"] = callsign_counts["count_simulated"].apply(lambda x: x/callsign_counts["count_simulated"].sum())

    # Read in the proportion of jobs allocated to each callsign group in historical data
    callsign_counts_historic = (
        pd.read_csv("historical_data/historical_monthly_totals_by_callsign.csv")
        .drop(columns="month")
        .sum()
        .reset_index(name="count_historic")
        )
    callsign_counts_historic.rename(columns={'index':'callsign'}, inplace=True)

    callsign_counts = callsign_counts_simulated.merge(callsign_counts_historic, on="callsign")

    # Calculate
    calculate_chi_squared_and_cramers(callsign_counts, what="callsign")
