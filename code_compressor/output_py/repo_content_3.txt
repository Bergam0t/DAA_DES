****************************************
tests\test_against_reality.py
****************************************
"""
Validation testing for the Devon Air Ambulance Discrete Event Simulation (DES) Model.

These check that the results generated by the model when fed with 'current' parameters are within
acceptable range that sufficiently mirrors real world.

Unlike some other testing, we wouldn't expect perfect matches here due to inherent variability.
Therefore, functions like pytest.approx() will be used for testing values fall within a reasonable
margin of the real-world figures.

Planned tests are listed below with a [].
Implemented tests are listed below with a [x].

## Arrivals

[x] Total number of calls over period
[x] Distribution of number of calls received per day
[] Pattern of calls across the course of a day
[] Pattern of calls across seasons
[] Split of calls across AMPDS cards

## Utilisation

[] Total utilisation
[] Average jobs per day per vehicle
[x] Allocation across callsigns
[x] Allocation across callsign groups
[x] Allocation within callsign group

## Job durations

[x] Average total job durations by vehicle type
[x] Distribution of total job durations by vehicle type
[] Average total job stage durations by vehicle type
[] Distribution of job stage durations by vehicle type


## HEMS result (things like stand down en route, treated by not conveyed) proportions

[] HEMS result proportions reflect reality overall
[] HEMS result proportions by resource reflect reality overall

"""

import numpy as np
from scipy import stats
import pandas as pd
from datetime import datetime
import gc
import os
import textwrap

import pytest
from _pytest.outcomes import Failed  # Needed to catch pytest.fail
import warnings

from helpers import (
    warn_with_message,
    fail_with_message,
    calculate_chi_squared_and_cramers,
    format_sigfigs,
)

# Workaround to deal with relative import issues
# https://discuss.streamlit.io/t/importing-modules-in-pages/26853/2
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).resolve().parent.parent))

from des_parallel_process import (
    parallelProcessJoblib,
    collateRunResults,
    runSim,
    removeExistingResults,
)

##############################################################################
# Begin tests                                                                #
##############################################################################


##################################
# Calls in period                #
##################################
# -------------------------------------------#
# Average daily calls                       #
# -------------------------------------------#
@pytest.mark.calls
def test_average_daily_calls_in_period(simulation_results):
    try:
        event_df = simulation_results  # defined in conftest.py

        arrivals = event_df[event_df["time_type"] == "arrival"].copy()
        # Check we have one row per patient before proceeding
        assert len(arrivals) == len(arrivals.drop_duplicates(["P_ID", "run_number"]))

        arrivals["timestamp_dt"] = pd.to_datetime(arrivals["timestamp_dt"])

        arrivals["month"] = arrivals["timestamp_dt"].dt.strftime("%Y-%m-01")
        monthly_jobs_per_run = (
            arrivals[["P_ID", "run_number", "month"]]
            .groupby(["run_number", "month"])
            .count()
        )

        average_monthly_jobs = (
            monthly_jobs_per_run.groupby("month").mean().round(3).reset_index()
        )

        # Pull in historical data
        historical_monthly_jobs = pd.read_csv(
            "historical_data/historical_monthly_totals_all_calls.csv"
        )

        # Pull out daily number of calls across simulation and reality
        sim_calls = np.array(average_monthly_jobs["P_ID"])  # simulated data
        real_calls = np.array(historical_monthly_jobs["inc_date"])  # real data

        # Welch’s t-test (does not assume equal variances)
        t_stat, p_value = stats.ttest_ind(sim_calls, real_calls, equal_var=False)

        # Mean difference and effect size
        mean_diff = np.mean(sim_calls) - np.mean(real_calls)
        pooled_std = np.sqrt(
            (np.std(sim_calls, ddof=1) ** 2 + np.std(real_calls, ddof=1) ** 2) / 2
        )
        cohen_d = mean_diff / pooled_std

        # Thresholds
        p_thresh = 0.05
        warn_effect = 0.2
        fail_effect = 0.5

        # Output for debugging

        # Decision logic
        # Will only fail if significance threshold is met and cohen's D is sufficiently large
        if p_value < p_thresh and abs(cohen_d) > fail_effect:
            fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] **Mean monthly calls** significantly different between
                        simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                        Sim mean: {np.mean(sim_calls):.2f}, Real mean: {np.mean(real_calls):.2f}.
                        Mean diff: {mean_diff:.2f}.""")
        # Else will provide appropriate warning
        elif p_value < p_thresh and abs(cohen_d) > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in **mean monthly calls**
                          between simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                          Sim mean: {np.mean(sim_calls):.2f}, Real mean: {np.mean(real_calls):.2f}.
                          Mean diff: {mean_diff:.2f}.""")
        elif abs(cohen_d) > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                          difference in **mean monthly calls** between simulation and reality
                          (p={p_value:.4f}, Cohen's d={cohen_d:.2f}) but did not meet the p-value
                          threshold for significance.
                          Sim mean: {np.mean(sim_calls):.2f}, Real mean: {np.mean(real_calls):.2f}.
                          Mean diff: {mean_diff:.2f}.""")

    finally:
        del event_df, arrivals
        gc.collect()


# -------------------------------------------#
# Distribution of Calls Received per Day    #
# -------------------------------------------#


@pytest.mark.calls
def test_distribution_daily_calls(simulation_results):
    try:
        event_df = simulation_results  # defined in conftest.py

        arrivals = event_df[event_df["time_type"] == "arrival"].copy()
        # Check we have one row per patient before proceeding
        assert len(arrivals) == len(arrivals.drop_duplicates(["P_ID", "run_number"]))

        arrivals["timestamp_dt"] = pd.to_datetime(arrivals["timestamp_dt"])
        arrivals["date"] = arrivals["timestamp_dt"].dt.strftime("%Y-%m-%d")
        sim_daily_call_counts = (
            arrivals.groupby(["run_number", "date"])[["P_ID"]]
            .count()
            .reset_index()
            .rename(columns={"P_ID": "calls_in_day"})
        )
        historical_daily_calls = pd.read_csv(
            "historical_data/historical_daily_calls_breakdown.csv"
        )

        assert len(sim_daily_call_counts) > 0, "No simulated daily calls found"
        assert len(historical_daily_calls) > 0, "No historical data found"

        statistic, p_value = stats.ks_2samp(
            sim_daily_call_counts["calls_in_day"],
            historical_daily_calls["calls_in_day"],
        )

        # Thresholds
        p_thresh = 0.05
        warn_effect = 0.1  #  A KS statistic of 0.1 implies up to a 10% difference between CDFs — reasonable as a caution threshold.
        fail_effect = 0.2  # A 20% difference netweem CDFs is substantial — reasonable for failure.

        if p_value < p_thresh and statistic > fail_effect:
            fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] Significant and large difference in distribution of
                        **daily calls** between simulation and reality
                        (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
        # Else will provide appropriate warning
        elif p_value < p_thresh and statistic > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in distribution of
                          **daily calls** between simulation and reality
                          (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
        elif statistic > warn_effect:
            warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                          difference in distribution of **daily calls** between simulation
                          and reality (p={p_value:.4f}, KS statistic={statistic:.2f}) but did not
                          meet the p-value threshold for significance.""")

    finally:
        del event_df, arrivals
        gc.collect()


##################################
# Total Job Durations            #
##################################


# ------------------------------------------------------------#
# Average Total Job Durations (by vehicle type)              #
# ------------------------------------------------------------#
@pytest.mark.jobdurations
def test_average_total_job_durations(simulation_results):
    try:
        event_df = simulation_results  # defined in conftest.py

        simulated_job_time_df = event_df[
            event_df["event_type"].isin(["resource_use", "resource_use_end"])
        ].copy()
        simulated_job_time_df["timestamp_dt"] = pd.to_datetime(
            simulated_job_time_df["timestamp_dt"]
        )
        simulated_job_time_df = (
            simulated_job_time_df[
                ["P_ID", "run_number", "event_type", "timestamp_dt", "vehicle_type"]
            ]
            .pivot(
                index=["P_ID", "run_number", "vehicle_type"],
                columns="event_type",
                values="timestamp_dt",
            )
            .reset_index()
        )

        assert simulated_job_time_df["resource_use"].notna().all(), (
            "Missing 'resource_use' times."
        )
        assert simulated_job_time_df["resource_use_end"].notna().all(), (
            "Missing 'resource_use_end' times."
        )

        simulated_job_time_df["resource_use_duration"] = (
            simulated_job_time_df["resource_use_end"]
            - simulated_job_time_df["resource_use"]
        )
        simulated_job_time_df["resource_use_duration_minutes"] = (
            simulated_job_time_df["resource_use_duration"].dt.total_seconds()
        ) / 60

        historical_time_df = pd.read_csv(
            "historical_data/historical_job_durations_breakdown.csv"
        )

        for vehicle in ["helicopter", "car"]:
            # Pull out daily number of calls across simulation and reality
            sim_durations = np.array(
                simulated_job_time_df[simulated_job_time_df["vehicle_type"] == vehicle][
                    "resource_use_duration_minutes"
                ]
            )  # simulated data
            real_durations = np.array(
                historical_time_df[historical_time_df["vehicle_type"] == vehicle][
                    "value"
                ]
            )  # real data

            assert len(sim_durations) > 10, (
                f"Too few simulated jobs for {vehicle} to perform a meaningful test."
            )
            assert len(real_durations) > 10, (
                f"Too few real jobs for {vehicle} to perform a meaningful test."
            )

            # Welch’s t-test (does not assume equal variances)
            t_stat, p_value = stats.ttest_ind(
                sim_durations, real_durations, equal_var=False
            )

            # Mean difference and effect size
            sim_mean = np.mean(sim_durations)
            real_mean = np.mean(real_durations)
            mean_diff = sim_mean - real_mean
            pooled_std = np.sqrt(
                (
                    np.std(sim_durations, ddof=1) ** 2
                    + np.std(real_durations, ddof=1) ** 2
                )
                / 2
            )
            cohen_d = mean_diff / pooled_std

            # Thresholds
            p_thresh = 0.05
            warn_effect = 0.2
            fail_effect = 0.5

            # Output for debugging

            # Decision logic
            # Will only fail if significance threshold is met and cohen's D is sufficiently large
            if p_value < p_thresh and abs(cohen_d) > fail_effect:
                fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] **Average total job durations** for {vehicle}s significantly different between
                            simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                            Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                            Mean diff: {mean_diff:.2f}.""")
            # Else will provide appropriate warning
            elif p_value < p_thresh and abs(cohen_d) > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in **Average total job durations** for {vehicle}s
                            between simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                            Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                            Mean diff: {mean_diff:.2f}.""")
            elif abs(cohen_d) > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                            difference in **Average total job durations** for {vehicle}s between simulation and reality
                            (p={p_value:.4f}, Cohen's d={cohen_d:.2f}) but did not meet the p-value
                            threshold for significance.
                            Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                            Mean diff: {mean_diff:.2f}.""")

    finally:
        del event_df, simulated_job_time_df, historical_time_df
        gc.collect()


# ------------------------------------------------------------#
# Distribution of Total Job Durations (by vehicle type)      #
# ------------------------------------------------------------#
@pytest.mark.jobdurations
def test_distribution_total_job_durations(simulation_results):
    try:
        event_df = simulation_results  # defined in conftest.py

        simulated_job_time_df = event_df[
            event_df["event_type"].isin(["resource_use", "resource_use_end"])
        ].copy()
        simulated_job_time_df["timestamp_dt"] = pd.to_datetime(
            simulated_job_time_df["timestamp_dt"]
        )
        simulated_job_time_df = (
            simulated_job_time_df[
                ["P_ID", "run_number", "event_type", "timestamp_dt", "vehicle_type"]
            ]
            .pivot(
                index=["P_ID", "run_number", "vehicle_type"],
                columns="event_type",
                values="timestamp_dt",
            )
            .reset_index()
        )

        assert simulated_job_time_df["resource_use"].notna().all(), (
            "Missing 'resource_use' times."
        )
        assert simulated_job_time_df["resource_use_end"].notna().all(), (
            "Missing 'resource_use_end' times."
        )

        simulated_job_time_df["resource_use_duration"] = (
            simulated_job_time_df["resource_use_end"]
            - simulated_job_time_df["resource_use"]
        )
        simulated_job_time_df["resource_use_duration_minutes"] = (
            simulated_job_time_df["resource_use_duration"].dt.total_seconds()
        ) / 60

        historical_time_df = pd.read_csv(
            "historical_data/historical_job_durations_breakdown.csv"
        )

        # Thresholds
        p_thresh = 0.05
        warn_effect = 0.1  #  A KS statistic of 0.1 implies up to a 10% difference between CDFs — reasonable as a caution threshold.
        fail_effect = 0.2  # A 20% difference netweem CDFs is substantial — reasonable for failure.

        ##########################
        # CHECK HELO DISTRIBUTION
        ##########################
        historical_time_df_helos_only = historical_time_df[
            historical_time_df["vehicle_type"] == "helicopter"
        ]
        simulated_job_time_df_helos_only = simulated_job_time_df[
            simulated_job_time_df["vehicle_type"] == "helicopter"
        ]

        statistic_helo, p_value_helo = stats.ks_2samp(
            historical_time_df_helos_only["value"],
            simulated_job_time_df_helos_only["resource_use_duration_minutes"],
        )

        def check_output(
            what,
            p_value,
            statistic,
            p_thresh=p_thresh,
            fail_effect=fail_effect,
            warn_effect=warn_effect,
        ):
            if p_value < p_thresh and statistic > fail_effect:
                fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] Significant and large difference in distribution of
                        {what} between simulation and reality
                        (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
            # Else will provide appropriate warning
            elif p_value < p_thresh and statistic > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in distribution of
                             {what} between simulation and reality
                            (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
            elif statistic > warn_effect:
                warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                            difference in distribution of {what} between simulation
                            and reality (p={p_value:.4f}, KS statistic={statistic:.2f}) but did not
                            meet the p-value threshold for significance.""")

        check_output(
            what="**HELO TOTAL JOB DURATION DISTRIBUTION**",
            p_value=p_value_helo,
            statistic=statistic_helo,
        )

        ##########################
        # CHECK CAR DISTRIBUTION
        ##########################
        historical_time_df_cars_only = historical_time_df[
            historical_time_df["vehicle_type"] == "car"
        ]
        simulated_job_time_df_cars_only = simulated_job_time_df[
            simulated_job_time_df["vehicle_type"] == "car"
        ]

        statistic_car, p_value_car = stats.ks_2samp(
            historical_time_df_cars_only["value"],
            simulated_job_time_df_cars_only["resource_use_duration_minutes"],
        )

        check_output(
            what="**CAR TOTAL JOB DURATION DISTRIBUTION**",
            p_value=p_value_car,
            statistic=statistic_car,
        )

    finally:
        del event_df, simulated_job_time_df, historical_time_df
        gc.collect()


##################################
# Job Durations by Job Stage     #
##################################


# ------------------------------------------------------------#
# Average Job Durations by Stage (by vehicle type)           #
# ------------------------------------------------------------#
@pytest.mark.jobdurations
def test_average_per_stage_job_durations(simulation_results):
    try:
        run_results = simulation_results  # defined in conftest.py

        job_times = [
            "time_allocation",
            "time_mobile",
            "time_to_scene",
            "time_on_scene",
            "time_to_hospital",
            "time_to_clear",
        ]

        run_results = run_results[run_results["event_type"].isin(job_times)][
            ["P_ID", "run_number", "time_type", "event_type", "vehicle_type"]
        ]
        run_results["time_type"] = run_results["time_type"].astype("float")

        historical_time_df = pd.read_csv(
            "historical_data/historical_job_durations_breakdown.csv"
        )

        all_results = []
        errors = []
        warnings = []

        for time_type in job_times:
            for vehicle in ["helicopter", "car"]:
                try:
                    # Pull out daily number of calls across simulation and reality
                    sim_durations = np.array(
                        run_results[
                            (run_results["vehicle_type"] == vehicle)
                            & (run_results["event_type"] == time_type)
                        ]["time_type"].astype("float")
                    )  # simulated data

                    real_durations = np.array(
                        historical_time_df[
                            (historical_time_df["vehicle_type"] == vehicle)
                            & (historical_time_df["name"] == time_type)
                        ]["value"]
                    )  # real data

                    assert len(sim_durations) > 10, (
                        f"Too few simulated jobs for {vehicle} to perform a meaningful test."
                    )
                    assert len(real_durations) > 10, (
                        f"Too few real jobs for {vehicle} to perform a meaningful test."
                    )

                    # Welch’s t-test (does not assume equal variances)
                    t_stat, p_value = stats.ttest_ind(
                        sim_durations,
                        real_durations,
                        equal_var=False,
                        nan_policy="omit",
                    )

                    # Mean difference and effect size
                    sim_mean = np.nanmean(sim_durations)
                    real_mean = np.nanmean(real_durations)
                    mean_diff = sim_mean - real_mean
                    pooled_std = np.sqrt(
                        (
                            np.nanstd(sim_durations, ddof=1) ** 2
                            + np.nanstd(real_durations, ddof=1) ** 2
                        )
                        / 2
                    )
                    cohen_d = mean_diff / pooled_std

                    all_results.append(
                        {
                            "time_type": time_type,
                            "vehicle_type": vehicle,
                            "mean_sim": sim_mean,
                            "mean_real": real_mean,
                            "mean_diff": mean_diff,
                            "t_stat": t_stat,
                            "p_value": p_value,
                            "cohen_d": cohen_d,
                        }
                    )

                    # Thresholds
                    p_thresh = 0.05
                    warn_effect = 0.2
                    fail_effect = 0.5

                    # Output for debugging

                    # Decision logic
                    # Will only fail if significance threshold is met and cohen's D is sufficiently large
                    if p_value < p_thresh and abs(cohen_d) > fail_effect:
                        errors.append(
                            f"""[FAIL - COMPARISON WITH REALITY] **Average total job durations** for {vehicle}s {time_type} significantly different between simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}). Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}. Mean diff: {mean_diff:.2f}."""
                        )
                    # Else will provide appropriate warning
                    elif p_value < p_thresh and abs(cohen_d) > warn_effect:
                        warnings.append(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in **Average total job durations** for {vehicle}s {time_type}
                                    between simulation and reality (p={p_value:.4f}, Cohen's d={cohen_d:.2f}).
                                    Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                                    Mean diff: {mean_diff:.2f}.""")
                    elif abs(cohen_d) > warn_effect:
                        warnings.append(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
                                    difference in **Average total job durations** for {vehicle}s {time_type} between simulation and reality
                                    (p={p_value:.4f}, Cohen's d={cohen_d:.2f}) but did not meet the p-value
                                    threshold for significance.
                                    Sim mean: {sim_mean:.2f}, Real mean: {real_mean:.2f}.
                                    Mean diff: {mean_diff:.2f}.""")
                except Failed as e:  # Specifically catch pytest.fail
                    errors.append(
                        f"Group {time_type}:{vehicle} failed with pytest.fail: {str(e)}"
                    )
                except Exception as e:
                    errors.append(
                        f"Group {time_type}:{vehicle} failed with error: {str(e)}"
                    )

        all_results_df = pd.DataFrame(all_results)
        numeric_cols = [
            "mean_sim",
            "mean_real",
            "mean_diff",
            "t_stat",
            "p_value",
            "cohen_d",
        ]
        all_results_df[numeric_cols] = all_results_df[numeric_cols].map(
            format_sigfigs, na_action="ignore"
        )
        all_results_df.to_csv(
            "tests/test_outputs/TEST_OUTPUT_average_per_stage_job_durations.csv"
        )

        if warnings:
            warn_with_message(
                "Some callsign group comparisons had warnings:\n" + "\n".join(warnings)
            )

        if errors:
            pytest.fail("Some callsign group comparisons failed:\n" + "\n".join(errors))

    finally:
        del run_results, historical_time_df
        gc.collect()


# #------------------------------------------------------------#
# # Distribution of Job Durations by stage (by vehicle type)   #
# #------------------------------------------------------------#
# @pytest.mark.jobdurations
# def test_distribution_per_stage_job_durations(simulation_results):
#     try:
#         event_df = simulation_results # defined in conftest.py

#         # Read simulation results
#         event_df = pd.read_csv("data/run_results.csv")

#         simulated_job_time_df = event_df[event_df['event_type'].isin(['resource_use', 'resource_use_end'])].copy()
#         simulated_job_time_df['timestamp_dt'] = pd.to_datetime(simulated_job_time_df['timestamp_dt'])
#         simulated_job_time_df = simulated_job_time_df[['P_ID', 'run_number', 'event_type', 'timestamp_dt', 'vehicle_type']].pivot(index=["P_ID", "run_number", "vehicle_type"], columns="event_type", values="timestamp_dt").reset_index()

#         assert simulated_job_time_df['resource_use'].notna().all(), "Missing 'resource_use' times."
#         assert simulated_job_time_df['resource_use_end'].notna().all(), "Missing 'resource_use_end' times."

#         simulated_job_time_df['resource_use_duration'] = simulated_job_time_df['resource_use_end'] - simulated_job_time_df['resource_use']
#         simulated_job_time_df['resource_use_duration_minutes'] = (simulated_job_time_df['resource_use_duration'].dt.total_seconds()) / 60

#         historical_time_df = pd.read_csv("historical_data/historical_job_durations_breakdown.csv")

#         # Thresholds
#         p_thresh = 0.05
#         warn_effect = 0.1 #  A KS statistic of 0.1 implies up to a 10% difference between CDFs — reasonable as a caution threshold.
#         fail_effect = 0.2 # A 20% difference netweem CDFs is substantial — reasonable for failure.

#         ##########################
#         # CHECK HELO DISTRIBUTION
#         ##########################
#         historical_time_df_helos_only = historical_time_df[historical_time_df["vehicle_type"] == "helicopter"]
#         simulated_job_time_df_helos_only = simulated_job_time_df[simulated_job_time_df["vehicle_type"] == "helicopter"]

#         statistic_helo, p_value_helo = stats.ks_2samp(
#             historical_time_df_helos_only['value'],
#             simulated_job_time_df_helos_only['resource_use_duration_minutes']
#         )

#         def check_output(what, p_value, statistic, p_thresh=p_thresh, fail_effect=fail_effect, warn_effect=warn_effect):
#             if p_value < p_thresh and statistic > fail_effect:
#                 fail_with_message(f"""[FAIL - COMPARISON WITH REALITY] Significant and large difference in distribution of
#                         {what} between simulation and reality
#                         (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
#             # Else will provide appropriate warning
#             elif p_value < p_thresh and statistic > warn_effect:
#                 warn_with_message(f"""[WARN - COMPARISON WITH REALITY] Possible practical difference in distribution of
#                              {what} between simulation and reality
#                             (p={p_value:.4f}, KS statistic={statistic:.2f}).""")
#             elif statistic > warn_effect:
#                 warn_with_message(f"""[WARN - COMPARISON WITH REALITY - NOT STATISTICALLY SIGNIFICANT] Possible practical
#                             difference in distribution of {what} between simulation
#                             and reality (p={p_value:.4f}, KS statistic={statistic:.2f}) but did not
#                             meet the p-value threshold for significance.""")

#         check_output(
#             what="**HELO TOTAL JOB DURATION DISTRIBUTION**",
#             p_value=p_value_helo,
#             statistic=statistic_helo
#         )

#         ##########################
#         # CHECK CAR DISTRIBUTION
#         ##########################
#         historical_time_df_cars_only = historical_time_df[historical_time_df["vehicle_type"] == "car"]
#         simulated_job_time_df_cars_only = simulated_job_time_df[simulated_job_time_df["vehicle_type"] == "car"]

#         statistic_car, p_value_car = stats.ks_2samp(
#             historical_time_df_cars_only['value'],
#             simulated_job_time_df_cars_only['resource_use_duration_minutes']
#         )

#         check_output(
#             what="**CAR TOTAL JOB DURATION DISTRIBUTION**",
#             p_value=p_value_car,
#             statistic=statistic_car
#         )

#     finally:
#         del event_df, simulated_job_time_df, historical_time_df
#         gc.collect()


##############################################################
# Split between callsign groups                              #
##############################################################


@pytest.mark.callsigngroup
def test_proportions_callsigngroup_allocations(simulation_results):
    # Calculate proportion of jobs allocated to each callsign group in the simulation
    callsign_group_counts_simulated = (
        simulation_results[simulation_results["event_type"] == "resource_use"][
            "callsign_group"
        ]
        .value_counts()
        .reset_index(name="count_simulated")
    )
    # callsign_group_counts["proportion_simulated"] = callsign_group_counts["count_simulated"].apply(lambda x: x/callsign_group_counts["count_simulated"].sum())
    callsign_group_counts_simulated["callsign_group"] = callsign_group_counts_simulated[
        "callsign_group"
    ].astype("int")

    # Read in the proportion of jobs allocated to each callsign group in historical data
    callsign_group_counts_historic = (
        pd.read_csv("historical_data/historical_monthly_totals_by_callsign.csv")
        .drop(columns="month")
        .sum()
        .reset_index(name="count_historic")
    )
    callsign_group_counts_historic["callsign_group"] = callsign_group_counts_historic[
        "index"
    ].str.extract("(\d+)")
    callsign_group_counts_historic = (
        callsign_group_counts_historic.drop(columns="index")
        .groupby("callsign_group")
        .sum()
        .reset_index()
    )
    callsign_group_counts_historic["callsign_group"] = callsign_group_counts_historic[
        "callsign_group"
    ].astype("int")

    callsign_group_counts = callsign_group_counts_simulated.merge(
        callsign_group_counts_historic, on="callsign_group"
    )

    # Calculate
    calculate_chi_squared_and_cramers(callsign_group_counts, what="callsign group")


##############################################################
# Split between callsigns                                    #
##############################################################


@pytest.mark.callsign
def test_proportions_callsign_allocations(simulation_results):
    # Calculate proportion of jobs allocated to each callsign in the simulation
    callsign_counts_simulated = (
        simulation_results[simulation_results["event_type"] == "resource_use"][
            "callsign"
        ]
        .value_counts()
        .reset_index(name="count_simulated")
    )
    # callsign_counts["proportion_simulated"] = callsign_counts["count_simulated"].apply(lambda x: x/callsign_counts["count_simulated"].sum())

    # Read in the proportion of jobs allocated to each callsign group in historical data
    callsign_counts_historic = (
        pd.read_csv("historical_data/historical_monthly_totals_by_callsign.csv")
        .drop(columns="month")
        .sum()
        .reset_index(name="count_historic")
    )
    callsign_counts_historic.rename(columns={"index": "callsign"}, inplace=True)

    callsign_counts = callsign_counts_simulated.merge(
        callsign_counts_historic, on="callsign"
    )

    # Calculate
    calculate_chi_squared_and_cramers(callsign_counts, what="callsign")


#######################################################
## Split between callsigns within callsign group      #
#######################################################


@pytest.mark.callsign
def test_proportions_within_callsign_group(simulation_results):
    # Calculate proportion of jobs allocated to each callsign in the simulation
    callsign_counts_simulated = (
        simulation_results[simulation_results["event_type"] == "resource_use"][
            "callsign"
        ]
        .value_counts()
        .reset_index(name="count_simulated")
    )
    # callsign_counts["proportion_simulated"] = callsign_counts["count_simulated"].apply(lambda x: x/callsign_counts["count_simulated"].sum())

    # Read in the proportion of jobs allocated to each callsign group in historical data
    callsign_counts_historic = (
        pd.read_csv("historical_data/historical_monthly_totals_by_callsign.csv")
        .drop(columns="month")
        .sum()
        .reset_index(name="count_historic")
    )
    callsign_counts_historic.rename(columns={"index": "callsign"}, inplace=True)

    callsign_counts = callsign_counts_simulated.merge(
        callsign_counts_historic, on="callsign"
    )

    callsign_counts["callsign_group"] = callsign_counts["callsign"].str.extract(
        r"(\d+)"
    )

    errors = []

    for callsign_group in callsign_counts["callsign_group"].unique():
        try:
            calculate_chi_squared_and_cramers(
                callsign_counts[callsign_counts["callsign_group"] == callsign_group],
                what=f"split_within_callsign_group_{callsign_group}",
            )
        except Failed as e:  # Specifically catch pytest.fail
            errors.append(f"Group {callsign_group} failed with pytest.fail: {str(e)}")
        except Exception as e:
            errors.append(f"Group {callsign_group} failed with error: {str(e)}")

    if errors:
        pytest.fail("Some callsign group comparisons failed:\n" + "\n".join(errors))
****************************************

****************************************
tests\test_reproducibility.py
****************************************
"""Reproducibility testing for the Discrete-Event Simulation (DES) Model.

These check that results are consistent when random seeds are provided but different when seeds
differ or across multiple runs - while still being reproducible with the same master seed.

Many tests inspired by list here:
https://github.com/pythonhealthdatascience/rap_template_python_des/blob/main/tests/test_unittest_model.py

Planned tests are listed below with a [].
Implemented tests are listed below with a [x].

## Multiple Runs

[x] Results dataframe is longer when model conducts more runs
[x] Results differ across multiple runs
[x] Arrivals differ across multiple runs
[x] Running the model sequentially and in parallel produce identical results when seeds set

## Seeds

[x] Model behaves consistently across repeated runs when provided with a seed and no parameters change
[x] Arrivals are identical across simulations when provided with a seed even when other parameters are varied
"""

import pandas as pd
import pytest
from datetime import datetime
import os
import gc

# Workaround to deal with relative import issues
# https://discuss.streamlit.io/t/importing-modules-in-pages/26853/2
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).resolve().parent.parent))

from des_parallel_process import (
    parallelProcessJoblib,
    collateRunResults,
    runSim,
    removeExistingResults,
)


@pytest.mark.reproducibility
def test_results_differ_across_runs_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2  # 2 weeks
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=42,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=13,
        ).reset_index()

        assert not results_df_1.equals(results_df_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results were the same across runs provided with different seeds"
        )

    finally:
        del results_df_1, results_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_results_differ_across_runs_parallelProcessJobLib():
    try:
        removeExistingResults(remove_run_results_csv=True)

        SIM_DURATION = 60 * 24 * 7 * 2  # 2 weeks
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=2,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=42,
        )

        collateRunResults()

        results_df = pd.read_csv("data/run_results.csv")

        results_df_run_1 = results_df[results_df["run_number"] == 1]
        results_df_run_2 = results_df[results_df["run_number"] == 2]
        assert len(results_df_run_1) > 0, "Results df for run 1 is empty"
        assert len(results_df_run_2) > 0, "Results df for run 2 is empty"

        assert not results_df_run_1.equals(results_df_run_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results for run 1 and run 2 in parallel execution are identical"
        )

    finally:
        del results_df, results_df_run_1, results_df_run_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_seed_gives_different_arrival_pattern_runSim():
    """
    When passing different seeds to the runSim method, check arrivals differ
    """
    # try:
    removeExistingResults(remove_run_results_csv=True)

    RUN = 1
    TOTAL_RUNS = 1
    SIM_DURATION = 60 * 24 * 7 * 2  # 2 weeks
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False

    # Should only differ in random seed
    results_df_1 = runSim(
        run=RUN,
        total_runs=TOTAL_RUNS,
        sim_duration=SIM_DURATION,
        warm_up_time=WARM_UP_TIME,
        sim_start_date=SIM_START_DATE,
        amb_data=AMB_DATA,
        random_seed=42,
    ).reset_index()

    arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
        ["P_ID", "timestamp_dt"]
    ]

    results_df_2 = runSim(
        run=RUN,
        total_runs=TOTAL_RUNS,
        sim_duration=SIM_DURATION,
        warm_up_time=WARM_UP_TIME,
        sim_start_date=SIM_START_DATE,
        amb_data=AMB_DATA,
        random_seed=13,
    ).reset_index()

    arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
        ["P_ID", "timestamp_dt"]
    ]

    assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
    assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

    assert not arrivals_df_1.equals(arrivals_df_2), (
        "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are the same when different random seed provided (runSim function)"
    )

    # finally:
    #    del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
    #    gc.collect()


@pytest.mark.reproducibility
def test_different_seed_gives_different_arrival_pattern_parallelProcessJoblib():
    """
    When passing different seeds to the parallelProcessJoblib method, check arrivals differ
    """
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=42,
        )

        collateRunResults()

        results_df_1 = pd.read_csv("data/run_results.csv")

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=13,
        )

        collateRunResults()

        results_df_2 = pd.read_csv("data/run_results.csv")

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ]
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ]
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        assert not arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are the same when different random seed provided (parallelProcessJoblib function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_calls_per_day_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False
        RANDOM_SEED = 42

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ]
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ]

        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        arrivals_df_1["day"] = pd.to_datetime(arrivals_df_1["timestamp_dt"]).dt.date
        arrivals_df_2["day"] = pd.to_datetime(arrivals_df_2["timestamp_dt"]).dt.date

        arrivals_df_1 = arrivals_df_1["day"].value_counts(sort=False)
        arrivals_df_2 = arrivals_df_2["day"].value_counts(sort=False)

        arrivals_df_1.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_calls_per_day_runSim - arrivals_df_1.csv"
        )
        arrivals_df_2.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_calls_per_day_runSim - arrivals_df_2.csv"
        )

        assert arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Number of daily arrivals are not the same when same random seed provided and parameters held constant (runSim function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_calls_per_hour_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False
        RANDOM_SEED = 42

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)

        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        arrivals_df_1["day"] = pd.to_datetime(
            arrivals_df_1["timestamp_dt"]
        ).dt.strftime("%Y-%m-%d %H")
        arrivals_df_2["day"] = pd.to_datetime(
            arrivals_df_2["timestamp_dt"]
        ).dt.strftime("%Y-%m-%d %H")

        arrivals_df_1 = arrivals_df_1["day"].value_counts(sort=False)
        arrivals_df_2 = arrivals_df_2["day"].value_counts(sort=False)

        arrivals_df_1.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_calls_per_hour_runSim - arrivals_df_1.csv"
        )
        arrivals_df_2.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_calls_per_hour_runSim - arrivals_df_2.csv"
        )

        assert arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Number of daily arrivals are not the same when same random seed provided and parameters held constant (runSim function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_arrival_pattern_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False
        RANDOM_SEED = 42

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        assert arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are not the same when same random seed provided and parameters held constant (runSim function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_seed_gives_different_arrival_pattern_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=42,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=13,
        ).reset_index()

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        assert not arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are the same when different random seed provided and parameters held constant (runSim function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_arrival_pattern_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False
    RANDOM_SEED = 42

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=RANDOM_SEED,
        )

        collateRunResults()

        results_df_1 = pd.read_csv("data/run_results.csv")

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=RANDOM_SEED,
        )

        collateRunResults()

        results_df_2 = pd.read_csv("data/run_results.csv")

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        arrivals_df_1.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_arrival_pattern_parallelProcessJoblib - arrivals_df_1.csv"
        )
        arrivals_df_2.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_arrival_pattern_parallelProcessJoblib - arrivals_df_2.csv"
        )

        assert arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are not the same when same random seed provided and parameters held constant (parallelProcessJoblib function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_seed_gives_different_arrival_pattern_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=42,
        )

        collateRunResults()

        results_df_1 = pd.read_csv("data/run_results.csv")

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=13,
        )

        collateRunResults()

        results_df_2 = pd.read_csv("data/run_results.csv")

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        assert not arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are the same when different random seed provided and parameters held constant (parallelProcessJoblib function)"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_arrival_pattern_across_runs_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=2,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=42,
        )

        collateRunResults()

        results_df = pd.read_csv("data/run_results.csv")
        results_df_1 = results_df[results_df["run_number"] == 1]
        results_df_2 = results_df[results_df["run_number"] == 2]

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        arrivals_df_1.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_arrivals_behaviour_parallelProcessJobLib - arrivals_df_1.csv"
        )
        arrivals_df_2.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_arrivals_behaviour_parallelProcessJobLib - arrivals_df_2.csv"
        )

        assert not arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are the same across different runs across different runs with the parallelProcessJoblib function"
        )

    finally:
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_arrival_pattern_VARYING_PARAMETERS_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False
        RANDOM_SEED = 42

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        rota = pd.read_csv("tests/rotas_test/HEMS_ROTA_test_one_helicopter_simple.csv")

        rota.to_csv("actual_data/HEMS_ROTA.csv", index=False)

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        assert arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are not the same when same random seed provided and other aspects varied (runSim function)"
        )

    finally:
        default_rota = pd.read_csv("actual_data/HEMS_ROTA_DEFAULT.csv")
        default_rota.to_csv("actual_data/HEMS_ROTA.csv", index=False)
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_arrival_pattern_VARYING_PARAMETERS_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False
    RANDOM_SEED = 42

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=RANDOM_SEED,
        )

        collateRunResults()

        results_df_1 = pd.read_csv("data/run_results.csv")

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=RANDOM_SEED,
        )

        collateRunResults()

        results_df_2 = pd.read_csv("data/run_results.csv")

        arrivals_df_1 = results_df_1[results_df_1["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        arrivals_df_2 = results_df_2[results_df_2["time_type"] == "arrival"][
            ["P_ID", "timestamp_dt"]
        ].reset_index(drop=True)
        assert len(arrivals_df_1) > 0, "Arrivals df 1 is empty"
        assert len(arrivals_df_2) > 0, "Arrivals df 2 is empty"

        assert arrivals_df_1.equals(arrivals_df_2), (
            "[FAIL - REPRODUCIBILITY - ARRIVALS] Arrivals are not the same when same random seed provided and other aspects varied (parallelProcessJoblib function)"
        )

    finally:
        default_rota = pd.read_csv("actual_data/HEMS_ROTA_DEFAULT.csv")
        default_rota.to_csv("actual_data/HEMS_ROTA.csv", index=False)
        del results_df_1, results_df_2, arrivals_df_1, arrivals_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_results_pattern_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False
        RANDOM_SEED = 42

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=RANDOM_SEED,
        ).reset_index()

        assert len(results_df_1) > 0, "Results df 1 is empty"
        assert len(results_df_2) > 0, "Results df 2 is empty"

        results_df_1.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_results_pattern_runSim - results_df_1.csv"
        )
        results_df_2.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_same_seed_gives_consistent_results_pattern_runSim - results_df_2.csv"
        )

        assert results_df_1.equals(results_df_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results are not the same when same random seed provided (runSim function)"
        )

    finally:
        del results_df_1, results_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_same_seed_gives_consistent_results_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False
    RANDOM_SEED = 42

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=RANDOM_SEED,
        )

        collateRunResults()

        results_df_1 = pd.read_csv("data/run_results.csv")

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=RANDOM_SEED,
        )

        collateRunResults()

        results_df_2 = pd.read_csv("data/run_results.csv")

        assert len(results_df_1) > 0, "Results df 1 is empty"
        assert len(results_df_2) > 0, "Results df 2 is empty"

        assert results_df_1.equals(results_df_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results are not the same when same random seed provided (parallelProcessJoblib function)"
        )

    finally:
        del results_df_1, results_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_seed_gives_different_results_pattern_runSim():
    try:
        removeExistingResults(remove_run_results_csv=True)

        RUN = 1
        TOTAL_RUNS = 1
        SIM_DURATION = 60 * 24 * 7 * 2
        WARM_UP_TIME = 0
        SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
        AMB_DATA = False

        # Should only differ in random seed
        results_df_1 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=42,
        ).reset_index()

        results_df_2 = runSim(
            run=RUN,
            total_runs=TOTAL_RUNS,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            random_seed=13,
        ).reset_index()

        assert len(results_df_1) > 0, "Results df 1 is empty"
        assert len(results_df_2) > 0, "Results df 2 is empty"

        assert not results_df_1.equals(results_df_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results are the same when different random seed provided (runSim function)"
        )

    finally:
        del results_df_1, results_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_seed_gives_different_results_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=42,
        )

        collateRunResults()

        results_df_1 = pd.read_csv("data/run_results.csv")

        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=1,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=13,
        )

        collateRunResults()

        results_df_2 = pd.read_csv("data/run_results.csv")

        assert len(results_df_1) > 0, "Results df 1 is empty"
        assert len(results_df_2) > 0, "Results df 2 is empty"

        assert not results_df_1.equals(results_df_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results are the same when different random seed provided (parallelProcessJoblib function)"
        )

    finally:
        del results_df_1, results_df_2
        gc.collect()


@pytest.mark.reproducibility
def test_different_result_pattern_across_runs_parallelProcessJoblib():
    SIM_DURATION = 60 * 24 * 7 * 2
    WARM_UP_TIME = 0
    SIM_START_DATE = datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S")
    AMB_DATA = False

    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=2,
            sim_duration=SIM_DURATION,
            warm_up_time=WARM_UP_TIME,
            sim_start_date=SIM_START_DATE,
            amb_data=AMB_DATA,
            master_seed=42,
        )

        collateRunResults()

        results_df = pd.read_csv("data/run_results.csv")
        results_df_1 = (
            results_df[results_df["run_number"] == 1]
            .drop(columns="run_number")
            .reset_index(drop=True)
        )
        results_df_2 = (
            results_df[results_df["run_number"] == 2]
            .drop(columns="run_number")
            .reset_index(drop=True)
        )

        assert len(results_df_1) > 0, "Results df 1 is empty"
        assert len(results_df_1) > 0, "Results df 2 is empty"

        results_df_1.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_result_behaviour_parallelProcessJobLib - results_df_1.csv"
        )
        results_df_2.to_csv(
            "tests/test_outputs/TEST_OUTPUT_test_result_behaviour_parallelProcessJobLib - results_df_2.csv"
        )

        assert not results_df_1.equals(results_df_2), (
            "[FAIL - REPRODUCIBILITY - RESULTS] Results are the same across different runs across different runs with the parallelProcessJoblib function"
        )

    finally:
        del results_df_1, results_df_2
        gc.collect()
****************************************

****************************************
tests\test_unittest_model.py
****************************************
"""Unit testing for the Discrete-Event Simulation (DES) Model.

These check specific parts of the simulation and code, ensuring they work
correctly and as expected.

Many tests inspired by list here:
https://github.com/pythonhealthdatascience/rap_template_python_des/blob/main/tests/test_unittest_model.py

Planned tests are listed below with a [].
Implemented tests are listed below with a [x].

## General Checks

[x] Results dataframe is longer when model is run for longer


## Multiple Runs

[x] Results dataframe is longer when model conducts more runs
[x] Results differ across multiple runs
[x] Arrivals differ across multiple runs
[x] Running the model sequentially and in parallel produce identical results when seeds set

## Seeds

[x] Model behaves consistently across repeated runs when provided with a seed and no parameters change
[x] Arrivals are identical across simulations when provided with a seed even when other parameters are varied

## Warm-up period impact

[x] Data is not present in results dataframe for warm-up period
[x] Results dataframe is empty if only a warm-up period is provided

## Arrivals

[] All patients who arrive outside of the warm-up period have an entry in the results dataframe
[x] Number of arrivals increase if parameter adjusted
[x] Number of arrivals decrease if parameter adjusted
[x] No activity generated or model fails to complete run if number of arrivals is 0

## Sensible Resource Use

[] All provided resources get at least some utilisation in a model that
   runs for a sufficient length of time with sufficient demand
[x] The same callsign is never sent on two jobs at once
[x] Resources belonging to the same callsign group don't get sent on jobs at the same time
[] Resources don't leave on service and never return
[] Resource use duration is never negative (i.e. resource use for an individual never ends before it starts)
[] Utilisation never exceeds 100%
[] Utilisation never drops below 0%
[] No one waits in the model for a resource to become availabile - they leave and are recorded as missed
[] Resources are used in the expected order determined within the model

## Activity during inactive periods

[x] Resources do not respond during times they are meant to be off shift
[x] Resources aren't used during their service interval (determined by reg, not callsign)
[] Inactive periods correctly change across seasons if set to do so

"""

import pandas as pd
import pytest
from datetime import datetime
import os
import gc

# Workaround to deal with relative import issues
# https://discuss.streamlit.io/t/importing-modules-in-pages/26853/2
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).resolve().parent.parent))

from des_parallel_process import (
    parallelProcessJoblib,
    collateRunResults,
    runSim,
    removeExistingResults,
)
from helpers import save_logs

##############################################################################
# Begin tests                                                                #
##############################################################################


@pytest.mark.quick
def test_model_runs():
    try:
        removeExistingResults(remove_run_results_csv=True)

        parallelProcessJoblib(
            total_runs=1,
            sim_duration=60 * 24 * 7 * 5,  # Run for five weeks
            warm_up_time=0,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
            print_debug_messages=True,
        )

        collateRunResults()

        save_logs("test_model_runs_5_weeks.txt")

        # Read simulation results
        results_df = pd.read_csv("data/run_results.csv")

        assert len(results_df) > 5, (
            "[FAIL - BASIC FUNCTIONS] Model failed to run for a period of 5 weeks"
        )

    finally:
        del results_df
        gc.collect()


@pytest.mark.medium
def test_model_runs_more():
    try:
        removeExistingResults(remove_run_results_csv=True)

        parallelProcessJoblib(
            total_runs=1,
            sim_duration=60 * 24 * 7 * 52 * 2,  # Run for two years
            warm_up_time=0,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
            print_debug_messages=True,
        )

        collateRunResults()

        save_logs("test_model_runs_2_years.txt")

        # Read simulation results
        results_df = pd.read_csv("data/run_results.csv")

        assert len(results_df) > 5, (
            "[FAIL - BASIC FUNCTIONS] Model failed to run for a period of 2 years"
        )

    finally:
        del results_df
        gc.collect()


def test_more_results_for_longer_run():
    try:
        removeExistingResults(remove_run_results_csv=True)

        for i in range(10):
            results_df_1 = runSim(
                run=1,
                total_runs=1,
                sim_duration=60 * 24 * 7 * 2,  # run for 2 weeks
                warm_up_time=0,
                sim_start_date=datetime.strptime(
                    "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
                ),
                amb_data=False,
            ).reset_index()

            results_df_2 = runSim(
                run=1,
                total_runs=1,
                sim_duration=60 * 24 * 7 * 4,  # run for twice as long - 4 weeks
                warm_up_time=0,
                sim_start_date=datetime.strptime(
                    "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
                ),
                amb_data=False,
            ).reset_index()

            assert len(results_df_1) < len(results_df_2), (
                "[FAIL - BASIC FUNCTIONS] Fewer results seen in longer model run"
            )
    finally:
        del results_df_1, results_df_2
        gc.collect()


def longer_df_when_more_runs_conducted():
    try:
        removeExistingResults(remove_run_results_csv=True)

        parallelProcessJoblib(
            total_runs=1,
            sim_duration=60 * 24 * 7 * 5,
            warm_up_time=0,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
        )

        collateRunResults()

        # Read simulation results
        results = pd.read_csv("data/run_results.csv")

        results_1_run = len(results)

        removeExistingResults(remove_run_results_csv=True)

        parallelProcessJoblib(
            total_runs=2,
            sim_duration=60 * 24 * 7 * 5,
            warm_up_time=0,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
        )

        collateRunResults()

        # Read simulation results
        results = pd.read_csv("data/run_results.csv")

        results_2_runs = len(results)

        assert results_1_run < results_2_runs, (
            "[FAIL - BASIC FUNCTIONS] Fewer results seen with a higher number of runs"
        )

    finally:
        del results
        gc.collect()


def test_arrivals_increase_if_demand_param_increased():
    try:
        removeExistingResults(remove_run_results_csv=True)

        for i in range(5):
            results_df_1 = runSim(
                run=1,
                total_runs=1,
                sim_duration=60 * 24 * 7 * 2,  # run for 2 weeks
                warm_up_time=0,
                sim_start_date=datetime.strptime(
                    "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
                ),
                amb_data=False,
                demand_increase_percent=1.0,
            ).reset_index()

            results_df_1 = results_df_1[results_df_1["time_type"] == "arrival"]

            results_df_2 = runSim(
                run=1,
                total_runs=1,
                sim_duration=60 * 24 * 7 * 2,
                warm_up_time=0,
                sim_start_date=datetime.strptime(
                    "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
                ),
                amb_data=False,
                demand_increase_percent=1.2,
            ).reset_index()

            results_df_2 = results_df_2[results_df_2["time_type"] == "arrival"]

            assert len(results_df_1) < len(results_df_2), (
                "[FAIL - DEMAND PARAMETER] Fewer jobs observed when demand increase parameter above one"
            )
    finally:
        del results_df_1, results_df_2
        gc.collect()


def test_arrivals_decrease_if_demand_param_decrease():
    try:
        removeExistingResults(remove_run_results_csv=True)

        for i in range(5):
            results_df_1 = runSim(
                run=1,
                total_runs=1,
                sim_duration=60 * 24 * 7 * 2,  # run for 2 weeks
                warm_up_time=0,
                sim_start_date=datetime.strptime(
                    "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
                ),
                amb_data=False,
                demand_increase_percent=1.0,
            ).reset_index()

            results_df_1 = results_df_1[results_df_1["time_type"] == "arrival"]

            results_df_2 = runSim(
                run=1,
                total_runs=1,
                sim_duration=60 * 24 * 7 * 2,
                warm_up_time=0,
                sim_start_date=datetime.strptime(
                    "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
                ),
                amb_data=False,
                demand_increase_percent=0.8,
            ).reset_index()

            results_df_2 = results_df_2[results_df_2["time_type"] == "arrival"]

            assert len(results_df_1) > len(results_df_2), (
                "[FAIL - DEMAND PARAMETER] More jobs observed when demand increase parameter below one"
            )
    finally:
        del results_df_1, results_df_2
        gc.collect()


def test_output_when_no_demand():
    removeExistingResults(remove_run_results_csv=True)

    try:
        results_df_1 = runSim(
            run=1,
            total_runs=1,
            sim_duration=60 * 24 * 7 * 2,  # run for 2 weeks
            warm_up_time=0,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
            demand_increase_percent=0,
        ).reset_index()

        assert len(results_df_1) == 0

    except Exception:
        # Any exception is allowed
        pass


@pytest.mark.warmup
def test_warmup_only():
    """
    Ensures no results are recorded during the warm-up phase.

    This is tested by running the simulation with a non-zero warm-up time
    but zero actual simulation duration. Since the simulation doesn't run
    past the warm-up, no outputs should be produced.
    """
    try:
        removeExistingResults(remove_run_results_csv=True)

        # Run the simulation with only a warm-up period and no actual simulation time
        parallelProcessJoblib(
            total_runs=5,
            sim_duration=0,
            warm_up_time=24 * 7 * 60,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
        )

        collateRunResults()

        # Read simulation results
        results = pd.read_csv("data/run_results.csv")

        # Assert that the results are empty, i.e., no output was generated during warm-up
        assert len(results) == 0, (
            f"[FAIL - WARM-UP] {len(results)} results seem to have been generated during the warm-up period"
        )
    finally:
        del results
        gc.collect()


@pytest.mark.warmup
def test_no_results_recorded_from_warmup():
    """
    Ensures no results are recorded during the warm-up phase.

    This test runs the simulation with both a warm-up and post-warm-up period.
    It verifies that no records are generated that fall within the warm-up time.
    """
    try:
        removeExistingResults(remove_run_results_csv=True)

        warm_up_length = 60 * 24 * 3  # 3 days

        # Run the simulation with a warm-up period and simulation time
        parallelProcessJoblib(
            total_runs=2,
            sim_duration=60 * 24 * 7,  # 7 days
            warm_up_time=warm_up_length,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
        )

        collateRunResults()

        # Read simulation results
        results = pd.read_csv("data/run_results.csv")

        # Filter results to only those within the warm-up period (timestamp < warm-up time)
        results_in_warmup = results[results["timestamp"] < warm_up_length]

        # Assert no records were made during the warm-up period
        assert len(results_in_warmup) == 0, (
            f"[FAIL - WARM-UP] {len(results_in_warmup)} results appear in the results df that shouldn't due to falling in warm-up period"
        )

    finally:
        del results, results_in_warmup
        gc.collect()


@pytest.mark.resources
def test_simultaneous_allocation_same_resource_group(simulation_results):
    """
    Ensures no two jobs are allocated to resources from the same resource group at overlapping times.

    This checks for logical consistency in dispatch logic to prevent simultaneous usage
    of resources grouped together (e.g., mutually exclusive vehicles).

    In the simulation, where a helicopter and car belong to the same resource group, it is assumed
    that they can never be running at the same time due to the presence of a single crew crewing
    that single vehicle.

    For this reason, we are joining on callsign rather than registration (as when H71 is
    reallocated temporarily to H70, we need to continue to check that CC70 and the temporary
    H70 are not running simultaneously).
    """
    try:
        removeExistingResults(remove_run_results_csv=True)

        # Remove existing failure log if it exists
        if os.path.exists(
            "tests/test_outputs/simultaneous_allocation_same_callsigngroup_FAILURES.csv"
        ):
            os.remove(
                "tests/test_outputs/simultaneous_allocation_same_callsigngroup_FAILURES.csv"
            )

        if os.path.exists(
            "tests/test_outputs/simultaneous_allocation_same_callsigngroup_FULL.csv"
        ):
            os.remove(
                "tests/test_outputs/simultaneous_allocation_same_callsigngroup_FULL.csv"
            )

        results = simulation_results  # defined in conftest.py

        # Extract start and end times of resource usage
        resource_use_start_and_end = results[
            results["event_type"].isin(["resource_use", "resource_use_end"])
        ][
            [
                "P_ID",
                "run_number",
                "event_type",
                "callsign",
                "callsign_group",
                "timestamp_dt",
                "registration",
            ]
        ]

        # Merge start and end events into single row per job
        resource_use_start = (
            resource_use_start_and_end[
                resource_use_start_and_end["event_type"] == "resource_use"
            ]
            .rename(columns={"timestamp_dt": "resource_use_start"})
            .drop(columns="event_type")
        )

        resource_use_end = (
            resource_use_start_and_end[
                resource_use_start_and_end["event_type"] == "resource_use_end"
            ]
            .rename(columns={"timestamp_dt": "resource_use_end"})
            .drop(columns="event_type")
        )

        resource_use_wide = resource_use_start.merge(
            resource_use_end,
            how="outer",
            on=["P_ID", "run_number", "callsign", "callsign_group", "registration"],
        ).sort_values(["run_number", "P_ID"])

        resource_use_wide["resource_use_start"] = pd.to_datetime(
            resource_use_wide["resource_use_start"]
        )
        resource_use_wide["resource_use_end"] = pd.to_datetime(
            resource_use_wide["resource_use_end"]
        )

        # Get a list of callsign groups that appear in the sim output to iterate through
        callsign_groups = resource_use_wide["callsign_group"].unique()

        # Initialise an empty list to store all instances of overlaps in
        all_overlaps = []

        # For each group, identify any overlapping resource usage
        for callsign_group in callsign_groups:
            single_callsign = resource_use_wide[
                resource_use_wide["callsign_group"] == callsign_group
            ]

            # Sort by group and start time
            df_sorted = single_callsign.sort_values(
                by=["run_number", "callsign_group", "resource_use_start"]
            )

            # Shift end times within each group to compare with the next start
            df_sorted["prev_resource_use_start"] = df_sorted.groupby(
                ["run_number", "callsign_group"]
            )["resource_use_start"].shift()
            df_sorted["prev_resource_use_end"] = df_sorted.groupby(
                ["run_number", "callsign_group"]
            )["resource_use_end"].shift()
            df_sorted["prev_resource_callsign"] = df_sorted.groupby(
                ["run_number", "callsign_group"]
            )["callsign"].shift()
            df_sorted["prev_resource_reg"] = df_sorted.groupby(
                ["run_number", "callsign_group"]
            )["registration"].shift()
            df_sorted["prev_P_ID"] = df_sorted.groupby(
                ["run_number", "callsign_group"]
            )["P_ID"].shift()

            # Find overlaps
            df_sorted["overlap"] = (
                df_sorted["resource_use_start"] < df_sorted["prev_resource_use_end"]
            )

            # Filter to overlapping rows
            overlaps = df_sorted[df_sorted["overlap"]]

            print(f"Callsign Group {callsign_group} - jobs: {len(single_callsign)}")
            print(f"Callsign Group {callsign_group} - overlaps: {len(overlaps)}")

            all_overlaps.append(overlaps)

        all_overlaps_df = pd.concat(all_overlaps)

        if len(all_overlaps_df) > 0:
            all_overlaps_df.to_csv(
                "tests/test_outputs/simultaneous_allocation_same_callsigngroup_FAILURES.csv"
            )
            resource_use_wide.to_csv(
                "tests/test_outputs/simultaneous_allocation_same_callsigngroup_FULL.csv"
            )

        assert len(all_overlaps_df) == 0, (
            f"[FAIL - RESOURCE ALLOCATION LOGIC] {len(all_overlaps_df)} instances found of resources from the same callsign group being sent on two or more jobs at once across {len(resource_use_wide)} calls"
        )

    finally:
        del (
            resource_use_start_and_end,
            resource_use_start,
            resource_use_end,
            resource_use_wide,
            single_callsign,
            df_sorted,
            overlaps,
            all_overlaps,
            all_overlaps_df,
        )
        gc.collect()


@pytest.mark.resources
def test_simultaneous_allocation_same_resource(simulation_results):
    """
    Ensures no single resource is allocated to multiple jobs at the same time.

    Checks that a specific callsign (i.e., physical unit) is not double-booked.
    """
    try:
        if os.path.exists(
            "tests/test_outputs/simultaneous_allocation_same_resource_FAILURES.csv"
        ):
            os.remove(
                "tests/test_outputs/simultaneous_allocation_same_resource_FAILURES.csv"
            )

        if os.path.exists(
            "tests/test_outputs/simultaneous_allocation_same_resource_FULL.csv"
        ):
            os.remove(
                "tests/test_outputs/simultaneous_allocation_same_resource_FULL.csv"
            )

        results = simulation_results  # defined in conftest.py

        resource_use_start_and_end = results[
            results["event_type"].isin(["resource_use", "resource_use_end"])
        ][
            [
                "P_ID",
                "run_number",
                "event_type",
                "callsign",
                "callsign_group",
                "registration",
                "timestamp_dt",
            ]
        ]

        resource_use_start = (
            resource_use_start_and_end[
                resource_use_start_and_end["event_type"] == "resource_use"
            ]
            .rename(columns={"timestamp_dt": "resource_use_start"})
            .drop(columns="event_type")
        )

        resource_use_end = (
            resource_use_start_and_end[
                resource_use_start_and_end["event_type"] == "resource_use_end"
            ]
            .rename(columns={"timestamp_dt": "resource_use_end"})
            .drop(columns="event_type")
        )

        resource_use_wide = resource_use_start.merge(
            resource_use_end,
            how="outer",
            on=["P_ID", "run_number", "callsign", "callsign_group", "registration"],
        ).sort_values(["run_number", "P_ID"])

        resource_use_wide["resource_use_start"] = pd.to_datetime(
            resource_use_wide["resource_use_start"]
        )
        resource_use_wide["resource_use_end"] = pd.to_datetime(
            resource_use_wide["resource_use_end"]
        )

        callsigns = resource_use_wide["callsign"].unique()

        all_overlaps = []

        for callsign in callsigns:
            single_callsign = resource_use_wide[
                resource_use_wide["callsign"] == callsign
            ]

            assert len(single_callsign) > 0, (
                f"Single callsign df for {callsign} is empty"
            )

            # Sort by group and start time
            df_sorted = single_callsign.sort_values(
                by=["run_number", "callsign", "resource_use_start"]
            )
            print(df_sorted)

            # Shift end times within each group to compare with the next start
            df_sorted["prev_resource_use_start"] = df_sorted.groupby(
                ["run_number", "callsign"]
            )["resource_use_start"].shift()
            df_sorted["prev_resource_use_end"] = df_sorted.groupby(
                ["run_number", "callsign"]
            )["resource_use_end"].shift()
            df_sorted["prev_resource_callsign"] = df_sorted.groupby(
                ["run_number", "callsign"]
            )["callsign"].shift()
            df_sorted["prev_resource_reg"] = df_sorted.groupby(
                ["run_number", "callsign"]
            )["registration"].shift()
            df_sorted["prev_P_ID"] = df_sorted.groupby(["run_number", "callsign"])[
                "P_ID"
            ].shift()

            # Find overlaps
            df_sorted["overlap"] = (
                df_sorted["resource_use_start"] < df_sorted["prev_resource_use_end"]
            )

            # Filter to overlapping rows
            overlaps = df_sorted[df_sorted["overlap"]]

            print(f"Callsign {callsign} - instances: {len(single_callsign)}")
            print(f"Callsign {callsign} - overlaps: {len(overlaps)}")

            all_overlaps.append(overlaps)

        all_overlaps_df = pd.concat(all_overlaps)

        if len(all_overlaps_df) > 0:
            all_overlaps_df.to_csv(
                "tests/test_outputs/simultaneous_allocation_same_resource_FAILURES.csv"
            )
            resource_use_wide.to_csv(
                "tests/test_outputs/simultaneous_allocation_same_resource_FULL.csv"
            )

        assert len(all_overlaps_df) == 0, (
            f"[FAIL - RESOURCE ALLOCATION LOGIC] {len(all_overlaps_df)} instances found of resources being sent on two or more jobs at once across {len(resource_use_wide)} calls"
        )
    finally:
        del (
            resource_use_start_and_end,
            resource_use_start,
            resource_use_end,
            resource_use_wide,
            single_callsign,
            df_sorted,
            overlaps,
            all_overlaps,
            all_overlaps_df,
        )
        gc.collect()


@pytest.mark.resources
def test_no_response_during_off_shift_times(simulation_results):
    """
    Ensures no response is initiated outside of a resource's rota'd hours.

    Checks that callsigns are only used when their operating hours (seasonal) allow them to be active.
    Includes a manual test case for validation.
    """
    try:
        removeExistingResults(remove_run_results_csv=True)

        if os.path.exists("tests/test_outputs/offline_calls_FAILURES.csv"):
            os.remove("tests/test_outputs/offline_calls_FAILURES.csv")

        results = simulation_results  # defined in conftest.py

        resource_use_start = results[results["event_type"] == "resource_use"].rename(
            columns={"timestamp_dt": "resource_use_start"}
        )[
            [
                "P_ID",
                "run_number",
                "callsign",
                "resource_use_start",
                "day",
                "hour",
                "month",
                "qtr",
            ]
        ]

        resource_use_start["resource_use_start"] = pd.to_datetime(
            resource_use_start["resource_use_start"]
        )

        hems_rota_df = pd.read_csv("tests/rotas_test/HEMS_ROTA_test.csv")

        def normalize_hour_range(start, end):
            """Handles overnight hours by mapping to a 0–47 scale (so 2am next day = 26)"""
            if end <= start:
                end += 24
            return start, end

        # Make a copy of the rota and normalize hours
        rota_df = hems_rota_df.copy()
        rota_df[["summer_start", "summer_end"]] = rota_df[
            ["summer_start", "summer_end"]
        ].apply(lambda col: pd.to_numeric(col, errors="coerce"))

        rota_df[["winter_start", "winter_end"]] = rota_df[
            ["winter_start", "winter_end"]
        ].apply(lambda col: pd.to_numeric(col, errors="coerce"))

        # Group by callsign, take min start and max end, but account for overnight shifts
        def combine_shifts(group):
            summer_starts, summer_ends = zip(
                *[
                    normalize_hour_range(s, e)
                    for s, e in zip(group["summer_start"], group["summer_end"])
                ]
            )
            winter_starts, winter_ends = zip(
                *[
                    normalize_hour_range(s, e)
                    for s, e in zip(group["winter_start"], group["winter_end"])
                ]
            )

            return pd.Series(
                {
                    "summer_start": min(summer_starts),
                    "summer_end": max(summer_ends),
                    "winter_start": min(winter_starts),
                    "winter_end": max(winter_ends),
                }
            )

        rota_simplified = (
            rota_df.groupby("callsign")
            .apply(combine_shifts, include_groups=False)
            .reset_index()
        )

        merged_df = pd.merge(
            resource_use_start, rota_simplified, on="callsign", how="left"
        )

        def is_summer(month):
            return 4 <= month <= 9

        def check_if_available(row):
            if is_summer(row["month"]):
                start = row["summer_start"]
                end = row["summer_end"]
            else:
                start = row["winter_start"]
                end = row["winter_end"]

            hour = row["hour"]
            hour_extended = (
                hour if hour >= start else hour + 24
            )  # extend into next day if needed

            return start <= hour_extended < end

        # Apply the function to determine if the call is offline
        # check_if_available(...) returns True if the resource is available for that call.
        # Applying ~ in front of that means:
        # “Store True in is_offline when the resource is NOT available.”
        # So:
        # True from check_if_available ➝ False in is_offline
        # False from check_if_available ➝ True in is_offline
        merged_df["is_offline"] = ~merged_df.apply(check_if_available, axis=1)

        # Filter the DataFrame to get only the offline calls
        offline_calls = merged_df[merged_df["is_offline"]]

        if len(offline_calls) > 0:
            offline_calls.to_csv("tests/test_outputs/offline_calls_FAILURES.csv")

        # Check there are no offline calls
        assert len(offline_calls) == 0, (
            f"[FAIL - RESOURCE ALLOCATION LOGIC - ROTA] {len(offline_calls)} calls appear to have had a response initiated outside of rota'd hours"
        )

        # Add several test cases that should fail to the dataframe and rerun to ensure that
        # the test is actually written correctly as well!

        additional_rows = pd.DataFrame(
            [
                {
                    "P_ID": 99999,
                    "run_number": 1,
                    "callsign": "H70",
                    "resource_use_start": "2024-01-01 04:00:00",
                    "day": "Mon",
                    "hour": 4,
                    "month": 1,
                    "qtr": 1,
                },
                {
                    "P_ID": 99998,
                    "run_number": 1,
                    "callsign": "CC71",
                    "resource_use_start": "2024-01-01 04:00:00",
                    "day": "Mon",
                    "hour": 4,
                    "month": 1,
                    "qtr": 1,
                },
                {
                    "P_ID": 99997,
                    "run_number": 1,
                    "callsign": "CC71",
                    "resource_use_start": "2024-01-01 22:00:00",
                    "day": "Mon",
                    "hour": 22,
                    "month": 1,
                    "qtr": 1,
                },
                {
                    "P_ID": 99996,
                    "run_number": 1,
                    "callsign": "CC72",
                    "resource_use_start": "2024-01-01 07:00:00",
                    "day": "Mon",
                    "hour": 7,
                    "month": 1,
                    "qtr": 1,
                },
                # Also add an extra row that should pass
                {
                    "P_ID": 99995,
                    "run_number": 1,
                    "callsign": "CC72",
                    "resource_use_start": "2024-01-01 09:00:00",
                    "day": "Mon",
                    "hour": 9,
                    "month": 1,
                    "qtr": 1,
                },
            ]
        )

        resource_use_start = pd.concat([resource_use_start, additional_rows])

        merged_df = pd.merge(
            resource_use_start, rota_simplified, on="callsign", how="left"
        )
        merged_df["is_offline"] = ~merged_df.apply(check_if_available, axis=1)

        # Filter the DataFrame to get only the offline calls
        offline_calls = merged_df[merged_df["is_offline"]]

        # 4 rows we added should fail, 1 row we added should pass
        # (update this if adding additional test cases)
        assert len(offline_calls) == (len(additional_rows) - 1), (
            "[FAIL - RESOURCE ALLOCATION LOGIC - ROTA] The function for testing resources being allocated out of rota'd hours is not behaving correctly"
        )

    finally:
        del offline_calls, merged_df, resource_use_start, results
        gc.collect()


@pytest.mark.resources
def test_no_response_during_service(simulation_results):
    try:
        if os.path.exists("tests/test_outputs/responses_during_servicing_FULL.csv"):
            os.remove("tests/test_outputs/responses_during_servicing_FULL.csv")

        if os.path.exists("tests/test_outputs/responses_during_servicing_FAILURES.csv"):
            os.remove("tests/test_outputs/responses_during_servicing_FAILURES.csv")

        # Load key data files produced by the simulation - results and generated service intervals
        results = simulation_results  # defined in conftest.py
        services = pd.read_csv(
            "tests/service_dates_fixture.csv"
        )  # THIS DOES NOT GET GENERATED IN A SUBFOLDER

        # Ensure service start and end dates are datetimes
        services["service_start_date"] = pd.to_datetime(
            services["service_start_date"], format="%Y-%m-%d", errors="coerce"
        )
        services["service_end_date"] = pd.to_datetime(
            services["service_end_date"], format="%Y-%m-%d", errors="coerce"
        )

        # Extract 'resource_use' events from the results and parse relevant info
        resource_use_start = results[results["event_type"] == "resource_use"].rename(
            columns={"timestamp_dt": "resource_use_start"}
        )[
            # Note we are going to be using registration here as our identifier - not the callsign
            [
                "P_ID",
                "run_number",
                "registration",
                "callsign",
                "resource_use_start",
                "day",
                "hour",
                "month",
                "qtr",
            ]
        ]

        resource_use_start["resource_use_start"] = pd.to_datetime(
            resource_use_start["resource_use_start"]
        )

        # Merge resource usage with service records via registration
        # Note that registration is the area of importance for servicing - not the callsign
        # g-daas (H70) should borrow g-daan (H71) during servicing, leading to the callsign H70 remaining
        # in action during the servicing, and H71 showing no activity during servicing of g-daas.
        merged_df = pd.merge(
            resource_use_start, services, on="registration", how="left"
        )

        # Keep only rows where service start and end dates are valid
        # (i.e. discard any rows where no servicing exists in the servicing dataframe)
        # At present we don't have any servicing of cars (standalone or helicopter backup cars)
        # so those rows will not be of interest to us.
        valid_servicing = merged_df.dropna(
            subset=["service_start_date", "service_end_date"]
        )

        valid_servicing.to_csv("tests/test_outputs/responses_during_servicing_FULL.csv")

        # Identify any rows where the resource_use_start falls within the servicing interval
        violations = valid_servicing[
            (
                valid_servicing["resource_use_start"]
                >= valid_servicing["service_start_date"]
            )
            & (
                valid_servicing["resource_use_start"]
                <= valid_servicing["service_end_date"]
            )
        ]

        # Save violations to file if any are found
        if len(violations) > 0:
            violations.to_csv(
                "tests/test_outputs/responses_during_servicing_FAILURES.csv"
            )

        # Assert that no responses occurred during servicing periods
        assert len(violations) == 0, (
            f"[FAIL - RESOURCE ALLOCATION LOGIC - SERVICING] {len(violations)} resource_use_start values fall within a servicing interval"
        )

    except:
        del violations, valid_servicing, merged_df, resource_use_start, results
        gc.collect()
****************************************
