****************************************
CITATION.cff
****************************************
# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: Devon Air Ambulance Discrete Event Simulation Model
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: software
authors:
  - given-names: Sammi
    family-names: Rosser
    email: s.j.rosser@exeter.ac.uk
    affiliation: >-
      NIHR Applied Research Collaboration for the South West
      Peninsula (PenARC), University of Exeter
    orcid: 'https://orcid.org/0000-0002-9552-8988'
  - given-names: Richard
    family-names: Pilbery
    affiliation: Yorkshire Ambulance Service NHS Trust
    orcid: 'https://orcid.org/0000-0002-5797-9788'
  - given-names: Hannah
    family-names: Trebilcock
    affiliation: Devon Air Ambulance
identifiers:
  - type: doi
    value: 10.5281/zenodo.15555520
repository-code: 'https://github.com/RichardPilbery/DAA_DES'
url: 'https://daa-des-demo.streamlit.app/'
keywords:
  - discrete event simulation
  - air ambulance
license: GPL-3.0
****************************************

****************************************
CODE_OF_CONDUCT.md
****************************************
# Code of Conduct

## Our Pledge
We are committed to creating a welcoming and inclusive environment for everyone who participates in this project.
We pledge to make participation in our community a harassment-free experience for all, regardless of:

- Age
- Body size
- Disability
- Ethnicity
- Gender identity and expression
- Level of experience
- Nationality
- Personal appearance
- Race
- Religion
- Sexual identity and orientation

## Our Standards
Examples of behavior that contributes to a positive environment include:

- Being respectful and considerate in all interactions
- Offering constructive feedback
- Using inclusive language
- Focusing on what is best for the community
- Showing empathy toward other contributors

Examples of unacceptable behavior include:

- Harassment or discrimination of any kind
- Personal attacks, trolling, or insulting comments
- Public or private harassment
- Publishing others’ private information without consent
- Any conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities
Project maintainers are responsible for clarifying standards of acceptable behavior and are expected to take appropriate action in response to any unacceptable behavior.
They have the right and responsibility to remove, edit, or reject contributions that do not align with this Code of Conduct.

## Scope
This Code of Conduct applies both within project spaces (e.g., GitHub issues, pull requests, discussions) and in public spaces when someone is representing the project or its community.

## Enforcement
Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the maintainers via GitHub Issues or by direct contact.
All complaints will be reviewed and investigated promptly and fairly.

## Attribution
This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/), version 2.1.
****************************************

****************************************
LICENSE
****************************************
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.
****************************************

****************************************
packages.txt
****************************************
wget
****************************************

****************************************
pyproject.toml
****************************************
[build-system]
requires = ["setuptools>=65.5", "wheel"]
build-backend = "setuptools.build_meta"

[project]
version = "0.1.0"
name = "air_ambulance_des"
description = "Air ambulance discrete event simulation tools"
authors = [
    {name = "Richard Pilbery"},
    {name = "Sammi Rosser"},
    {name = "Hannah Trebilcock"}
]
readme = "README.md"
keywords = ["simulation", "air ambulance", "discrete event"]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent"
]

[tool.setuptools.packages.find]
where = ["."]
include = ["air_ambulance_des*"]
****************************************

****************************************
pytest.ini
****************************************
[pytest]
markers =
    warmup: Tests relating to the warm-up period
    resources: Tests relating to the correct logic around resource allocation and unavailability
    calls: Tests relating to the number of calls/jobs generated
    jobdurations: Tests relating to the duration of jobs
    quick: Tests to quickly check the sim is functioning at a basic level
    medium: Tests to quickly check the sim runs over a longer period of time than in the quick test
    reproducibility: Tests to check that random seeds behave as expected
    performance: Tests to check that basic metrics behave as expected when core parameters change
    callsign: Tests relating to correct allocation of callsigns
    callsigngroup: Tests relating to correct allocation of callsign groups
****************************************

****************************************
README.md
****************************************
# Devon Air Ambulance Discrete Event Simulation

This repository contains all model and web app code for the Devon Air Ambulance Simulation modelling project.


## DES model logic

The model creates patient episodes and associated outcomes based on the following sequence:

1.  Obtain AMPDS card category based on hour of day
2.  Choose a callsign based on activation criteria, which helicopter (if any) is available, whether helicopter can currently fly (servicing or weather impacts),
3.  Based on callsign, determine the HEMS result (Stand Down Before Mobile, Stand Down En Route, Landed but no patient contact, Patient Treated (Not Conveyed), Patient Conveyed)
4.  Based on the HEMS result determine the patient outcome (Airlifted, Conveyed by land with DAA, Conveyed by land without DAA, Deceased, Unknown)

A full breakdown of the model logic can be found in **reference/daa_des_model_logic**

![](reference/daa_des_model_logic.png)

## Issue Tracking and Roadmap

Project issues are tracked using the Github issues system and can be accessed [here](https://github.com/RichardPilbery/DAA_DES/issues).

Project milestones can be found [here](https://github.com/RichardPilbery/DAA_DES/milestones?direction=desc&sort=title&state=open).

These project milestones currently supersede information contained in the `roadmap.md` file.

Tickets actively being worked on by contributors can be found on the [project board](https://github.com/users/RichardPilbery/projects/1).


## API Documentation

The function and class documentation can be accessed at the following link: [https://richardpilbery.github.io/DAA_DES/](https://richardpilbery.github.io/DAA_DES/)

This documentation is automatically generated using [pdoc](https://pdoc.dev/) and will be regenerated when the code is updated on the main branch on Github. If you are a contributor and wish to see

It is also available as part of the enhanced documentation offering available at [bergam0t.quarto.pub/air-ambulance-simulation/](https://bergam0t.quarto.pub/air-ambulance-simulation/).

## Data output items

The model generates a CSV file containing raw data, which can then be wrangled and presented separately, either following a model run(s) or at any time after the model has been run.

**All** runs of the model get added to the **all_results.csv** file.

The last run - whether run by the functions in can be found in the **run_results.csv** file.

The table below identifies the column headers and provides a brief description

| Column name               | Description |
| --------------------------| ------------------------------------------------------------------------------ |
| P_ID                      | Patient ID    |
| run_number                | Run number in cases where the model is run repeatedly (e.g. 100x) to enable calculations of confidence intervals etc.|
| time_type                 | Category that elapsed time represents e.g. 'call start', 'on scene', 'leave scene', 'arrive hospital', 'handover', 'time clear'|
| timestamp                 | Elapsed time in seconds since model started running |
| day                       | Day of the week as a string ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun') |
| hour                      | Hour of call as integer between 0–23 |
| weekday                   | String identifying whether the call started on a weekday or weekend |
| month                     | Integer representing the month of the year (1–12) |
| qtr                       | Integer representing the yearly quarter of the call (1–4) |
| callsign                  | String representing callsign of resource (either HEMS or Ambulance service) |
| triage_code               | Call triage outcome represented as one of the AMPDS 'golden' codes (or OTHER) |
| age                       | Integer representing patient age in years |
| sex                       | String representing patient sex ('male', 'female') |
| time_to_first_response    | Integer representing time in minutes to first response (ambulance service or HEMS) |
| time_to_cc                | Integer representing time in minutes to first HEMS response |
| cc_conveyed               | Integer indicating whether HEMS conveyed patient (1 or 0) |
| cc_flown                  | Integer indicating whether HEMS conveyed the patient by air (1 or 0) |
| cc_travelled_with         | Integer indicating whether patient was conveyed by ambulance but HEMS personnel travelled with the patient to hospital (1 or 0) |
| hems                      | Integer indicating whether HEMS attended the incident (1 or 0) |
| cc_desk                   | Integer indicating whether HEMS activation was due to critical care desk dispatch |
| dispatcher_intevention    | Integer indicating whether the ambulance service dispatcher activated HEMS for a called which did not meet auto-dispatch criteria (1 or 0) |

## Environment Setup

### Installing the project locally

Clone this repository to your machine.

The core environment is provided in the `requirements.txt` file in the root folder of the repository.

This has been used in conjunction with Python 3.11.9.

Using your preferred virtual environment manager, install the requirements specified in `requirements.txt`.

When you have completed this, you will need to undertake one additional step; installing the air ambulance simulation code.

This is achieved by running `pip install -e .`

> [!IMPORTANT]
> Be sure to run this command from the root of the repository (the folder containing pyproject.toml and requirements.txt).

This installs the air ambulance simulation code as an *editable package*. This means that any changes to the classes and functions in the folder `air_ambulance_des` will automatically be recognised in the web app or anywhere else you call the code from while using this environment, without needing to reinstall the package.

This additional step is necessary because the simulation code is organised as a Python package (air_ambulance_des) rather than a set of standalone scripts. This supports long term reusability, testing and documentation workflows.

### Remote hosting

A devcontainer.json file has also been provided in the .devcontainer folder; this allows VSCode to access or create a container with the appropriate versions of Python and all requirements, including automatically installing the air ambulance simulation code as an editable package (so you will **not** need to set up a virtual environment or run `pip install -e .`).

Alternatively, you could open up the development environment in Github Codespaces, which will achieve the same purpose without you having to clone the repository and set up an environment on your local machine. To access this, ensure you are logged into GitHub, then look for the green 'Code' button at the top of this repository. Click on this and select 'Create codespace on main'.

![](readme_assets/2025-10-22-16-12-57.png)

You can find out more about codespaces at [github.com/features/codespaces](https://github.com/features/codespaces).

## Web App

Assuming you have installed the environment as above, either locally or in Github Codespaces, the web app can be run using the command

`streamlit run app/app.py`

The app will attempt to use multiple cores of the user's computer to undertake several runs of the model simultaneously.

The web app can also be accessed without needing to install anything at [daa-des-demo.streamlit.app/](https://daa-des-demo.streamlit.app/). However, not that this will run more slowly and will complete fewer runs by default due to limitations of the hosting platform (which prevents multi-core running); it is recommended to download the app and run it locally if you are not just looking to get an idea of the app's capabilities.

### Quarto

If you wish to be able to download the output from the web app as a Quarto file, you will need to also install Quarto.

Quarto can be downloaded at [https://quarto.org/docs/get-started/](https://quarto.org/docs/get-started/).

- It is recommended that, when asked by the installer, you add Quarto to your PATH variable.
- It is important to note that while a [Python package for quarto exists](https://pypi.org/project/quarto/), this is not the full Quarto command line utility, which will need to be installed separately.

Note that if you are using the .devcontainer, Quarto will be available within the container without undertaking any additional steps.

### Hosted version of web app

The Streamlit app is also available at the following link: [https://daa-des-demo.streamlit.app/](https://daa-des-demo.streamlit.app/)

Note that for permissions reasons, the hosted version of the app runs off a fork ((https://github.com/Bergam0t/DAA_DES)[https://github.com/Bergam0t/DAA_DES]) instead of this main repository.
Therefore, the hosted version may not always be the most up to date version of the app.

You can determine if the fork is currently up to date with this repository by navigating to the fork using the link above and looking at the following message.

![](readme_assets/2025-03-07-16-40-19.png)

Note that due to limitations of the hosting platform, each run of the model is undertaken sequentially, not in parallel, so execution times are much longer.

## Estimated Run Times

As of March 2025, the app has been tested on a pc with the following specs

- OS: Windows 11 (Version 10.0.22631 Build 22631)
- Processor: 12th Gen Intel(R) Core(TM) i7-12700H, 2300 Mhz, 14 Core(s), 20 Logical Processor(s)
- Installed Physical Memory (RAM): 32.0 GB

When executing the model through the streamlit interface, running locally so parallel processing can be used, the following run times were observed.

**2 years simulated time, default demand, default parameters (2 helicopters, 2 backup cars, 1 additional car)**

*including generation of all plots via web interface*

- **12 runs, 730 days:** ~2 minutes (~90 seconds for initial model running)
- **100 runs, 730 days:** ~8 minutes (~4 minutes for initial model running)


## Details for Contributors

Please refer to our code of conduct here: [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)

### Repository Structure

#### Root

#### actual_data

This folder contains key input data that is called upon by the model, pertaining to areas including rotas and servicing.

#### air_ambulance_des

The model logic is structured as a package in this folder.

**class_ambulance.py**:
This file defines the core resource class.

**class_hems_availability.py**:
This file contains classes controlling the logic around the allocation of resources to jobs.

**class_hems.py**:
This file defines the HEMS class as a child of the core resource class defined in class_ambulance.

**class_historic_results.py**
This file defines a class that allows for generation of metrics and plots relating to the historical data. This allows for comparison of the model with historic data in a number of ways.

**class_patient.py**:
This file defines the core patient class. The model uses a single patient class.

**class_simulation_inputs.py**:
This manages the data files that are handed to the simulation, ensuring they are made available to the historical data and simulation data classes to facilitate calculations.

**class_simulation_trial_results.py**:
This file defines a class that allows for generation of metrics and plots relating to the data generated by running the simulation.

**des_hems.py**:
This file is the key file controlling call generation and call journeys, and is where most of the logic relating to the patient journey is held.

**des_parallel_process.py**:
This file contains functions to perform a single run of the model, or multiple runs using parallel processing. It also handles the collation of results files and associated filespace cleanup.

*There are also several supporting files.*

**distribution_fit_utils.py**: This file contains functions and classes for fitting distributions from raw data. Note that instructions are currently not given for the required data format for using this on your own data. However, the distributions generated by these functions for the site the model is being developed for can be found in the *distribution_data* folder, allowing other parts of the app to be run in the absence of raw job-level data.

**utils.py**:
This file contains various supporting functions used across other files.

#### air_ambulance_des_docs

This folder contains various files related to the enhanced Quarto documentation for this model and app.
This does not contain the rendered output - only manually written documentation files, as well as configuration files to support the generation of the documentation.

#### app

This folder contains code relating to the generation of the interactive web app interface.

#### checklists

This folder contains

#### distribution_data

This folder contains the distributions that are generated from the call-level data and the module **air_ambulance_des.distribution_fit_utils**.

#### docs

This folder contains the pdoc documentation. Note that this is usually generated as part of an automated GitHub workflow, so this folder will usually be empty apart from a `.gitignore` file.

#### docs_quarto

This folder contains the rendered quarto documentation.

#### historical_data

This folder contains aggregated historical data.

This data is currently used to visualise the historical range of data and compare it with the simulated results.

The data will also be used as part of the pytest testing suite to ensure models run with the parameters in use at the time these simulations were run successfully mimic the observed real-world patterns.

#### readme_assets

This folder contains images used within this readme.

#### reference

This folder contains

If editing the files, you should use the .drawio versions, which can be opened with the free tool diagrams.net.

It is recommended that you edit and export the diagrams in light mode, and export using a 50 pixel border, to ensure a consistent appearance is maintained.

#### tests

This folder contains files that will be picked up by the pytest framework. They are planned to contain various tests including validation/verification tests, and unit tests.

To run all tests, use the command `pytest` in a terminal, running from the root folder of the project.

To run a subset of tests, you can choose subsets of tests. For example, to run all resource-related tests based on their tag (known as a marker), run `pytest -m resources`. More options can be seen [here](https://docs.pytest.org/en/stable/example/markers.html).

Markers available include:

```
    warmup: Tests relating to the warm-up period
    resources: Tests relating to the correct logic around resource allocation and unavailability
    calls: Tests relating to the number of calls/jobs generated
    jobdurations: Tests relating to the duration of jobs
    quick: Tests to quickly check the sim is functioning at a basic level
    reproducibility: Tests to check that random seeds behave as expected
    performance: Tests to check that basic metrics behave as expected when core parameters change
```

See `pytest.ini` for the most up-to-date list of markers.

You can also use `pytest -k some_string` to match tests with a name containing a particular string (replacing `some_string` in the command with your own search term).

Alternatively, see the [pytest documentation](https://docs.pytest.org/en/stable/how-to/usage.html) for additional ways to manage groups of tests.

To generate an interactive html report, run `pytest --html=report.html`

To generate a code coverage report, run `pytest --cov=. --cov-report=html`.

### Regenerating documentation

#### pdoc (basic class/function documentation)

In general, you will not to generate the pdoc documentation as this is controlled by a Github actions workflow (.github/docs.yml).

However, you may wish to do so for testing purposes or to preview the documentation without pushing changes - though note that your rendered/previewed version will be ignored by GitHub.

To regenerate pdoc documentation, ensure you are in the root of the repo (folder 'DAA_DES')

Then run `pdoc air_ambulance_des -o docs/`

This will output the pdoc documentation to the `docs/` folder.

To preview the documentation, navigate to this folder and open the file `index.html`.

The pdoc documentation is hosted on GitHub pages at [richardpilbery.github.io/DAA_DES/](https://richardpilbery.github.io/DAA_DES/)

#### quarto/quartodoc (basic class/function documentation + additional enhanced documentation)

To regenerate the quartodoc/quarto documentation, first make sure you have the quarto CLI installed and the quartodoc package installed in your environment (which should happen automatically if you have set up the environment from requirements.txt)

First, move into the folder `air_ambulance_des_docs`. If you are in the root of the repository, you can do this by running `cd air_ambulance_des_docs`.

Next, run `quartodoc build`. This will rebuild the sidebar and the autogenerated class and function documentation.

Then, run `quarto render`. This will fully render both the autogenerated quartodoc pages and all additional pages that are included.

If you wish to preview the finished documentation, run `quarto preview`. This will open a preview server. Depending on your IDE settings, this may open in a panel in the IDE, or it may open as a tab in your default web browser.

The quarto/quartodoc documentation is hosted on quartopub at [bergam0t.quarto.pub/air-ambulance-simulation/](https://bergam0t.quarto.pub/air-ambulance-simulation/).

To update the published version of this documentation, run `quarto publish`. Please get in touch with [`Bergam0t`]() if you need to do this as you will need an [access token](https://quarto.org/docs/publishing/quarto-pub.html#access-tokens) issuing.
****************************************

****************************************
requirements.txt
****************************************
fitter==1.7.1
Jinja2==3.1.6
MarkupSafe==2.1.5
pandas==2.2.2
pdoc==14.7.0
Pygments==2.18.0
python-dateutil==2.9.0.post0
pytz==2024.1
simpy==4.1.1
six==1.16.0
tzdata==2024.1
plotly==6.0.1
streamlit==1.50.0
sim-tools==0.6.1
vidigi==0.0.3
joblib==1.4.2
streamlit-extras==0.5.0
schemdraw==0.19
openpyxl==3.1.5
python-calamine==0.3.1
nbformat==5.10.4
nbclient==0.10.2
scour==0.38.2
psutil==7.0.0
ipykernel==6.29.5
pytest==8.3.4
pytest-cov==6.0.0
pytest-html==4.1.1
quartodoc==0.11.1
****************************************

****************************************
roadmap.md
****************************************
# Model Version 1

## October 2024 - February 2025

- Development of initial version of simulation model
- Development of user interface for model
- Allow changing of
    - Rotas
    - Helicopter Models (with associated changes to servicing schedule)
    - Overall Demand

## March 2025

- Incorporation of 'missed' call data to better match true demand
- Refinements to model to ensure real-world patterns are well reflected


## April - May 2025

- Refinements based on initial feedback
- Add animations of resource usage

# Future Model Development

- Add geographical elements to the model
****************************************

****************************************
run_dist_fit_utils.py
****************************************
if __name__ == "__main__":
    from air_ambulance_des.distribution_fit_utils import DistributionFitUtils

    test = DistributionFitUtils(
        "external_data/clean_daa_import_missing_2023_2024.csv",
        calculate_school_holidays=True,
    )
    # test = DistributionFitUtils('external_data/clean_daa_import.csv')
    test.import_and_wrangle()
    test.run_sim_on_historical_params()

# Testing ----------
# python distribution_fit_utils.py
****************************************

****************************************
run_parallel_process.py
****************************************
from air_ambulance_des.des_parallel_process import (
    removeExistingResults,
    parallelProcessJoblib,
)
from datetime import datetime

if __name__ == "__main__":
    removeExistingResults()
    # parallelProcessJoblib(1, (1*365*24*60), (0*60), datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"), False, False, 1.0, 1.0, True)
    parallelProcessJoblib(
        12,
        (2 * 365 * 24 * 60),
        (0 * 60),
        datetime.strptime("2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"),
        False,
        False,
        1.0,
        1.0,
    )

# Testing ----------
# python des_parallel_process.py
****************************************

****************************************
.devcontainer\devcontainer.json
****************************************
{
  "name": "Python 3",
  "image": "mcr.microsoft.com/devcontainers/python:1-3.11-bullseye",
  "customizations": {
    "codespaces": {
      "openFiles": [
        "README.md"
        ]
    },
    "vscode": {
      "settings": {},
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance"
      ]
    }
  },
  "updateContentCommand": "[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt && pip3 install -e .; echo '✅ Packages installed and Requirements met'",
  "postAttachCommand": {
    "server": "streamlit run app/app.py --server.enableCORS false --server.enableXsrfProtection false"
  },
  "portsAttributes": {
    "8501": {
      "label": "Application",
      "onAutoForward": "openPreview"
    }
  },
  "forwardPorts": [
    8501
  ]
}
****************************************

****************************************
.github\workflows\docs.yml
****************************************
name: website

# build the documentation whenever there are new commits on main
on:
  push:
    branches:
      - main
    # Alternative: only build for tags.
    # tags:
    #   - '*'

# security: restrict permissions for CI jobs.
permissions:
  contents: read

jobs:
  # Build the documentation and upload the static HTML files as an artifact.
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # ADJUST THIS: install all dependencies (including pdoc)
      - run: pip install -r requirements.txt
      # ADJUST THIS: build your documentation into docs/.
      # We use a custom build script for pdoc itself, ideally you just run `pdoc -o docs/ ...` here.
      - run: pdoc air_ambulance_des -o docs/

      - uses: actions/upload-pages-artifact@v3
        with:
          path: docs/

  # Deploy the artifact to GitHub pages.
  # This is a separate job so that only actions/deploy-pages has the necessary permissions.
  deploy:
    needs: build
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
****************************************

****************************************
.streamlit\config.toml
****************************************
[theme]
base="light"
primaryColor="#a6093d"
secondaryBackgroundColor="#D5F5F6"
textColor="#000000"
****************************************

****************************************
air_ambulance_des\class_ambulance.py
****************************************
from air_ambulance_des.utils import Utils
import pandas as pd


class Ambulance:
    """
    # The Ambulance class

    This class defines an 'Ambulance'; effectively any resource that
    responds to a patient/incident. This includes HEMS, which is a
    child class of Ambulance.


    """

    def __init__(self, ambulance_type="ambulance", callsign="AMBULANCE"):
        self.mobile = ""
        self.as_scene = ""
        self.leaving_scene = ""
        self.at_hospital = ""
        self.clear = ""
        self.stood_down = ""

        self.ambulance_type = ambulance_type
        self.callsign = callsign

    def what_am_i(self):
        print(f"I am {self.ambulance_type}")


# TEST
# Run on python command line

# from class_ambulance import Ambulance
# from class_hems import HEMS

# a = HEMS("CC72", "2021-01-01 00:00:00")
# a.what_am_i()

# b = Ambulance("2021-01-01 00:00:00")
# b.what_am_i()
****************************************

****************************************
air_ambulance_des\class_hems.py
****************************************
import pandas as pd
from air_ambulance_des.utils import Utils
from air_ambulance_des.class_ambulance import Ambulance


class HEMS(Ambulance):
    """
    # The HEMS class

    This class defines a HEMS resource


    """

    def __init__(
        self,
        callsign: str,
        callsign_group: str,
        vehicle_type: str,
        category: str,
        registration: str,
        summer_start: str,
        winter_start: str,
        summer_end: str,
        winter_end: str,
        servicing_schedule: pd.DataFrame,
        resource_id: str = None,
        summer_season: list[int] = None,
    ):
        # Inherit all parent class functions
        super().__init__(ambulance_type="HEMS")

        self.utilityClass = Utils()

        self.callsign = callsign
        self.callsign_group = callsign_group
        self.available = 1
        self.being_serviced = False
        self.flying_time = 0
        self.vehicle_type = vehicle_type
        self.category = category
        self.registration = registration
        self.summer_start = summer_start
        self.winter_start = winter_start
        self.summer_end = summer_end
        self.winter_end = winter_end

        # Read in the summer months from a file
        summer_months = pd.read_csv("actual_data/rota_start_end_months.csv")
        summer_start_month = int(
            summer_months[summer_months["what"] == "summer_start_month"][
                "month"
            ].values[0]
        )
        summer_end_month = int(
            summer_months[summer_months["what"] == "summer_end_month"]["month"].values[
                0
            ]
        )

        self.summer_season = [
            x for x in range(summer_start_month, summer_end_month + 1)
        ]

        # Pre-determine the servicing schedule when the resource is created
        self.servicing_schedule = servicing_schedule

        self.in_use = False
        self.resource_id = resource_id

    def service_check(self, current_dt: pd.Timestamp, GDAAS_service: bool) -> bool:
        if self.registration == "g-daan":
            if GDAAS_service:
                self.callsign_group = 70
                self.callsign = "H70"

            else:
                self.callsign_group = 71
                self.callsign = "H71"
                self.in_use = (
                    False  # We might need to re-think this in 24/7 scenarios although
                )
                # presumably the callsign will not change during an incident, only after.

            # GDASS being serviced

            # print(f"reg {self.registration} now has callsign {self.callsign}")

        return self.unavailable_due_to_service(current_dt)

    def unavailable_due_to_service(self, current_dt: pd.Timestamp) -> bool:
        """
        Returns logical value denoting whether the HEMS resource is currently
        unavailable due to being serviced

        """

        for index, row in self.servicing_schedule.iterrows():
            # print(row)
            if row["service_start_date"] <= current_dt <= row["service_end_date"]:
                self.being_serviced = True
                return True

        self.being_serviced = False
        return False

    def hems_resource_on_shift(self, hour: int, month: int) -> bool:
        """
        Function to determine whether the HEMS resource is within
        its operational hours
        """

        # Assuming summer hours are quarters 2 and 3 i.e. April-September
        # Can be modified if required.
        # SR NOTE: If changing these, please also modify in
        # write_run_params() function in des_parallel_process
        start = self.summer_start if month in self.summer_season else self.winter_start
        end = self.summer_end if month in self.summer_season else self.winter_end

        return self.utilityClass.is_time_in_range(int(hour), int(start), int(end))
****************************************

****************************************
air_ambulance_des\class_hems_availability.py
****************************************
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from typing import Any, Generator
import pandas as pd
from simpy import FilterStore, Event
from numpy.random import SeedSequence

from air_ambulance_des.class_patient import Patient
from air_ambulance_des.utils import Utils
from air_ambulance_des.class_hems import HEMS

import logging
from enum import IntEnum


class ResourceAllocationReason(IntEnum):
    NONE_AVAILABLE = 0
    MATCH_PREFERRED_CARE_CAT_HELI = 1
    MATCH_PREFERRED_CARE_CAT_CAR = 2
    CC_MATCH_EC_HELI = 3
    CC_MATCH_EC_CAR = 4
    EC_MATCH_CC_HELI = 5
    EC_MATCH_CC_CAR = 6
    REG_HELI_BENEFIT_MATCH_EC_HELI = 7
    REG_HELI_BENEFIT_MATCH_CC_HELI = 8
    REG_NO_HELI_BENEFIT_GROUP_AND_VEHICLE = 9
    REG_NO_HELI_BENEFIT_GROUP = 10
    OTHER_VEHICLE_TYPE = 11
    REG_NO_HELI_BENEFIT_ANY = 12
    REG_NO_HELI_BENEFIT_VEHICLE = 13


class HEMSAvailability:
    """
    # The HEMS Availability class

    This class is a filter store which can provide HEMS resources
    based on the time of day and servicing schedule


    """

    def __init__(
        self,
        env,
        sim_start_date,
        sim_duration,
        utility: Utils,
        servicing_overlap_allowed=False,
        servicing_buffer_weeks=4,
        servicing_preferred_month=1,
        print_debug_messages=False,
        master_seed=SeedSequence(42),
    ):
        self.LOOKUP_LIST = [
            "No HEMS resource available",  # 0
            "Preferred HEMS care category and vehicle type match",  # 1
            "Preferred HEMS care category match but not vehicle type",  # 2
            "HEMS CC case EC helicopter available",  # 3
            "HEMS CC case EC car available",  # 4
            "HEMS EC case CC helicopter available",  # 5
            "HEMS EC case CC car available",  # 6
            "HEMS REG helicopter case EC helicopter available",
            "HEMS REG helicopter case CC helicopter available",
            "HEMS REG case no helicopter benefit preferred group and vehicle type allocated",  # 9
            "HEMS REG case no helicopter benefit preferred group allocated",  # 10
            "No HEMS resource available (pref vehicle type = 'Other')",  # 11
            "HEMS REG case no helicopter benefit first free resource allocated",  # 12
            "HEMS REG case no helicopter benefit free helicopter allocated",  # 13
        ]

        self.env = env
        self.print_debug_messages = print_debug_messages
        self.master_seed = master_seed
        self.utilityClass = utility

        # Adding options to set servicing parameters.
        self.servicing_overlap_allowed = servicing_overlap_allowed
        self.serviing_buffer_weeks = servicing_buffer_weeks
        self.servicing_preferred_month = servicing_preferred_month
        self.sim_start_date = sim_start_date

        self.debug(f"Sim start date {self.sim_start_date}")
        # For belts and braces, add an additional year to
        # calculate the service schedules since service dates can be walked back to the
        # previous year
        self.sim_end_date = sim_start_date + timedelta(
            minutes=sim_duration + (1 * 365 * 24 * 60)
        )

        # School holidays
        self.school_holidays = pd.read_csv("actual_data/school_holidays.csv")

        self.HEMS_resources_list = []

        self.active_callsign_groups = (
            set()
        )  # Prevents same crew being used twice...hopefully...
        self.active_registrations = set()  # Prevents same vehicle being used twice
        self.active_callsigns = set()

        # Create a store for HEMS resources
        self.store = FilterStore(env)

        self.serviceStore = FilterStore(env)

        # Prepare HEMS resources for ingesting into store
        self.prep_HEMS_resources()

        # Populate the store with HEMS resources
        self.populate_store()

        # Daily servicing check (in case sim starts during a service)
        [dow, hod, weekday, month, qtr, current_dt] = (
            self.utilityClass.date_time_of_call(self.sim_start_date, self.env.now)
        )

        self.daily_servicing_check(current_dt, hod, month)

    def debug(self, message: str):
        if self.print_debug_messages:
            logging.debug(message)
            # print(message)

    def daily_servicing_check(self, current_dt: datetime, hour: int, month: int):
        """
        Function to iterate through the store and trigger the service check
        function in the HEMS class
        """
        h: HEMS

        self.debug("------ DAILY SERVICING CHECK -------")

        GDAAS_service = False

        all_resources = self.serviceStore.items + self.store.items
        for h in all_resources:
            if h.registration == "g-daas":
                GDAAS_service = h.unavailable_due_to_service(current_dt)
                break

        self.debug(f"GDAAS_service is {GDAAS_service}")

        # --- Return from serviceStore to store ---
        to_return = [
            (s.category, s.registration)
            for s in self.serviceStore.items
            if not s.service_check(current_dt, GDAAS_service)  # Note the NOT here!
        ]

        # Attempted fix for gap after return from H70 duties
        for h in self.store.items:
            if h.registration == "g-daan" and not GDAAS_service:
                h.callsign_group = 71
                h.callsign = "H71"

        if to_return:
            self.debug("Service store has items to return")

        for category, registration in to_return:
            s = yield self.serviceStore.get(
                lambda item: item.category == category
                and item.registration == registration
            )
            yield self.store.put(s)
            self.debug(
                f"Returned [{s.category} / {s.registration}] from service to store"
            )

        # --- Send from store to serviceStore ---
        to_service = [
            (h.category, h.registration)
            for h in self.store.items
            if h.service_check(current_dt, GDAAS_service)
        ]

        for category, registration in to_service:
            self.debug("****************")
            self.debug(
                f"HEMS [{category} / {registration}] being serviced, removing from store"
            )

            h = yield self.store.get(
                lambda item: item.category == category
                and item.registration == registration
            )

            self.debug(
                f"HEMS [{h.category} / {h.registration}] successfully removed from store"
            )
            yield self.serviceStore.put(h)
            self.debug(f"HEMS [{h.category} / {h.registration}] moved to service store")
            self.debug("***********")

        self.debug(self.current_store_status(hour, month))
        self.debug(self.current_store_status(hour, month, "service"))

        [dow, hod, weekday, month, qtr, current_dt] = (
            self.utilityClass.date_time_of_call(self.sim_start_date, self.env.now)
        )
        for h in self.store.items:
            if h.registration == "g-daan":
                self.debug(
                    f"[{self.env.now}] g-daan status: in_use={h.in_use}, callsign={h.callsign}, group={h.callsign_group}, on_shift={h.hems_resource_on_shift(hod, month)}"
                )

        self.debug("------ END OF DAILY SERVICING CHECK -------")

    def prep_HEMS_resources(self) -> None:
        """
        This function ingests HEMS resource data from a user-supplied CSV file
        and populates a list of HEMS class objects. The key activity here is
        the calculation of service schedules for each HEMS object, taking into account a
        user-specified preferred month of servicing, service duration, and a buffer period
        following a service to allow for over-runs and school holidays

        """

        schedule = []
        service_dates = []

        # Calculate service schedules for each resource

        SERVICING_SCHEDULE = pd.read_csv("actual_data/service_schedules_by_model.csv")
        SERVICE_HISTORY = pd.read_csv("actual_data/service_history.csv", na_values=0)
        CALLSIGN_REGISTRATION = pd.read_csv(
            "actual_data/callsign_registration_lookup.csv"
        )

        SERVICING_SCHEDULE = SERVICING_SCHEDULE.merge(
            CALLSIGN_REGISTRATION, how="right", on="model"
        )

        SERVICING_SCHEDULE = SERVICING_SCHEDULE.merge(
            SERVICE_HISTORY, how="left", on="registration"
        )

        self.debug(f"prep_hems_resources: schedule {SERVICING_SCHEDULE}")

        for index, row in SERVICING_SCHEDULE.iterrows():
            # self.debug(row)
            current_resource_service_dates = []
            # Check if service date provided
            if not pd.isna(row["last_service"]):
                # self.debug(f"Checking {row['registration']} with previous service date of {row['last_service']}")
                last_service = datetime.strptime(row["last_service"], "%Y-%m-%d")
                service_date = last_service

                while last_service < self.sim_end_date:
                    end_date = (
                        last_service
                        + timedelta(weeks=int(row["service_duration_weeks"]))
                        + timedelta(weeks=self.serviing_buffer_weeks)
                    )

                    service_date, end_date = self.find_next_service_date(
                        last_service,
                        row["service_schedule_months"],
                        service_dates,
                        row["service_duration_weeks"],
                    )

                    schedule.append((row["registration"], service_date, end_date))
                    # self.debug(service_date)
                    service_dates.append(
                        {
                            "service_start_date": service_date,
                            "service_end_date": end_date,
                        }
                    )

                    current_resource_service_dates.append(
                        {
                            "service_start_date": service_date,
                            "service_end_date": end_date,
                        }
                    )
                    # self.debug(service_dates)
                    # self.debug(current_resource_service_dates)
                    last_service = service_date
            else:
                schedule.append((row["registration"], None, None))
                # self.debug(schedule)

            service_df = pd.DataFrame(
                schedule,
                columns=["registration", "service_start_date", "service_end_date"],
            )

            service_df.to_csv("data/service_dates.csv", index=False)

        # Append those to the HEMS resource.

        HEMS_RESOURCES = (
            pd.read_csv("actual_data/HEMS_ROTA.csv")
            # Add model and servicing rules
            .merge(SERVICING_SCHEDULE, on=["callsign", "vehicle_type"], how="left")
        )

        for index, row in HEMS_RESOURCES.iterrows():
            s = service_df[service_df["registration"] == row["registration"]]
            # self.debug(s)

            # Create new HEMS resource and add to HEMS_resource_list
            # pd.DataFrame(columns=['year', 'service_start_date', 'service_end_date'])
            hems = HEMS(
                callsign=row["callsign"],
                callsign_group=row["callsign_group"],
                vehicle_type=row["vehicle_type"],
                category=row["category"],
                registration=row["registration"],
                summer_start=row["summer_start"],
                winter_start=row["winter_start"],
                summer_end=row["summer_end"],
                winter_end=row["winter_end"],
                servicing_schedule=s,
                resource_id=row["registration"],
            )

            self.HEMS_resources_list.append(hems)

        # self.debug(self.HEMS_resources_list)

    def populate_store(self):
        """
        Function to populate the filestore with HEMS class objects
        contained in a class list
        """

        h: HEMS
        for h in self.HEMS_resources_list:
            self.debug(f"Populating resource store: HEMS({h.callsign})")
            self.debug(h.servicing_schedule)
            self.store.put(h)

    def add_hems(self):
        """
        Future function to allow for adding HEMS resources.
        We might not use this (we could just amend the HEMS_ROTA dataframe, for example)
        but might be useful for 'what if' simulations
        """
        pass

    # def resource_allocation_lookup(self, prefered_lookup: int):
    #     """
    #         Function to return description of lookup allocation choice

    #     """

    #     lookup_list = [
    #         "No HEMS resource available",
    #         "Preferred HEMS care category and vehicle type match",
    #         "Preferred HEMS care category match but not vehicle type",
    #         "HEMS CC case EC helicopter available",
    #         "HEMS CC case EC car available",
    #         "HEMS EC case CC helicopter available",
    #         "HEMS EC case CC car available",
    #         "HEMS helicopter case EC helicopter available",
    #         "HEMS helicopter case CC helicopter available",
    #         "HEMS REG case no helicopter benefit preferred group and vehicle type allocated",
    #         "HEMS REG case no helicopter benefit preferred group allocated",
    #         "No HEMS resource available (pref vehicle type = 'Other')",
    #         "HEMS REG case no helicopter benefit first free resource allocated"
    #     ]

    #     return lookup_list[prefered_lookup]

    def resource_allocation_lookup(self, reason: ResourceAllocationReason) -> str:
        return self.LOOKUP_LIST[reason.value]

    def current_store_status(self, hour, month, store="resource") -> list[str]:
        """
        Debugging function to return current state of store
        """

        current_store_items = []

        h: HEMS

        if store == "resource":
            for h in self.store.items:
                current_store_items.append(
                    f"{h.callsign} ({h.category} online: {h.hems_resource_on_shift(hour, month)} {h.registration})"
                )
        else:
            for h in self.serviceStore.items:
                current_store_items.append(
                    f"{h.callsign} ({h.category} online: {h.hems_resource_on_shift(hour, month)} {h.registration})"
                )

        return current_store_items

    # def preferred_resource_available(self, pt: Patient) -> list[HEMS | None, str]:

    #     """
    #         Check whether the preferred resource group is available. Returns a list with either the HEMS resource or None, and
    #         an indication as to whether the resource was available, or another resource in an established hierachy of
    #         availability can be allocated
    #     """

    #     # Initialise object HEMS as a placeholder object
    #     hems: HEMS | None = None
    #     # Initialise variable 'preferred' to False
    #     preferred = 999 # This will be used to ensure that the most desireable resource
    #                 # is allocated given that multiple matches may be found
    #     preferred_lookup = 0 # This will be used to code the resource allocation choice

    #     preferred_care_category = pt.hems_cc_or_ec

    #     self.debug(f"EC/CC resource with {preferred_care_category} and hour {pt.hour} and qtr {pt.qtr}")

    #     h: HEMS
    #     for h in self.store.items:

    #         # There is a hierachy of calls:
    #         # CC = H70 helicopter then car, then H71 helicopter then car then CC72
    #         # EC = H71 helicopter then car, then CC72, then H70 helicopter then car
    #         # If no resources then return None

    #         if not h.in_use and h.hems_resource_on_shift(pt.hour, pt.qtr):
    #             # self.debug(f"Checking whether to generate ad-hoc reason for {h} ({h.vehicle_type} - {h.callsign_group})")

    #             if ( # Skip this resource if any of the following are true:
    #                 h.in_use or
    #                 h.being_serviced or
    #                 not h.hems_resource_on_shift(pt.hour, pt.qtr) or
    #                 # Skip if crew is already in use
    #                 h.callsign_group in self.active_callsign_groups or
    #                 h.registration in self.active_registrations
    #             ):
    #                 # self.debug(f"Skipping ad-hoc unavailability check for {h}")
    #                 continue

    #             if h.vehicle_type == "car":
    #                 ad_hoc_reason = "available"
    #             else:
    #                 ad_hoc_reason = self.utilityClass.sample_ad_hoc_reason(pt.hour, pt.qtr, h.registration)

    #             self.debug(f"({h.callsign}) Sampled reason for patient {pt.id} ({pt.hems_cc_or_ec}) is: {ad_hoc_reason}")

    #             if ad_hoc_reason != "available":
    #                 continue

    #             if h.category == preferred_care_category and h.vehicle_type == "helicopter" and not h.being_serviced:
    #                 hems = h
    #                 preferred = 1 # Top choice
    #                 preferred_lookup = 1
    #                 return [hems,  self.resource_allocation_lookup(preferred_lookup)]

    #             elif h.category == preferred_care_category and not h.being_serviced:
    #                 hems = h
    #                 preferred = 2 # Second choice (correct care category, but not vehicle type)
    #                 preferred_lookup = 2

    #             elif preferred_care_category == 'CC':
    #                 if h.vehicle_type == 'helicopter' and h.category == 'EC' and not h.being_serviced:
    #                     if preferred > 3:
    #                         hems = h
    #                         preferred = 3 # Third  choice (EC helicopter)
    #                         preferred_lookup = 3

    #                 elif h.category == 'EC' and not h.being_serviced:
    #                     if preferred > 4:
    #                         hems = h
    #                         preferred = 4
    #                         preferred_lookup = 4

    #             elif preferred_care_category == 'EC':
    #                 if h.vehicle_type == 'helicopter' and h.category == 'CC' and not h.being_serviced:
    #                     if preferred > 3:
    #                         hems = h
    #                         preferred = 3 # CC helicopter available
    #                         preferred_lookup = 5

    #                 elif h.category == 'CC' and not h.being_serviced:
    #                     hems = h
    #                     preferred = 4
    #                     preferred_lookup = 6

    #     self.debug(f"preferred lookup {preferred_lookup} and preferred = {preferred}")

    #     if preferred_lookup != 999:
    #         return [hems, self.resource_allocation_lookup(preferred_lookup)]
    #     else:
    #         return [None, self.resource_allocation_lookup(0)]

    def preferred_resource_available(self, pt: Patient) -> list[HEMS | None, str]:
        """
        Determine the best available HEMS resource for an EC/CC case based on the
        patient's preferred care category (EC = Enhanced Care, CC = Critical Care),
        vehicle type, and ad-hoc availability. Returns the chosen HEMS unit and
        the reason for its selection.

        Returns:
            A list containing:
                - The selected HEMS unit (or None if none available)
                - A lookup code describing the allocation reason
        """
        # Retrieve patient’s preferred care category
        preferred_category = pt.hems_cc_or_ec
        self.debug(
            f"EC/CC resource with {preferred_category} and hour {pt.hour} and qtr {pt.qtr}"
        )

        best_hems: HEMS | None = None  # Best-matching HEMS unit found so far
        best_priority = float("inf")  # Lower values = better matches
        best_lookup = (
            ResourceAllocationReason.NONE_AVAILABLE
        )  # Reason for final allocation

        for h in self.store.items:
            # --- FILTER OUT UNAVAILABLE RESOURCES ---
            if (
                h.in_use  # Already dispatched
                or h.being_serviced  # Currently under maintenance
                or not h.hems_resource_on_shift(
                    pt.hour, pt.month
                )  # Not scheduled for shift now
                or h.callsign_group
                in self.active_callsign_groups  # Another unit from this group is active (so crew is engaged elsewhere)
                or h.registration
                in self.active_registrations  # This specific unit is already dispatched
            ):
                continue  # Move to the next HEMS unit

            # Check ad-hoc reason
            # For "car" units, assume always available.
            # For helicopters, simulate availability using ad-hoc logic (e.g., weather, servicing).
            reason = (
                "available"
                if h.vehicle_type == "car"
                else self.utilityClass.sample_ad_hoc_reason(
                    pt.hour, pt.qtr, h.registration
                )
            )
            self.debug(
                f"({h.callsign}) Sampled reason for patient {pt.id} ({pt.hems_cc_or_ec}) is: {reason}"
            )

            if reason != "available":
                continue  # Skip this unit if not usable

            # Decide priority and reason
            priority = None
            lookup = None

            # 1. Best case: resource category matches preferred care category *and* is a helicopter
            if h.category == preferred_category and h.vehicle_type == "helicopter":
                priority = 1
                lookup = ResourceAllocationReason.MATCH_PREFERRED_CARE_CAT_HELI

            # 2. Next best: resource category matches preferred care category, but is a car
            elif h.category == preferred_category:
                priority = 2
                lookup = ResourceAllocationReason.MATCH_PREFERRED_CARE_CAT_CAR

            # 3–4. Category mismatch fallback options:
            # For a CC preference, fall back to EC providers if needed
            elif preferred_category == "CC":
                if h.category == "EC" and h.vehicle_type == "helicopter":
                    priority = 3
                    lookup = ResourceAllocationReason.CC_MATCH_EC_HELI
                elif h.category == "EC":
                    priority = 4
                    lookup = ResourceAllocationReason.CC_MATCH_EC_CAR

            # For an EC preference, fall back to CC providers if needed
            elif preferred_category == "EC":
                if h.category == "CC" and h.vehicle_type == "helicopter":
                    priority = 3
                    lookup = ResourceAllocationReason.EC_MATCH_CC_HELI
                elif h.category == "CC":
                    priority = 4
                    lookup = ResourceAllocationReason.EC_MATCH_CC_CAR

            # --- CHECK IF THIS IS THE BEST OPTION SO FAR ---
            if priority is not None and priority < best_priority:
                best_hems = h
                best_priority = priority
                best_lookup = lookup

                # Immediate return if best possible match found (priority 1)
                if priority == 1:
                    self.debug(
                        f"Top priority match found: {best_lookup.name} ({best_lookup.value})"
                    )
                    return [best_hems, self.resource_allocation_lookup(best_lookup)]

        # Final fallback: return the best match found (if any), or none with reason
        self.debug(
            f"Selected best lookup: {best_lookup.name} ({best_lookup.value}) with priority = {best_priority}"
        )
        return [best_hems, self.resource_allocation_lookup(best_lookup)]

    def allocate_resource(self, pt: Patient) -> Any | Event:
        """
        Attempt to allocate a resource from the preferred group.
        """
        resource_event: Event = self.env.event()

        def process() -> Generator[Any, Any, None]:
            self.debug(
                f"Allocating resource for {pt.id} and care cat {pt.hems_cc_or_ec}"
            )

            pref_res: list[HEMS | None, str] = self.preferred_resource_available(pt)

            if pref_res[0] is None:
                return resource_event.succeed([None, pref_res[1], None])

            primary = pref_res[0]

            # Block if in-use by callsign group, registration, or callsign
            if primary.callsign_group in self.active_callsign_groups:
                self.debug(f"[BLOCKED] Callsign group {primary.callsign_group} in use")
                return resource_event.succeed([None, pref_res[1], None])

            if primary.registration in self.active_registrations:
                self.debug(f"[BLOCKED] Registration {primary.registration} in use")
                return resource_event.succeed([None, pref_res[1], None])

            if primary.callsign in self.active_callsigns:
                self.debug(f"[BLOCKED] Callsign {primary.callsign} already in use")
                return resource_event.succeed([None, pref_res[1], None])

            self.active_callsign_groups.add(primary.callsign_group)
            self.active_registrations.add(primary.registration)
            self.active_callsigns.add(primary.callsign)

            with self.store.get(lambda r: r == primary) as primary_request:
                result = yield primary_request | self.env.timeout(0.1)

                if primary_request in result:
                    primary.in_use = True
                    pt.hems_callsign_group = primary.callsign_group
                    pt.hems_vehicle_type = primary.vehicle_type
                    pt.hems_category = primary.category

                    # self.active_callsign_groups.add(primary.callsign_group)
                    # self.active_registrations.add(primary.registration)
                    # self.active_callsigns.add(primary.callsign)

                    # Try to get a secondary resource
                    with self.store.get(
                        lambda r: r != primary
                        and r.callsign_group == pt.hems_callsign_group
                        and r.category == pt.hems_category
                        and r.hems_resource_on_shift(pt.hour, pt.month)
                        and r.callsign_group not in self.active_callsign_groups
                        and r.registration not in self.active_registrations
                        and r.callsign not in self.active_callsigns
                    ) as secondary_request:
                        result2 = yield secondary_request | self.env.timeout(0.1)
                        secondary = None

                        if secondary_request in result2:
                            secondary = result2[secondary_request]
                            secondary.in_use = True
                            self.active_callsign_groups.add(secondary.callsign_group)
                            self.active_registrations.add(secondary.registration)
                            self.active_callsigns.add(secondary.callsign)

                    return resource_event.succeed([primary, pref_res[1], secondary])
                else:
                    # Roll back if unsuccessful
                    self.active_callsign_groups.discard(primary.callsign_group)
                    self.active_registrations.discard(primary.registration)
                    self.active_callsigns.discard(primary.callsign)

                    return resource_event.succeed([None, pref_res[1], None])

        self.env.process(process())
        return resource_event

    def return_resource(self, resource: HEMS, secondary_resource: HEMS | None) -> None:
        """
        Class to return HEMS class object back to the filestore.
        """

        resource.in_use = False
        self.active_callsign_groups.discard(resource.callsign_group)
        self.active_registrations.discard(resource.registration)
        self.active_callsigns.discard(resource.callsign)
        self.store.put(resource)
        self.debug(f"{resource.callsign} finished job")

        if secondary_resource is not None:
            secondary_resource.in_use = False
            self.active_callsign_groups.discard(secondary_resource.callsign_group)
            self.active_registrations.discard(secondary_resource.registration)
            self.active_callsigns.discard(secondary_resource.callsign)
            self.store.put(secondary_resource)
            self.debug(
                f"{secondary_resource.callsign} free as {resource.callsign} finished job"
            )

    def years_between(self, start_date: datetime, end_date: datetime) -> list[int]:
        """
        Function to return a list of years between given start and end date
        """
        return list(range(start_date.year, end_date.year + 1))

    def do_ranges_overlap(
        self, start1: datetime, end1: datetime, start2: datetime, end2: datetime
    ) -> bool:
        """
        Function to determine whether two sets of datetimes overlap
        """
        return max(start1, start2) <= min(end1, end2)

    def is_during_school_holidays(
        self, start_date: datetime, end_date: datetime
    ) -> bool:
        """
        Function to calculate whether given start and end date time period falls within
        a school holiday
        """

        for index, row in self.school_holidays.iterrows():
            if self.do_ranges_overlap(
                pd.to_datetime(row["start_date"]),
                pd.to_datetime(row["end_date"]),
                start_date,
                end_date,
            ):
                return True

        return False

    def is_other_resource_being_serviced(self, start_date, end_date, service_dates):
        """
        Function to determine whether any resource is being services between a
        given start and end date period.
        """

        for sd in service_dates:
            if self.do_ranges_overlap(
                sd["service_start_date"], sd["service_end_date"], start_date, end_date
            ):
                return True

        return False

    def find_next_service_date(
        self,
        last_service_date: datetime,
        interval_months: int,
        service_dates: list,
        service_duration: int,
    ) -> list[datetime]:
        """
        Function to determine the next service date for a resource. The date is determine by
        the servicing schedule for the resource, the preferred month of servicing, and to
        avoid dates that fall in either school holidays or when other resources are being serviced.
        """

        next_due_date = last_service_date + relativedelta(
            months=interval_months
        )  # Approximate month length
        end_date = next_due_date + timedelta(weeks=service_duration)

        preferred_date = datetime(next_due_date.year, self.servicing_preferred_month, 2)
        preferred_end_date = preferred_date + timedelta(weeks=service_duration)

        if next_due_date.month > preferred_date.month:
            preferred_date += relativedelta(years=1)

        # self.debug(f"Next due: {next_due_date} with end date {end_date} and preferred_date is {preferred_date} with pref end {preferred_end_date}")

        # If preferred date is valid, use it
        if preferred_date <= next_due_date and not self.is_during_school_holidays(
            preferred_date, preferred_end_date
        ):
            next_due_date = preferred_date

        while True:
            if self.is_during_school_holidays(
                next_due_date, end_date
            ) or self.is_other_resource_being_serviced(
                next_due_date, end_date, service_dates
            ):
                next_due_date -= timedelta(days=1)
                end_date = next_due_date + timedelta(weeks=service_duration)
                continue
            else:
                break

        return [next_due_date, end_date]

    # def preferred_regular_group_available(self, pt: Patient) -> list[HEMS | None, str]:
    #     """
    #     Check availability for REG jobs while avoiding assigning two units from
    #     the same crew/callsign group.
    #     """
    #     hems: HEMS | None = None
    #     preferred = 999
    #     preferred_lookup = 0

    #     preferred_group = pt.hems_pref_callsign_group
    #     preferred_vehicle_type = pt.hems_pref_vehicle_type
    #     helicopter_benefit = pt.hems_helicopter_benefit

    #     for h in self.store.items:

    #         if (
    #             h.in_use or
    #             h.being_serviced or
    #             not h.hems_resource_on_shift(pt.hour, pt.qtr) or
    #             # Skip if crew is already in use
    #             h.callsign_group in self.active_callsign_groups or
    #             h.registration in self.active_registrations
    #             ):
    #             continue

    #         if h.vehicle_type == "car":
    #             ad_hoc_reason = "available"
    #         else:
    #             ad_hoc_reason = self.utilityClass.sample_ad_hoc_reason(pt.hour, pt.qtr, h.registration)

    #         self.debug(f"({h.callsign}) Sampled reason for patient {pt.id} (REG) is: {ad_hoc_reason}")

    #         if ad_hoc_reason != "available":
    #             continue

    #         # Helicopter benefit cases
    #         if helicopter_benefit == 'y':
    #             if h.vehicle_type == 'helicopter' and h.category == 'EC':
    #                 hems = h
    #                 preferred_lookup = 7
    #                 break
    #             elif h.vehicle_type == 'helicopter':
    #                 if preferred > 2:
    #                     hems = h
    #                     preferred = 2
    #                     preferred_lookup = 8

    #         # Regular (non-helicopter) cases
    #         else:
    #             if (h.callsign_group == preferred_group
    #                 and h.vehicle_type == preferred_vehicle_type):
    #                 hems = h
    #                 preferred_lookup = 9
    #                 break
    #             elif h.callsign_group == preferred_group:
    #                 if preferred > 4:
    #                     hems = h
    #                     preferred = 4
    #                     preferred_lookup = 10
    #             else:
    #                 if preferred > 5:
    #                     hems = h
    #                     preferred = 5
    #                     preferred_lookup = 12

    #     return [hems, self.resource_allocation_lookup(preferred_lookup if hems else 0)]

    def preferred_regular_group_available(self, pt: Patient) -> list[HEMS | None, str]:
        """
        Determine the most suitable HEMS (Helicopter Emergency Medical Service) unit
        for a REG (regular) job, ensuring that two units from the same crew/callsign
        group are not simultaneously active. This method prioritizes matching the
        patient's preferences and helicopter benefit where applicable.

        Returns:
            A list containing:
                - The selected HEMS unit (or None if none available)
                - A lookup code describing the allocation reason
        """
        # Initialize selection variables
        hems: HEMS | None = None  # The selected resource
        preferred = float(
            "inf"
        )  # Lower numbers mean more preferred options (2, 4, 5 are used below)
        preferred_lookup = (
            ResourceAllocationReason.NONE_AVAILABLE
        )  # Reason code for selection

        preferred_group = pt.hems_pref_callsign_group  # Preferred crew group
        preferred_vehicle_type = (
            pt.hems_pref_vehicle_type
        )  # e.g., "car" or "helicopter"
        helicopter_benefit = (
            pt.hems_helicopter_benefit
        )  # "y" if helicopter has clinical benefit
        # Iterate through all HEMS resources stored
        for h in self.store.items:
            if (
                h.in_use  # Already dispatched
                or h.being_serviced  # Currently under maintenance
                or not h.hems_resource_on_shift(
                    pt.hour, pt.month
                )  # Not scheduled for shift now
                or h.callsign_group
                in self.active_callsign_groups  # Another unit from this group is active (so crew is engaged elsewhere)
                or h.registration
                in self.active_registrations  # This specific unit is already dispatched
            ):
                continue  # Move to the next HEMS unit

            # Check ad hoc availability
            # For "car" units, assume always available.
            # For helicopters, simulate availability using ad-hoc logic (e.g., weather, servicing).
            reason = (
                "available"
                if h.vehicle_type == "car"
                else self.utilityClass.sample_ad_hoc_reason(
                    pt.hour, pt.qtr, h.registration
                )
            )
            self.debug(
                f"({h.callsign}) Sampled reason for patient {pt.id} (REG) is: {reason}"
            )

            if reason != "available":
                continue  # Skip this unit if not usable

            # --- HELICOPTER BENEFIT CASE ---
            # P3 = Helicopter patient
            # Resources allocated in following order:
            # IF H70 available = SEND
            # ELSE H71 available = SEND

            # Assume 30% chance of knowing in advance that it's going to be a heli benefit case
            # when choosing to send a resource (and if you know that, don't fall back to
            # sending a car)
            if (
                helicopter_benefit == "y"
                and self.utilityClass.rngs["know_heli_benefit"].uniform(0, 1) <= 0.5
            ):
                # Priority 1: CC-category helicopter (assumed most beneficial)
                if h.vehicle_type == "helicopter" and h.category == "CC":
                    hems = h
                    preferred_lookup = (
                        ResourceAllocationReason.REG_HELI_BENEFIT_MATCH_CC_HELI
                    )
                    break
                # Priority 2: Any helicopter (less preferred than CC, hence priority = 2)
                elif h.vehicle_type == "helicopter" and preferred > 2:
                    hems = h
                    preferred = 2
                    preferred_lookup = (
                        ResourceAllocationReason.REG_HELI_BENEFIT_MATCH_EC_HELI
                    )

            # If no EC or CC helicopters are available, then:
            # - hems remains None
            # - preferred_lookup remains at its initial value (ResourceAllocationReason.NONE_AVAILABLE)
            # - The function exits the loop without assigning a resource.

            # --- REGULAR JOB WITH NO SIMULATED HELICOPTER BENEFIT ---
            else:
                # Best match: matching both preferred callsign group and vehicle type
                if (
                    h.callsign_group == preferred_group
                    and h.vehicle_type == preferred_vehicle_type
                ):
                    hems = h
                    preferred_lookup = (
                        ResourceAllocationReason.REG_NO_HELI_BENEFIT_GROUP_AND_VEHICLE
                    )
                    break
                # Next best: send a helicopter
                elif h.vehicle_type == "helicopter" and preferred > 3:
                    hems = h
                    preferred = 3
                    preferred_lookup = (
                        ResourceAllocationReason.REG_NO_HELI_BENEFIT_VEHICLE
                    )
                # Next best: match only on preferred callsign group
                elif h.callsign_group == preferred_group and preferred > 4:
                    hems = h
                    preferred = 4
                    preferred_lookup = (
                        ResourceAllocationReason.REG_NO_HELI_BENEFIT_GROUP
                    )
                # Fallback: any available resource
                elif preferred > 5:
                    hems = h
                    preferred = 5
                    preferred_lookup = ResourceAllocationReason.REG_NO_HELI_BENEFIT_ANY

        # Return the best found HEMS resource and reason for selection
        self.debug(
            f"Selected REG (heli benefit = {helicopter_benefit}) lookup: {preferred_lookup.name} ({preferred_lookup.value})"
        )
        return [
            hems,
            self.resource_allocation_lookup(
                preferred_lookup if hems else ResourceAllocationReason.NONE_AVAILABLE
            ),
        ]

    def allocate_regular_resource(self, pt: Patient) -> Any | Event:
        """
        Attempt to allocate a resource from the preferred group (REG jobs).
        """
        resource_event: Event = self.env.event()

        def process() -> Generator[Any, Any, None]:
            # if pt.hems_pref_vehicle_type == "Other":
            #     pref_res = [None, self.resource_allocation_lookup(11)]
            # else:
            pref_res = self.preferred_regular_group_available(pt)

            if pref_res[0] is None:
                return resource_event.succeed([None, pref_res[1], None])

            primary = pref_res[0]

            # Block if in-use by callsign group, registration, or callsign
            if primary.callsign_group in self.active_callsign_groups:
                self.debug(
                    f"[BLOCKED] Regular Callsign group {primary.callsign_group} in use"
                )
                return resource_event.succeed([None, pref_res[1], None])

            if primary.registration in self.active_registrations:
                self.debug(
                    f"[BLOCKED] Regular Registration {primary.registration} in use"
                )
                return resource_event.succeed([None, pref_res[1], None])

            if primary.callsign in self.active_callsigns:
                self.debug(
                    f"[BLOCKED] Regular Callsign {primary.callsign} already in use"
                )
                return resource_event.succeed([None, pref_res[1], None])

            self.active_callsign_groups.add(primary.callsign_group)
            self.active_registrations.add(primary.registration)
            self.active_callsigns.add(primary.callsign)

            with self.store.get(lambda r: r == primary) as primary_request:
                result = yield primary_request | self.env.timeout(0.1)

                if primary_request in result:
                    primary.in_use = True
                    pt.hems_callsign_group = primary.callsign_group
                    pt.hems_vehicle_type = primary.vehicle_type
                    pt.hems_category = primary.category

                    # self.active_callsign_groups.add(primary.callsign_group)
                    # self.active_registrations.add(primary.registration)
                    # self.active_callsigns.add(primary.callsign)

                    # Try to get a secondary resource
                    with self.store.get(
                        lambda r: r != primary
                        and r.callsign_group == pt.hems_callsign_group
                        and r.category == pt.hems_category
                        and r.hems_resource_on_shift(pt.hour, pt.month)
                        and r.callsign_group not in self.active_callsign_groups
                        and r.registration not in self.active_registrations
                        and r.callsign not in self.active_callsigns
                    ) as secondary_request:
                        result2 = yield secondary_request | self.env.timeout(0.1)
                        secondary = None

                        if secondary_request in result2:
                            secondary = result2[secondary_request]
                            secondary.in_use = True
                            self.active_callsign_groups.add(secondary.callsign_group)
                            self.active_registrations.add(secondary.registration)
                            self.active_callsigns.add(secondary.callsign)

                    return resource_event.succeed([primary, pref_res[1], secondary])

                else:
                    # Roll back if unsuccessful
                    self.active_callsign_groups.discard(primary.callsign_group)
                    self.active_registrations.discard(primary.registration)
                    self.active_callsigns.discard(primary.callsign)

                    return resource_event.succeed([None, pref_res[1], None])

        self.env.process(process())
        return resource_event
****************************************

****************************************
air_ambulance_des\class_historic_results.py
****************************************
import pandas as pd
import plotly.express as px
from plotly.graph_objects import Figure
from air_ambulance_des._processing_functions import graceful_methods, get_param


@graceful_methods
class HistoricResults:
    """
    Manages historic results
    """

    def __init__(
        self,
        historical_data_folder_path,
        historic_rota_df_path,
        historic_callsign_df_path,
        historic_servicing_df_path,
    ):
        self.historical_data_path = historical_data_folder_path

        # Attendance dataframes
        self.historical_missed_calls_by_month = None
        self.get_historical_missed_calls_by_month_df()

        # Missed calls
        self.historical_missed_calls_by_hour_df = None
        self.get_historical_missed_calls_by_hour()

        self.historical_missed_calls_by_quarter_and_hour_df = None
        self.get_historical_missed_calls_by_quarter_and_hour()

        # Jobs per month
        self.historical_jobs_per_month = None
        self.get_historical_jobs_per_month()

        self.historical_monthly_totals_by_callsign = None
        self.get_historical_monthly_totals_by_callsign()

        self.historical_monthly_totals_all_calls = None
        self.get_historical_monthly_totals_all_calls()

        self.historical_monthly_totals_by_hour_of_day = None
        self.get_historical_monthly_totals_by_hour_of_day()

        self.historical_monthly_totals_by_day_of_week = None
        self.get_historical_monthly_totals_by_day_of_week()

        # Jobs per day
        self.historical_jobs_per_day_per_callsign = None
        self.get_historical_jobs_per_day_per_callsign()

        self.historical_daily_calls_breakdown = None
        self.get_historical_daily_calls_breakdown()

        # Care categories
        self.historical_care_cat_counts = None
        self.get_historical_care_cat_counts()

        self.care_cat_by_hour_historic = None
        self.get_care_cat_by_hour_historic()

        # Activity durations
        self.historical_median_time_of_activities_by_month_and_resource_type = None
        self.get_historical_median_time_of_activities_by_month_and_resource_type()
        self.historical_job_durations_breakdown = None
        self.get_historical_job_durations_breakdown()

        # Utilisation (recorded)
        self.historical_monthly_resource_utilisation = None
        self.get_historical_monthly_resource_utilisation()

        ########################################
        # 'Calculated' historical data         #
        ########################################
        # There is various data that we don't have - for example, we don't
        # know how many patients who would have benefitted from enhanced care
        # failed to receive it, by the very nature of those jobs
        # Therefore we simulate it to the best of our ability during the fitting
        # process using historical rotas and the model, effectively running a
        # 'base case'/'current situation' that we can use for comparisons against
        # different scenarios

        self.run_params_used = None
        self.get_run_params_used()

        # Historic rotas and scheduling details
        self.historic_rota_df = pd.read_csv(historic_rota_df_path)
        self.historic_callsign_df = pd.read_csv(historic_callsign_df_path)
        self.historic_servicing_df = pd.read_csv(historic_servicing_df_path)

        # Calculated/simulated historical comparisons
        self.SIM_hist_params_missed_jobs_care_cat_summary = None
        self.get_SIM_hist_params_missed_jobs_care_cat_summary_df()

        # Suboptimal allocations (by care cat or vehicle type)
        self.SIM_hist_suboptimal_care_cat_sent_summary = None
        self.get_SIM_hist_suboptimal_care_cat_sent_summary_df()

        self.SIM_hist_suboptimal_vehicle_type_sent_summary = None
        self.get_SIM_hist_suboptimal_vehicle_type_sent_summary_df()

        # Missed calls (by care cat or vehicle type)
        self.SIM_hist_missed_jobs_care_cat_breakdown = None
        self.get_SIM_hist_missed_jobs_care_cat_breakdown_df()

        # Utilisation (calculated)
        self.historical_utilisation_df_complete = None
        self.historical_utilisation_df_summary = None
        self.make_RWC_utilisation_dataframe()

    ###################################################
    # MARK: Methods for data import from csvs         #
    ###################################################
    def get_historical_monthly_totals_by_hour_of_day(self):
        self.historical_monthly_totals_by_hour_of_day = pd.read_csv(
            f"{self.historical_data_path}/historical_monthly_totals_by_hour_of_day.csv"
        )

    def get_historical_daily_calls_breakdown(self):
        self.historical_daily_calls_breakdown = pd.read_csv(
            f"{self.historical_data_path}/historical_daily_calls_breakdown.csv"
        )

    def get_historical_monthly_totals_by_day_of_week(self):
        self.historical_monthly_totals_by_day_of_week = pd.read_csv(
            f"{self.historical_data_path}/historical_monthly_totals_by_day_of_week.csv"
        )

        self.historical_monthly_totals_by_day_of_week["month"] = pd.to_datetime(
            self.historical_monthly_totals_by_day_of_week["month"], format="ISO8601"
        )

    def get_run_params_used(self):
        self.run_params_used = pd.read_csv(
            f"{self.historical_data_path}/calculated/run_params_used.csv"
        )

    def get_historical_monthly_totals_all_calls(self):
        self.historical_monthly_totals_all_calls = pd.read_csv(
            f"{self.historical_data_path}/historical_monthly_totals_all_calls.csv"
        )

        self.historical_monthly_totals_all_calls["month"] = pd.to_datetime(
            self.historical_monthly_totals_all_calls["month"], format="ISO8601"
        )

        self.historical_monthly_totals_all_calls["Month_Numeric"] = (
            self.historical_monthly_totals_all_calls["month"].apply(lambda x: x.month)
        )

        self.historical_monthly_totals_all_calls["Year_Numeric"] = (
            self.historical_monthly_totals_all_calls["month"].apply(lambda x: x.year)
        )

    def get_SIM_hist_params_missed_jobs_care_cat_summary_df(self):
        self.SIM_hist_params_missed_jobs_care_cat_summary = pd.read_csv(
            f"{self.historical_data_path}/calculated/SIM_hist_params_missed_jobs_care_cat_summary.csv"
        )

    def get_SIM_hist_missed_jobs_care_cat_breakdown_df(self):
        self.SIM_hist_missed_jobs_care_cat_breakdown = pd.read_csv(
            f"{self.historical_data_path}/calculated/SIM_hist_params_missed_jobs_care_cat_breakdown.csv"
        )

    def get_SIM_hist_suboptimal_care_cat_sent_summary_df(self):
        self.SIM_hist_suboptimal_care_cat_sent_summary = pd.read_csv(
            f"{self.historical_data_path}/calculated/SIM_hist_params_suboptimal_care_cat_sent_summary.csv"
        )

    def get_SIM_hist_suboptimal_vehicle_type_sent_summary_df(self):
        self.SIM_hist_suboptimal_vehicle_type_sent_summary = pd.read_csv(
            f"{self.historical_data_path}/calculated/SIM_hist_params_suboptimal_vehicle_type_sent_summary.csv"
        )

    def get_historical_missed_calls_by_month_df(self):
        self.historical_missed_calls_by_month = pd.read_csv(
            f"{self.historical_data_path}/historical_missed_calls_by_month.csv"
        )

        self.historical_missed_calls_by_month = (
            self.historical_missed_calls_by_month.pivot(
                columns="callsign_group_simplified", index="month_start", values="count"
            ).reset_index()
        )

        self.historical_missed_calls_by_month.rename(
            columns={
                "HEMS (helo or car) available and sent": "jobs_attended",
                "No HEMS available": "jobs_not_attended",
            },
            inplace=True,
        )

        self.historical_missed_calls_by_month["all_received_calls"] = (
            self.historical_missed_calls_by_month["jobs_attended"]
            + self.historical_missed_calls_by_month["jobs_not_attended"]
        )

        self.historical_missed_calls_by_month["perc_unattended_historical"] = (
            self.historical_missed_calls_by_month["jobs_not_attended"]
            / self.historical_missed_calls_by_month["all_received_calls"].round(2)
        )

    def get_historical_missed_calls_by_hour(self):
        self.historical_missed_calls_by_hour_df = pd.read_csv(
            f"{self.historical_data_path}/historical_missed_calls_by_hour.csv"
        )

    def get_historical_monthly_resource_utilisation(self):
        self.historical_monthly_resource_utilisation = pd.read_csv(
            f"{self.historical_data_path}/historical_monthly_resource_utilisation.csv"
        )

    def get_historical_missed_calls_by_quarter_and_hour(
        self,
    ):
        self.historical_missed_calls_by_quarter_and_hour_df = pd.read_csv(
            f"{self.historical_data_path}/historical_missed_calls_by_quarter_and_hour.csv"
        )

    def get_historical_care_cat_counts(self):
        self.historical_care_cat_counts = pd.read_csv(
            f"{self.historical_data_path}/historical_care_cat_counts.csv"
        )

    def get_care_cat_by_hour_historic(self):
        self.care_cat_by_hour_historic = self.historical_care_cat_counts.copy()

        total_per_hour = self.care_cat_by_hour_historic.groupby("hour")[
            "count"
        ].transform("sum")

        # Add proportion column
        self.care_cat_by_hour_historic["proportion"] = (
            self.care_cat_by_hour_historic["count"] / total_per_hour
        )

    def get_historical_jobs_per_day_per_callsign(self):
        self.historical_jobs_per_day_per_callsign = pd.read_csv(
            f"{self.historical_data_path}/historical_jobs_per_day_per_callsign.csv"
        )

    def get_historical_jobs_per_month(self):
        self.historical_jobs_per_month = pd.read_csv(
            f"{self.historical_data_path}/historical_jobs_per_month.csv"
        )

        self.historical_jobs_per_month["month"] = pd.to_datetime(
            self.historical_jobs_per_month["month"], dayfirst=True
        )

    def get_historical_monthly_totals_by_callsign(self):
        self.historical_monthly_totals_by_callsign = pd.read_csv(
            f"{self.historical_data_path}/historical_monthly_totals_by_callsign.csv"
        )

    def get_historical_median_time_of_activities_by_month_and_resource_type(self):
        self.historical_median_time_of_activities_by_month_and_resource_type = pd.read_csv(
            f"{self.historical_data_path}/historical_median_time_of_activities_by_month_and_resource_type.csv",
            parse_dates=False,
        )

        # Parse month manually as more controllable
        self.historical_median_time_of_activities_by_month_and_resource_type[
            "month"
        ] = pd.to_datetime(
            self.historical_median_time_of_activities_by_month_and_resource_type[
                "month"
            ],
            format="%Y-%m-%d",
        )

    def get_historical_job_durations_breakdown(self):
        self.historical_job_durations_breakdown = pd.read_csv(
            f"{self.historical_data_path}/historical_job_durations_breakdown.csv"
        )

    ##################################################################
    # MARK: Methods for creating new dataframes from imported data   #
    ##################################################################

    def make_RWC_utilisation_dataframe(
        self,
    ):
        def calculate_theoretical_time(
            historical_df,
            rota_df,
            service_df,
            callsign_df,
            historical_monthly_resource_utilisation_df,
            long_format_df=True,
        ):
            # Pull in relevant rota, registration and servicing data
            rota_df = rota_df.merge(callsign_df, on="callsign")
            service_df = service_df.merge(callsign_df, on="registration")
            # print("==calculate_theoretical_time - rota_df after merging with callsign_df==")
            # print(rota_df)

            # Convert date columns to datetime format
            historical_monthly_resource_utilisation_df["month"] = pd.to_datetime(
                historical_monthly_resource_utilisation_df["month"]
            )

            # Create a dummy dataframe with every date in the range represented
            # We'll use this to make sure days with 0 activity get factored in to the calculations
            date_range = pd.date_range(
                start=historical_df["month"].min(),
                end=pd.offsets.MonthEnd().rollforward(historical_df["month"].max()),
                freq="D",
            )
            daily_df = pd.DataFrame({"date": date_range})

            # print("==historical_df==")
            # print(historical_df)

            service_df["service_start_date"] = pd.to_datetime(
                service_df["service_start_date"]
            )
            service_df["service_end_date"] = pd.to_datetime(
                service_df["service_end_date"]
            )

            def is_summer(date_obj):
                return current_date.month in [4, 5, 6, 7, 8, 9]

            # Initialize columns in df_availability for each unique callsign
            for callsign in rota_df["callsign"].unique():
                daily_df[callsign] = 0  # Initialize with 0 minutes

            daily_df = daily_df.set_index("date")

            # Iterate through each date in our availability dataframe
            for date_idx, current_date in enumerate(daily_df.index):
                is_current_date_summer = is_summer(current_date)

                # Iterate through each resource's rota entry
                for _, rota_entry in rota_df.iterrows():
                    callsign = rota_entry["callsign"]
                    start_hour_col = (
                        "summer_start" if is_current_date_summer else "winter_start"
                    )
                    end_hour_col = (
                        "summer_end" if is_current_date_summer else "winter_end"
                    )

                    start_hour = rota_entry[start_hour_col]
                    end_hour = rota_entry[end_hour_col]

                    # --- Calculate minutes for the current_date ---
                    minutes_for_callsign_on_date = 0

                    # Scenario 1: Shift is fully within one day (e.g., 7:00 to 19:00)
                    if start_hour < end_hour:
                        # Check if this shift is active on current_date (it always is in this logic,
                        # as we are calculating for the current_date based on its rota)
                        minutes_for_callsign_on_date = (end_hour - start_hour) * 60
                    # Scenario 2: Shift spans midnight (e.g., 19:00 to 02:00)
                    elif start_hour > end_hour:
                        # Part 1: Minutes from start_hour to midnight on current_date
                        minutes_today = (24 - start_hour) * 60
                        minutes_for_callsign_on_date += minutes_today

                        # Part 2: Minutes from midnight to end_hour on the *next* day
                        # These minutes need to be added to the *next day's* total for this callsign
                        if date_idx + 1 < len(
                            daily_df
                        ):  # Ensure there is a next day in our df
                            next_date = daily_df.index[date_idx + 1]
                            minutes_on_next_day = end_hour * 60
                            daily_df.loc[next_date, callsign] = (
                                daily_df.loc[next_date, callsign] + minutes_on_next_day
                            )

                    daily_df.loc[current_date, callsign] += minutes_for_callsign_on_date

            theoretical_availability = daily_df.copy().reset_index()
            theoretical_availability["month"] = theoretical_availability[
                "date"
            ].dt.strftime("%Y-%m-01")
            theoretical_availability = theoretical_availability.drop(columns=["date"])
            theoretical_availability = theoretical_availability.set_index("month")
            theoretical_availability = theoretical_availability.groupby("month").sum()
            theoretical_availability = theoretical_availability.reset_index()

            # print("==_utilisation_result_calculation.py - make_RWC_utilisation_dataframe - theoretical availability df==")
            # print(theoretical_availability_df)

            theoretical_availability.to_csv(
                "historical_data/calculated/theoretical_availability_historical.csv",
                index=False,
            )

            if long_format_df:
                theoretical_availability_df = theoretical_availability.melt(
                    id_vars="month"
                ).rename(
                    columns={
                        "value": "theoretical_availability",
                        "variable": "callsign",
                    }
                )

                theoretical_availability_df["theoretical_availability"] = (
                    theoretical_availability_df["theoretical_availability"].astype(
                        "float"
                    )
                )

                theoretical_availability_df = theoretical_availability_df.fillna(0.0)

            return theoretical_availability_df

        theoretical_availability_df = calculate_theoretical_time(
            historical_df=self.historical_monthly_resource_utilisation,
            rota_df=self.historic_rota_df,
            callsign_df=self.historic_callsign_df,
            service_df=self.historic_servicing_df,
            historical_monthly_resource_utilisation_df=self.historical_monthly_resource_utilisation,
            long_format_df=True,
        )

        # print("==theoretical_availability_df==")
        # print(theoretical_availability_df)
        theoretical_availability_df["month"] = pd.to_datetime(
            theoretical_availability_df["month"]
        )

        # theoretical_availability_df.to_csv("historical_data/calculated/theoretical_availability_historical.csv")

        historical_utilisation_df_times = (
            self.historical_monthly_resource_utilisation.set_index("month")
            .filter(like="total_time")
            .reset_index()
        )

        historical_utilisation_df_times.columns = [
            x.replace("total_time_", "")
            for x in historical_utilisation_df_times.columns
        ]

        historical_utilisation_df_times = historical_utilisation_df_times.melt(
            id_vars="month"
        ).rename(columns={"value": "usage", "variable": "callsign"})

        historical_utilisation_df_times = historical_utilisation_df_times.fillna(0)

        self.historical_utilisation_df_complete = pd.merge(
            left=historical_utilisation_df_times,
            right=theoretical_availability_df,
            on=["callsign", "month"],
            how="left",
        )

        self.historical_utilisation_df_complete["percentage_utilisation"] = (
            self.historical_utilisation_df_complete["usage"]
            / self.historical_utilisation_df_complete["theoretical_availability"]
        )

        self.historical_utilisation_df_complete["percentage_utilisation_display"] = (
            self.historical_utilisation_df_complete["percentage_utilisation"].apply(
                lambda x: f"{x:.1%}"
            )
        )

        self.historical_utilisation_df_complete.to_csv(
            "historical_data/calculated/complete_utilisation_historical.csv"
        )

        self.historical_utilisation_df_summary = (
            self.historical_utilisation_df_complete.groupby("callsign")[
                "percentage_utilisation"
            ].agg(["min", "max", "mean", "median"])
            * 100
        ).round(1)

        self.historical_utilisation_df_summary.to_csv(
            "historical_data/calculated/complete_utilisation_historical_summary.csv"
        )

    ############################
    # MARK: Plotting methods   #
    ############################
    def PLOT_historical_missed_jobs_data(self, format="stacked_bar") -> Figure:
        if format == "stacked_bar":
            return px.bar(
                self.historical_missed_calls_by_month[
                    ["month", "jobs_not_attended", "jobs_attended"]
                ].melt(id_vars="month"),
                x="month",
                y="value",
                color="variable",
            )

        elif format == "line_not_attended_count":
            return px.line(
                self.historical_missed_calls_by_month, x="month", y="jobs_not_attended"
            )

        elif format == "line_not_attended_perc":
            return px.line(
                self.historical_missed_calls_by_month,
                x="month",
                y="perc_unattended_historical",
            )

        elif format == "string":
            # This approach can distort the result by giving more weight to months with higher numbers of calls
            # However, for system-level performance, which is what we care about here, it's a reasonable option
            all_received_calls_period = self.historical_missed_calls_by_month[
                "all_received_calls"
            ].sum()
            all_attended_jobs_period = self.historical_missed_calls_by_month[
                "jobs_attended"
            ].sum()
            return (
                (all_received_calls_period - all_attended_jobs_period)
                / all_received_calls_period
            ) * 100

            # Alternative is to take the mean of means
            # return full_jobs_df['perc_unattended_historical'].mean()*100

        else:
            # Melt the DataFrame to long format
            df_melted = self.historical_missed_calls_by_month[
                ["month", "jobs_not_attended", "jobs_attended"]
            ].melt(id_vars="month")

            # Calculate proportions per month
            df_melted["proportion"] = df_melted.groupby("month")["value"].transform(
                lambda x: x / x.sum()
            )

            # Plot proportions
            fig = px.bar(
                df_melted,
                x="month",
                y="proportion",
                color="variable",
                text="value",  # Optional: to still show raw values on hover
            )

            fig.update_layout(
                barmode="stack", yaxis_tickformat=".0%", yaxis_title="Proportion"
            )
            fig.show()

    def PLOT_care_cat_counts_historic(self, show_proportions=False) -> Figure:
        title = "Care Category of calls in historical data by hour of day with EC/CC/Regular - Heli Benefit/Regular"

        if not show_proportions:
            fig = px.bar(
                self.care_cat_by_hour_historic,
                x="hour",
                y="count",
                color="care_category",
                title=title,
                category_orders={
                    "care_category": [
                        "CC",
                        "EC",
                        "REG - helicopter benefit",
                        "REG",
                        "Unknown - DAA resource did not attend",
                    ]
                },
            )
        else:
            fig = px.bar(
                self.care_cat_by_hour_historic,
                x="hour",
                y="proportion",
                color="care_category",
                title=title,
                category_orders={
                    "care_category": [
                        "CC",
                        "EC",
                        "REG - helicopter benefit",
                        "REG",
                        "Unknown - DAA resource did not attend",
                    ]
                },
            )

        fig.update_layout(xaxis=dict(dtick=1))

        return fig

    def PLOT_RWC_utilisation(self) -> Figure:
        fig = px.box(
            self.historical_utilisation_df_complete,
            x="percentage_utilisation",
            y="callsign",
        )

        return fig

    ##################################################################
    # MARK: Methods that return a figure/string for display          #
    ##################################################################
    def RETURN_missed_jobs_fig(self, care_category, what="average") -> str:
        row = self.SIM_hist_params_missed_jobs_care_cat_summary[
            (
                self.SIM_hist_params_missed_jobs_care_cat_summary["care_cat"]
                == care_category
            )
            & (
                self.SIM_hist_params_missed_jobs_care_cat_summary["time_type"]
                == "No Resource Available"
            )
        ]
        if what == "average":
            return row["jobs_per_year_average"].values[0]
        elif what == "min":
            return row["jobs_per_year_min"].values[0]
        elif what == "max":
            return row["jobs_per_year_max"].values[0]

    def RETURN_hist_util_fig(self, callsign="H70", average="mean") -> str:
        try:
            return self.historical_utilisation_df_summary[
                self.historical_utilisation_df_summary.index == callsign
            ][average].values[0]
        except IndexError:
            return f"Error returning value for callsign {callsign}. Available callsigns are {self.historical_utilisation_df_summary.index.unique()}"

    def RETURN_prediction_cc_patients_sent_ec_resource(self) -> tuple:
        row_of_interest = self.SIM_hist_suboptimal_care_cat_sent_summary[
            (
                self.SIM_hist_suboptimal_care_cat_sent_summary["hems_res_category"]
                != "CC"
            )
            & (self.SIM_hist_suboptimal_care_cat_sent_summary["care_cat"] == "CC")
        ]

        run_duration_days = float(get_param("sim_duration_days", self.run_params_used))

        return (
            (row_of_interest["mean"].values[0] / run_duration_days) * 365,
            (row_of_interest["min"].values[0] / run_duration_days) * 365,
            (row_of_interest["max"].values[0] / run_duration_days) * 365,
        )

    def RETURN_prediction_heli_benefit_patients_sent_car(self) -> tuple:
        row_of_interest = self.SIM_hist_suboptimal_vehicle_type_sent_summary[
            (
                self.SIM_hist_suboptimal_vehicle_type_sent_summary["vehicle_type"]
                == "car"
            )
            & (
                self.SIM_hist_suboptimal_vehicle_type_sent_summary["heli_benefit"]
                == "y"
            )
        ]

        run_duration_days = float(get_param("sim_duration_days", self.run_params_used))

        return (
            (row_of_interest["mean"].values[0] / run_duration_days) * 365,
            (row_of_interest["min"].values[0] / run_duration_days) * 365,
            (row_of_interest["max"].values[0] / run_duration_days) * 365,
        )
****************************************

****************************************
air_ambulance_des\class_patient.py
****************************************
from math import floor


class Patient:
    def __init__(self, p_id: int):
        # Unique ID for patient
        self.id = p_id

        # Incident specific data

        # Allocate triage code to patient
        # This might vary by time of day/year etc. so lookup table might be more complicated
        # or could use a simple regression?
        self.ampds_card = ""

        # Likely to be postcode sector i.e. BS1 9 of BS1 9HJ
        self.postcode = ""

        # incident location - latitude (2 decimal places)
        self.lat = ""
        self.long = ""

        # ? Include incident times here ?

        # Demographic data

        self.age = 0
        self.sex = "female"

        # print(f"AMPDS code is {self.triage_code} and prop_female is {prop_female} and sex is {self.sex}")

        # Keep track of cumulatative time
        self.time_in_sim = 0

        # Variables to keep track of time as the patient progresses through the model
        # These are updated at every step
        self.hour = 0
        self.day = "Mon"
        self.month = 1
        self.qtr = 1
        self.weekday = "weekday"
        self.current_dt = None  # TODO: Initialise as a dt?

        # HEMS/critical care specific items
        self.time_to_cc = 0
        self.cc_conveyed = 0
        self.cc_flown = 0
        self.cc_travelled_with = 0
        # Binary flag to indicate whether it is a 'HEMS job' whether they attend or not
        self.hems_case = -1
        # Binary flag to indicate whether patient cared for by HEMS or not
        self.hems = -1
        self.hems_result = ""
        self.hems_pref_vehicle_type = ""
        self.hems_pref_callsign_group = ""
        # Strings relating to allocated vehicle
        self.hems_vehicle_type = ""
        self.hems_callsign_group = ""
        self.hems_registration = ""
        self.hems_category = ""  # allocated vehicle registration
        self.callsign = ""  # allocated vehicle callsign

        self.pt_outcome = ""

        # Category to denote need for EC/CC or REG (regular) care
        self.hems_cc_or_ec = "REG"

        # Is the helicopter beneficial for this job?
        self.hems_helicopter_benefit = ""

        # Critical care desk staffed
        self.cc_desk = 0

        # Despatched by EOC outside of criteria
        # 0 = no, 1 = P1, 2 = P2, 3 = P3 maybe?
        self.dispatcher_intervention = 0

        self.time_to_first_respone = 0
****************************************

****************************************
air_ambulance_des\class_simulation_inputs.py
****************************************
import pandas as pd
from air_ambulance_des._processing_functions import graceful_methods


@graceful_methods
class SimulationInputs:
    def __init__(
        self, data_folder_path="../data", actual_data_folder_path="../actual_data"
    ):
        self.params_df_path = f"{data_folder_path}/run_params_used.csv"
        self.rota_path = f"{actual_data_folder_path}/HEMS_ROTA.csv"
        self.service_path = f"{data_folder_path}/service_dates.csv"
        self.callsign_path = (
            f"{actual_data_folder_path}/callsign_registration_lookup.csv"
        )
        self.rota_times = f"{actual_data_folder_path}/rota_start_end_months.csv"

        self.params_df = pd.read_csv(self.params_df_path)
        self.rota_df = pd.read_csv(self.rota_path)
        self.service_dates_df = pd.read_csv(self.service_path)
        self.callsign_registration_lookup_df = pd.read_csv(self.callsign_path)
        self.rota_start_end_months_df = pd.read_csv(self.rota_times)
****************************************

****************************************
air_ambulance_des\class_simulation_trial_results.py
****************************************
"""
File containing all calculations and visualisations relating to the number of jobs undertaken
in the simulation.
[x] Jobs across the course of the year
[x] Jobs across the course of the day
[x] Jobs by day of the week

[ ] Total number of jobs
[x] Total number of jobs by callsign
[ ] Total number of jobs by vehicle type
[ ] Total number of jobs by callsign group
[x] Jobs attended of those received (missed jobs)

Covers variation within the simulation, and comparison with real world data.

===

File containing all calculations and visualisations arelating to the patient and job outcomes/
results.

- Stand-down rates
- Patient outcomes

Covers variation within the simulation, and comparison with real world data.

===

File containing all calculations and visualisations arelating to the length of time
activities in the simulation take.

All of the below, split by vehicle type.
- Mobilisation Time
- Time to scene
- On-scene time
- Journey to hospital time
- Hospital to clear time
- Total job duration

Covers variation within the simulation, and comparison with real world data.

===

File containing all calculations and visualisations relating to the vehicles.

[] Simultaneous usage of different callsign groups
[] Total available hours
[] Servicing overrun
[] Instances of being unable to lift
[] Resource allocation hierarchies

Covers variation within the simulation, and comparison with real world data.

===

File containing all calculations and visualisations arelating to the patients.

[] Split by AMPDS card
[] Patient Ages
[] Patient Genders
[] Patient Ages by AMPDS
[] Patient Genders by AMPDS

Covers variation within the simulation, and comparison with real world data.


"""

import pandas as pd
import numpy as np
import re

import plotly.express as px
import plotly.graph_objects as go
from plotly.graph_objects import Figure
from plotly.subplots import make_subplots

import gc

import os
import sys

import datetime
from calendar import monthrange, day_name
import itertools
import textwrap

from scipy.stats import ks_2samp

import streamlit as st

from air_ambulance_des._processing_functions import (
    graceful_methods,
    get_param,
    fill_missing_values,
    calculate_time_difference,
)

sys.path.append(os.path.abspath(os.path.dirname(__file__)))
from air_ambulance_des.utils import (
    COLORSCHEME,
    q10,
    q90,
    q25,
    q75,
    format_sigfigs,
    iconMetricContainer,
)


@graceful_methods
class TrialResults:
    """
    Manages results from a model trial
    """

    def __init__(
        self,
        simulation_inputs,
        run_results: pd.DataFrame,
        daily_availability_data: pd.DataFrame,
        historical_data=None,
    ):
        self.run_results = run_results

        self.resource_use_events_only_df = self.run_results[
            self.run_results["event_type"].isin(["resource_use", "resource_use_end"])
        ].copy()

        self.historical_data = historical_data

        self.n_runs = len(self.run_results["run_number"].unique())

        self.simulation_inputs = simulation_inputs

        self.params_df = self.simulation_inputs.params_df

        self.call_df = None
        self.hourly_calls_per_run = None
        self.daily_calls_per_run = None

        self.theoretical_availability_df_long = None
        self.theoretical_availability_df = None
        self.total_avail_minutes = None

        # Placeholders for event duration dataframes
        self.simulation_event_duration_df = None
        self.simulation_event_duration_df_summary = None

        # Placeholders for utilisation dataframes
        # self.utilisation_model_df = None
        self.resource_use_wide = None
        self.utilisation_df_overall = None
        self.utilisation_df_per_run = None
        self.utilisation_df_per_run_by_csg = None

        # Placeholders for vehicle dataframes
        self.resource_allocation_outcomes_df = None

        self.missed_jobs_per_run_breakdown = None
        self.missed_jobs_per_run_care_cat_summary = None

        self.sim_averages_utilisation = None

        self.daily_availability_df = daily_availability_data

        self.event_counts_df = None
        self.event_counts_long = None

        # Set fixed order for months
        self.month_order = (
            pd.date_range("2000-01-01", periods=12, freq="MS").strftime("%B").tolist()
        )

        # Create a lookup dict to ensure formatting of weekday is consistent across simulated
        # and historical datasets
        self.day_dict = {
            "Mon": "Monday",
            "Tue": "Tuesday",
            "Wed": "Wednesday",
            "Thu": "Thursday",
            "Fri": "Friday",
            "Sat": "Saturday",
            "Sun": "Sunday",
        }

        # Create a list to ensure days of the week are displayed in the correct order in plots
        self.day_order = [
            "Monday",
            "Tuesday",
            "Wednesday",
            "Thursday",
            "Friday",
            "Saturday",
            "Sunday",
        ]

        # Add extra useful columns to run results
        self.enhance_run_results()
        # Add extra useful columns to the daily availability data
        self.update_daily_availability_df()
        # Create self.call_df
        self.make_job_count_df()
        # Create self.hourly_calls_per_run
        self.get_hourly_calls_per_run()
        # Create self.daily_calls_per_run
        self.get_daily_calls_per_run()
        # Create simulation event duration breakdown and summary
        self.create_simulation_event_duration_df()
        self.summarise_event_times()
        # Create event_counts_df and event_counts_long
        self.get_event_counts()
        # Create theoretical available resource time dataframe
        self.calculate_available_hours(
            summer_start=int(
                self.simulation_inputs.rota_start_end_months_df[
                    self.simulation_inputs.rota_start_end_months_df["what"]
                    == "summer_start_month"
                ]["month"].values[0]
            ),
            summer_end=int(
                self.simulation_inputs.rota_start_end_months_df[
                    self.simulation_inputs.rota_start_end_months_df["what"]
                    == "summer_end_month"
                ]["month"].values[0]
            ),
        )

        self.make_utilisation_model_dataframe()

        self.get_missed_call_df()
        self.prep_util_df_from_call_df()

    def enhance_run_results(self):
        self.run_results["timestamp_dt"] = pd.to_datetime(
            self.run_results["timestamp_dt"], format="ISO8601"
        )

        # Extract month from datetime
        self.run_results["month"] = self.run_results["timestamp_dt"].dt.strftime(
            "%B"
        )  # e.g., 'January', 'February'

        self.run_results["month"] = pd.Categorical(
            self.run_results["month"], categories=self.month_order, ordered=True
        )

        try:
            self.run_results = self.run_results.drop(columns=["Unnamed: 0"])
        except KeyError:
            pass

    def resource_allocation_outcomes(self):
        self.resource_allocation_outcomes_df = (
            (
                self.run_results[
                    self.run_results["event_type"] == "resource_preferred_outcome"
                ]
                .groupby(["time_type"])[["time_type"]]
                .count()
                / self.n_runs
            )
            .round(0)
            .astype("int")
            .rename(columns={"time_type": "Count"})
            .reset_index()
            .rename(columns={"time_type": "Resource Allocation Attempt Outcome"})
        ).copy()

    def update_daily_availability_df(self):
        self.daily_availability_df = self.daily_availability_df.melt(
            id_vars="month"
        ).rename(
            columns={
                "value": "theoretical_availability",
                "variable": "callsign",
            }
        )

    def calculate_available_hours(self, summer_start, summer_end):
        """
        Version of a function to calculate the number of hours a resource is available for use
        across the duration of the simulation, based on the rota used during the period, accounting
        for time during the simulation that uses the summer rota and time that uses the winter rota.

        Servicing is also taken into account.

        Warm up duration is taken into account.
        """
        # Convert data into DataFrames
        warm_up_end = get_param("warm_up_end_date", self.params_df)
        warm_up_end = datetime.datetime.strptime(warm_up_end, "%Y-%m-%d %H:%M:%S")

        sim_end = get_param("sim_end_date", self.params_df)
        sim_end = datetime.datetime.strptime(sim_end, "%Y-%m-%d %H:%M:%S")

        date_range = pd.date_range(
            start=warm_up_end.date(), end=sim_end.date(), freq="D"
        )
        daily_df = pd.DataFrame({"date": date_range})

        rota_df = self.simulation_inputs.rota_df
        service_df = self.simulation_inputs.service_dates_df

        callsign_df = self.simulation_inputs.callsign_registration_lookup_df

        rota_df = rota_df.merge(callsign_df, on="callsign")
        service_df = service_df.merge(callsign_df, on="registration")

        # Convert date columns to datetime format
        daily_df["date"] = pd.to_datetime(daily_df["date"])

        service_df["service_start_date"] = pd.to_datetime(
            service_df["service_start_date"]
        )
        service_df["service_end_date"] = pd.to_datetime(service_df["service_end_date"])

        def is_summer(date_obj):
            # return date_obj.month in [4,5,6,7,8,9]
            return date_obj.month in [i for i in range(summer_start, summer_end + 1)]

        # Initialize columns in df_availability for each unique callsign
        for callsign in rota_df["callsign"].unique():
            daily_df[callsign] = 0  # Initialize with 0 minutes

        daily_df = daily_df.set_index("date")

        # Iterate through each date in our availability dataframe
        for date_idx, current_date in enumerate(daily_df.index):
            is_current_date_summer = is_summer(current_date)

            # Iterate through each resource's rota entry
            for _, rota_entry in rota_df.iterrows():
                callsign = rota_entry["callsign"]
                start_hour_col = (
                    "summer_start" if is_current_date_summer else "winter_start"
                )
                end_hour_col = "summer_end" if is_current_date_summer else "winter_end"

                start_hour = rota_entry[start_hour_col]
                end_hour = rota_entry[end_hour_col]

                # --- Calculate minutes for the current_date ---
                minutes_for_callsign_on_date = 0

                # Scenario 1: Shift is fully within one day (e.g., 7:00 to 19:00)
                if start_hour < end_hour:
                    # Check if this shift is active on current_date (it always is in this logic,
                    # as we are calculating for the current_date based on its rota)
                    minutes_for_callsign_on_date = (end_hour - start_hour) * 60
                # Scenario 2: Shift spans midnight (e.g., 19:00 to 02:00)
                elif start_hour > end_hour:
                    # Part 1: Minutes from start_hour to midnight on current_date
                    minutes_today = (24 - start_hour) * 60
                    minutes_for_callsign_on_date += minutes_today

                    # Part 2: Minutes from midnight to end_hour on the *next* day
                    # These minutes need to be added to the *next day's* total for this callsign
                    if date_idx + 1 < len(
                        daily_df
                    ):  # Ensure there is a next day in our df
                        next_date = daily_df.index[date_idx + 1]
                        minutes_on_next_day = end_hour * 60

                        daily_df.loc[next_date, callsign] = (
                            daily_df.loc[next_date, callsign] + minutes_on_next_day
                        )

                daily_df.loc[current_date, callsign] += minutes_for_callsign_on_date

        self.theoretical_availability_df = daily_df
        self.theoretical_availability_df.index.name = "month"
        self.theoretical_availability_df = (
            self.theoretical_availability_df.reset_index()
        )

        self.theoretical_availability_df.fillna(0.0)

        self.theoretical_availability_df.to_csv(
            "data/daily_availability.csv", index=False
        )

        self.theoretical_availability_df["ms"] = self.theoretical_availability_df[
            "month"
        ].dt.strftime("%Y-%m-01")

        self.theoretical_availability_df.groupby("ms").sum(numeric_only=True).to_csv(
            "data/monthly_availability.csv"
        )

        self.theoretical_availability_df.drop(columns=["ms"], inplace=True)

        self.theoretical_availability_df_long = self.theoretical_availability_df.melt(
            id_vars="month"
        ).rename(columns={"value": "theoretical_availability", "variable": "callsign"})

        self.theoretical_availability_df_long["theoretical_availability"] = (
            self.theoretical_availability_df_long["theoretical_availability"].astype(
                "float"
            )
        )

        daily_available_minutes = self.theoretical_availability_df_long.copy()

        # print("==Daily Available Minutes==")
        # print(daily_available_minutes)

        self.total_avail_minutes = (
            daily_available_minutes.groupby("callsign")[["theoretical_availability"]]
            .sum(numeric_only=True)
            .reset_index()
            .rename(
                columns={"theoretical_availability": "total_available_minutes_in_sim"}
            )
        )

        self.total_avail_minutes["callsign_group"] = self.total_avail_minutes[
            "callsign"
        ].apply(lambda x: re.sub("\D", "", x))

        self.total_avail_minutes.to_csv(
            "data/daily_availability_summary.csv", index=False
        )

    def make_utilisation_model_dataframe(self):
        # Restrict to only events in the event log where resource use was starting or ending
        resource_use_only = self.run_results[
            self.run_results["event_type"].isin(["resource_use", "resource_use_end"])
        ].copy()

        # Pivot to wide-format dataframe with one row per patient/call
        # and columns for start and end types
        self.resource_use_wide = (
            resource_use_only[
                [
                    "P_ID",
                    "run_number",
                    "event_type",
                    "timestamp_dt",
                    "callsign_group",
                    "vehicle_type",
                    "callsign",
                ]
            ]
            .pivot(
                index=[
                    "P_ID",
                    "run_number",
                    "callsign_group",
                    "vehicle_type",
                    "callsign",
                ],
                columns="event_type",
                values="timestamp_dt",
            )
            .reset_index()
        )

        del resource_use_only

        # If utilisation end date is missing then set to end of model
        # as we can assume this is a call that didn't finish before the model did
        self.resource_use_wide = fill_missing_values(
            self.resource_use_wide,
            "resource_use_end",
            get_param("sim_end_date", self.params_df),
        )

        # If utilisation start time is missing, then set to start of model + warm-up time (if relevant)
        # as can assume this is a call that started before the warm-up period elapsed but finished
        # after the warm-up period elapsed
        # TODO: need to add in a check to ensure this only happens for calls at the end of the model,
        # not due to errors elsewhere that could fail to assign a resource end time
        self.resource_use_wide = fill_missing_values(
            self.resource_use_wide,
            "resource_use",
            get_param("warm_up_end_date", self.params_df),
        )

        # Calculate number of minutes the attending resource was in use on each call
        self.resource_use_wide["resource_use_duration"] = calculate_time_difference(
            self.resource_use_wide,
            "resource_use",
            "resource_use_end",
            unit="minutes",
        )

        # ============================================================ #
        # Calculage per-run utilisation,
        # stratified by callsign and vehicle type (car/helicopter)
        # ============================================================ #
        self.utilisation_df_per_run = self.resource_use_wide.groupby(
            ["run_number", "vehicle_type", "callsign"]
        )[["resource_use_duration"]].sum()

        # Join with df of how long each resource was available for in the sim
        # We will for now assume this is the same across each run
        self.utilisation_df_per_run = self.utilisation_df_per_run.reset_index(
            drop=False
        ).merge(self.total_avail_minutes, on="callsign", how="left")

        self.utilisation_df_per_run["perc_time_in_use"] = (
            self.utilisation_df_per_run["resource_use_duration"].astype(float)
            /
            # float(_processing_functions.get_param("sim_duration", params_df))
            self.utilisation_df_per_run["total_available_minutes_in_sim"].astype(float)
        )

        # Add column of nicely-formatted values to make printing values more streamlined
        self.utilisation_df_per_run["PRINT_perc"] = self.utilisation_df_per_run[
            "perc_time_in_use"
        ].apply(lambda x: f"{x:.1%}")

        # ============================================================ #
        # Calculage averge utilisation across simulation,
        # stratified by callsign group
        # ============================================================ #

        self.utilisation_df_per_run_by_csg = self.resource_use_wide.groupby(
            ["callsign_group"]
        )[["resource_use_duration"]].sum()

        self.utilisation_df_per_run_by_csg["resource_use_duration"] = (
            self.utilisation_df_per_run_by_csg["resource_use_duration"] / self.n_runs
        )

        self.utilisation_df_per_run_by_csg = (
            self.utilisation_df_per_run_by_csg.reset_index()
        )

        total_avail_minutes_per_csg = (
            self.total_avail_minutes.groupby("callsign_group")
            .head(1)
            .drop(columns="callsign")
        )

        total_avail_minutes_per_csg["callsign_group"] = total_avail_minutes_per_csg[
            "callsign_group"
        ].astype("float")

        self.utilisation_df_per_run_by_csg = self.utilisation_df_per_run_by_csg.merge(
            total_avail_minutes_per_csg, on="callsign_group", how="left"
        )

        self.utilisation_df_per_run_by_csg["perc_time_in_use"] = (
            self.utilisation_df_per_run_by_csg["resource_use_duration"].astype(float)
            /
            # float(_processing_functions.get_param("sim_duration", params_df))
            self.utilisation_df_per_run_by_csg["total_available_minutes_in_sim"].astype(
                float
            )
        )

        self.utilisation_df_per_run_by_csg["PRINT_perc"] = (
            self.utilisation_df_per_run_by_csg["perc_time_in_use"].apply(
                lambda x: f"{x:.1%}"
            )
        )

        # ============================================================ #
        # Calculage averge utilisation across simulation,
        # stratified by callsign and vehicle type (car/helicopter)
        # ============================================================ #
        self.utilisation_df_overall = self.utilisation_df_per_run.groupby(
            ["callsign", "vehicle_type"]
        )[["resource_use_duration"]].sum()

        self.utilisation_df_overall["resource_use_duration"] = (
            self.utilisation_df_overall["resource_use_duration"] / self.n_runs
        )

        self.utilisation_df_overall = self.utilisation_df_overall.reset_index(
            drop=False
        ).merge(self.total_avail_minutes, on="callsign", how="left")

        self.utilisation_df_overall["perc_time_in_use"] = (
            self.utilisation_df_overall["resource_use_duration"].astype(float)
            /
            # float(_processing_functions.get_param("sim_duration", params_df))
            self.utilisation_df_overall["total_available_minutes_in_sim"].astype(float)
        )

        # Add column of nicely-formatted values to make printing values more streamlined
        self.utilisation_df_overall["PRINT_perc"] = self.utilisation_df_overall[
            "perc_time_in_use"
        ].apply(lambda x: f"{x:.1%}")

    def make_job_count_df(self):
        """
        Given the event log produced by running the model, create a dataframe with one row per
        patient, but all pertinent information about each call added to that row if it would not
        usually be present until a later entry in the log
        """
        # hems_result and outcome columns aren't determined until a later step
        # backfill this per patient/run so we'll have access to it from the row for
        # the patient's arrival
        run_results_bfilled = self.run_results.copy()
        # print("==run results bfilled==")
        # print(run_results_bfilled.head())
        # print(run_results_bfilled.columns)

        if "P_ID" not in run_results_bfilled.columns:
            run_results_bfilled = run_results_bfilled.reset_index()

        run_results_bfilled["hems_result"] = run_results_bfilled.groupby(
            ["P_ID", "run_number"]
        ).hems_result.bfill()
        run_results_bfilled["outcome"] = run_results_bfilled.groupby(
            ["P_ID", "run_number"]
        ).outcome.bfill()
        # same for various things around allocated resource
        run_results_bfilled["vehicle_type"] = run_results_bfilled.groupby(
            ["P_ID", "run_number"]
        ).vehicle_type.bfill()
        run_results_bfilled["callsign"] = run_results_bfilled.groupby(
            ["P_ID", "run_number"]
        ).callsign.bfill()
        run_results_bfilled["registration"] = run_results_bfilled.groupby(
            ["P_ID", "run_number"]
        ).registration.bfill()

        # TODO - see what we can do about any instances where these columns remain NA
        # Think this is likely to relate to instances where there was no resource available?
        # Would be good to populate these columns with a relevant indicator if that's the case

        # Reduce down to just the 'arrival' row for each patient, giving us one row per patient
        # per run
        self.call_df = run_results_bfilled[
            run_results_bfilled["time_type"] == "arrival"
        ].drop(columns=["time_type", "event_type"])

        self.call_df.to_csv("data/call_df.csv", index=False)

        self.call_df["timestamp_dt"] = pd.to_datetime(
            self.call_df["timestamp_dt"], format="ISO8601"
        )
        self.call_df["month_start"] = (
            self.call_df["timestamp_dt"].dt.to_period("M").dt.to_timestamp()
        )

        self.call_df["day_date"] = pd.to_datetime(self.call_df["timestamp_dt"]).dt.date

    def get_daily_calls_per_run(self):
        # Calculate the number of calls per day across a full run
        self.daily_calls_per_run = (
            self.call_df.groupby(["day", "run_number"])[["P_ID"]]
            .count()
            .reset_index()
            .rename(columns={"P_ID": "count"})
        )

        # Ensure the days in the simulated dataset are formatted the same as the historical dataset
        self.daily_calls_per_run["day"] = self.daily_calls_per_run["day"].apply(
            lambda x: self.day_dict[x]
        )

    def get_calls_per_run(self):
        """
        Returns a series of the calls per simulation run
        """
        return self.call_df.groupby("run_number")[["P_ID"]].count().reset_index()

    def get_AVERAGE_calls_per_run(self):
        """
        Returns a count of the calls per run, averaged across all runs
        """
        calls_per_run = self.get_calls_per_run(self.call_df)
        return calls_per_run.mean()["P_ID"].round(2)

    def get_UNATTENDED_calls_per_run(self):
        """
        Returns a count of the unattended calls per run

        This is done by looking for any instances where no callsign was assigned, indicating that
        no resource was sent
        """
        return (
            self.call_df[self.call_df["callsign"].isna()]
            .groupby("run_number")[["P_ID"]]
            .count()
            .reset_index()
        )

    def get_AVERAGE_UNATTENDED_calls_per_run(self):
        """
        Returns a count of the calls per run, averaged across all runs

        This is done by looking for any instances where no callsign was assigned, indicating that
        no resource was sent
        """
        unattended_calls_per_run = self.get_UNATTENDED_calls_per_run(self.call_df)
        return unattended_calls_per_run.mean()["P_ID"].round(2)

    def display_UNTATTENDED_calls_per_run(self):
        """
        Alternative to get_perc_unattended_string(), using a different approach, allowing for
        robustness testing

        Here, this is done by looking for any instances where no callsign was assigned, indicating that
        no resource was sent
        """
        total_calls = self.get_AVERAGE_calls_per_run(self.call_df)
        unattended_calls = self.get_AVERAGE_UNATTENDED_calls_per_run(self.call_df)

        return f"{unattended_calls:.0f} of {total_calls:.0f} ({(unattended_calls / total_calls):.1%})"

    def get_hourly_calls_per_run(self):
        self.hourly_calls_per_run = (
            self.call_df.groupby(["hour", "run_number"])[["P_ID"]]
            .count()
            .reset_index()
            .rename(columns={"P_ID": "count"})
        )

    def PLOT_hourly_call_counts(
        self,
        box_plot=False,
        average_per_month=False,
        bar_colour="teal",
        title="Calls Per Hour",
        use_poppins=False,
        error_bar_colour="charcoal",
        show_error_bars_bar=True,
        show_historical=True,
    ) -> Figure:
        """
        Produces an interactive plot showing the number of calls that were received per hour in
        the simulation

        This can be compared with the processed historical data used to inform the simulation
        """

        fig = go.Figure()

        if show_historical:
            jobs_per_hour_historic = (
                self.historical_data.historical_monthly_totals_by_hour_of_day.copy()
            )

            jobs_per_hour_historic["month"] = pd.to_datetime(
                jobs_per_hour_historic["month"], format="ISO8601"
            )
            jobs_per_hour_historic["year_numeric"] = jobs_per_hour_historic[
                "month"
            ].apply(lambda x: x.year)
            jobs_per_hour_historic["month_numeric"] = jobs_per_hour_historic[
                "month"
            ].apply(lambda x: x.month)
            jobs_per_hour_historic_long = jobs_per_hour_historic.melt(
                id_vars=["month", "month_numeric", "year_numeric"]
            )
            # jobs_per_hour_historic_long["hour"] = jobs_per_hour_historic_long['variable'].str.extract(r"(\d+)\s")
            jobs_per_hour_historic_long.rename(
                columns={"variable": "hour"}, inplace=True
            )
            jobs_per_hour_historic_long["hour"] = jobs_per_hour_historic_long[
                "hour"
            ].astype("int")
            jobs_per_hour_historic_long = jobs_per_hour_historic_long[
                ~jobs_per_hour_historic_long["value"].isna()
            ]

            if not average_per_month:
                jobs_per_hour_historic_long["value"] = jobs_per_hour_historic_long[
                    "value"
                ] * (float(get_param("sim_duration", self.params_df)) / 60 / 24 / 30)

            jobs_per_hour_historic_agg = (
                jobs_per_hour_historic_long.groupby(["hour"])["value"].agg(
                    ["min", "max", q10, q90]
                )
            ).reset_index()

            fig.add_trace(
                go.Bar(
                    x=jobs_per_hour_historic_agg["hour"],
                    y=jobs_per_hour_historic_agg["max"]
                    - jobs_per_hour_historic_agg["min"],  # The range
                    base=jobs_per_hour_historic_agg["min"],  # Starts from the minimum
                    name="Historical Range",
                    marker_color="rgba(100, 100, 255, 0.2)",  # Light blue with transparency
                    hoverinfo="skip",  # Hide hover info for clarity
                    showlegend=True,
                    width=1.0,  # Wider bars to make them contiguous
                    offsetgroup="historical",  # Grouping ensures alignment
                )
            )

            fig.add_trace(
                go.Bar(
                    x=jobs_per_hour_historic_agg["hour"],
                    y=jobs_per_hour_historic_agg["q90"]
                    - jobs_per_hour_historic_agg["q10"],  # The range
                    base=jobs_per_hour_historic_agg["q10"],  # Starts from the minimum
                    name="Historical 80% Range",
                    marker_color="rgba(100, 100, 255, 0.3)",  # Light blue with transparency
                    hoverinfo="skip",  # Hide hover info for clarity
                    showlegend=True,
                    width=1.0,  # Wider bars to make them contiguous
                    offsetgroup="historical",  # Grouping ensures alignment
                )
            )

            fig.update_layout(
                xaxis=dict(dtick=1),
                barmode="overlay",  # Ensures bars overlay instead of stacking
                title="Comparison of Simulated and Historical Call Counts",
            )

        if box_plot:
            if average_per_month:
                self.hourly_calls_per_run["average_per_day"] = (
                    self.hourly_calls_per_run["count"]
                    / (float(get_param("sim_duration", self.params_df)) / 60 / 24 / 30)
                )
                y_column = "average_per_day"
                y_label = "Average Monthly Calls Per Hour Across Simulation<br>Averaged Across Simulation Runs"
            else:
                y_column = "count"
                y_label = "Total Calls Per Hour Across Simulation<br>Averaged Across Simulation Runs"

            # Add box plot trace
            fig.add_trace(
                go.Box(
                    x=self.hourly_calls_per_run["hour"],
                    y=self.hourly_calls_per_run[y_column],
                    name="Simulated Mean",
                    width=0.4,
                    marker=dict(color=COLORSCHEME[bar_colour]),
                    showlegend=True,
                    boxpoints="outliers",  # Show all data points
                )
            )

            # Update layout
            fig.update_layout(
                title=title,
                xaxis=dict(title="Hour", dtick=1),
                yaxis=dict(title=y_label),
            )

        else:
            # Create required dataframe for simulation output display
            aggregated_data = (
                self.hourly_calls_per_run.groupby("hour")
                .agg(
                    mean_count=("count", "mean"),
                    # std_count=("count", "std")
                    se_count=(
                        "count",
                        lambda x: x.std() / np.sqrt(len(x)),
                    ),  # Standard Error
                )
                .reset_index()
            )

            if show_error_bars_bar:
                # error_y = aggregated_data["std_count"]
                error_y = aggregated_data["se_count"]
            else:
                error_y = None

            if average_per_month:
                aggregated_data["mean_count"] = aggregated_data["mean_count"] / (
                    float(get_param("sim_duration", self.params_df)) / 60 / 24 / 30
                )
                # aggregated_data['std_count'] = aggregated_data['std_count'] / (float(_processing_functions.get_param("sim_duration", params_df))/60/24/ 30)
                aggregated_data["se_count"] = aggregated_data["se_count"] / (
                    float(get_param("sim_duration", self.params_df)) / 60 / 24 / 30
                )

                fig.add_trace(
                    go.Bar(
                        x=aggregated_data["hour"],
                        y=aggregated_data["mean_count"],
                        name="Simulated Mean",
                        marker=dict(
                            color=COLORSCHEME[bar_colour]
                        ),  # Use your color scheme
                        error_y=dict(type="data", array=error_y, visible=True)
                        if error_y is not None
                        else None,
                        width=0.4,  # Narrower bars in front
                        offsetgroup="simulated",
                    )
                )

                fig.update_layout(
                    xaxis=dict(dtick=1),
                    barmode="overlay",  # Ensures bars overlay instead of stacking
                    title=title,
                    yaxis_title="Average Monthly Calls Per Hour Across Simulation<br>Averaged Across Simulation Runs",
                    xaxis_title="Hour",
                )

            else:
                fig.add_trace(
                    go.Bar(
                        x=aggregated_data["hour"],
                        y=aggregated_data["mean_count"],
                        name="Simulated Mean",
                        marker=dict(
                            color=COLORSCHEME[bar_colour]
                        ),  # Use your color scheme
                        error_y=dict(type="data", array=error_y, visible=True)
                        if error_y is not None
                        else None,
                        width=0.4,  # Narrower bars in front
                        offsetgroup="simulated",
                    )
                )

                fig.update_layout(
                    xaxis=dict(dtick=1),
                    barmode="overlay",  # Ensures bars overlay instead of stacking
                    title=title,
                    yaxis_title="Total Calls Per Hour Across Simulation",
                    xaxis_title="Hour",
                )

        if not box_plot:
            fig = fig.update_traces(error_y_color=COLORSCHEME[error_bar_colour])

        if use_poppins:
            return fig.update_layout(
                font=dict(family="Poppins", size=18, color="black")
            )
        else:
            return fig

    ######
    # TODO: Update to pull historical data from hist data class
    #####
    def PLOT_monthly_calls(
        self,
        show_individual_runs=False,
        use_poppins=False,
        show_historical=False,
        show_historical_individual_years=False,
        job_count_col="total_jobs",
    ) -> Figure:
        call_counts_monthly = (
            self.call_df.groupby(["run_number", "month_start"])[["P_ID"]]
            .count()
            .reset_index()
            .rename(columns={"P_ID": "monthly_calls"})
        )

        # Identify first and last month in the dataset
        first_month = call_counts_monthly["month_start"].min()
        last_month = call_counts_monthly["month_start"].max()

        # Filter out the first and last month
        call_counts_monthly = call_counts_monthly[
            (call_counts_monthly["month_start"] != first_month)
            & (call_counts_monthly["month_start"] != last_month)
        ]

        # Compute mean of number of patients, standard deviation, and total number of monthly calls

        summary = (
            call_counts_monthly.groupby("month_start")["monthly_calls"]
            .agg(["mean", "std", "count", "max", "min", q10, q90])
            .reset_index()
        )

        # Create the plot
        fig = px.line(
            summary,
            x="month_start",
            y="mean",
            markers=True,
            labels={"mean": "Average Calls Per Month", "month_start": "Month"},
            title="Number of Monthly Calls Received in Simulation",
            color_discrete_sequence=[COLORSCHEME["navy"]],
        ).update_traces(line=dict(width=2.5))

        if show_individual_runs:
            # Get and reverse the list of runs as plotting in reverse will give a more logical
            # legend at the end
            run_numbers = list(call_counts_monthly["run_number"].unique())
            run_numbers.sort()
            run_numbers.reverse()

            for run in run_numbers:
                run_data = call_counts_monthly[call_counts_monthly["run_number"] == run]
                fig.add_trace(
                    go.Scatter(
                        x=run_data["month_start"],
                        y=run_data["monthly_calls"],
                        mode="lines",
                        line=dict(color="gray", width=2, dash="dot"),
                        opacity=0.6,
                        name=f"Simulation Run {run}",
                        showlegend=True,
                    )
                )

        # Add full range as a shaded region
        fig.add_traces(
            [
                go.Scatter(
                    x=summary["month_start"],
                    y=summary["max"],
                    mode="lines",
                    line=dict(width=0),
                    showlegend=False,
                ),
                go.Scatter(
                    x=summary["month_start"],
                    y=summary["min"],
                    mode="lines",
                    fill="tonexty",
                    line=dict(width=0),
                    fillcolor="rgba(0, 176, 185, 0.15)",
                    showlegend=True,
                    name="Full Range Across Simulation Runs",
                ),
            ]
        )

        # Add 10th-90th percentile interval as a shaded region
        fig.add_traces(
            [
                go.Scatter(
                    x=summary["month_start"],
                    y=summary["q90"],
                    mode="lines",
                    line=dict(width=0),
                    showlegend=False,
                ),
                go.Scatter(
                    x=summary["month_start"],
                    y=summary["q10"],
                    mode="lines",
                    fill="tonexty",
                    line=dict(width=0),
                    fillcolor="rgba(0, 176, 185, 0.3)",
                    showlegend=True,
                    name="80% Range Across Simulation Runs",
                ),
            ]
        )

        # Increase upper y limit to be sightly bigger than the max number of calls observed in a month
        # Ensure lower y limit is 0
        fig = fig.update_yaxes(
            {"range": (0, call_counts_monthly["monthly_calls"].max() * 1.1)}
        )

        if show_historical:
            # Convert to datetime
            # (using 'parse_dates=True' in read_csv isn't reliably doing that, so make it explicit here)
            historical_jobs_per_month = (
                self.historical_data.historical_monthly_totals_all_calls.copy()
            )

            historical_summary = (
                historical_jobs_per_month.groupby("Month_Numeric")[job_count_col]
                .agg(["max", "min"])
                .reset_index()
                .rename(columns={"max": "historic_max", "min": "historic_min"})
            )

            call_counts_monthly["Month_Numeric"] = call_counts_monthly[
                "month_start"
            ].apply(lambda x: x.month)

            # historical_jobs_per_month["New_Date"] = (
            #     historical_jobs_per_month["Month"]
            #     .apply(lambda x: datetime.date(year=first_month.year,day=1,month=x.month))
            #     )

            if (historical_jobs_per_month[job_count_col].max() * 1.1) > (
                call_counts_monthly["monthly_calls"].max() * 1.1
            ):
                fig = fig.update_yaxes(
                    {"range": (0, historical_jobs_per_month[job_count_col].max() * 1.1)}
                )

            if show_historical_individual_years:
                for idx, year in enumerate(
                    historical_jobs_per_month["Year_Numeric"].unique()
                ):
                    # Filter the data for the current year
                    year_data = historical_jobs_per_month[
                        historical_jobs_per_month["Year_Numeric"] == year
                    ]

                    new_df = call_counts_monthly.drop_duplicates("month_start").merge(
                        year_data, on="Month_Numeric", how="left"
                    )

                    # Add the trace for the current year
                    fig.add_trace(
                        go.Scatter(
                            x=new_df["month_start"],
                            y=new_df[job_count_col],
                            mode="lines+markers",
                            opacity=0.7,
                            name=str(year),  # Using the year as the trace name
                            line=dict(
                                color=list(COLORSCHEME.values())[idx], dash="dash"
                            ),  # Default to gray if no specific color found
                        )
                    )
            else:
                # Add a filled range showing the entire historical range
                call_counts_monthly = call_counts_monthly.merge(
                    historical_summary, on="Month_Numeric", how="left"
                )

                # Add a filled range (shaded area) for the historical range
                # print("==_job_count_calculation plot_monthly_calls(): call_counts_monthly")
                # print(call_counts_monthly)

                # Ensure we only have one row per month to avoid issues with filling the historical range
                call_counts_historical_plotting_min_max = call_counts_monthly[
                    ["month_start", "historic_max", "historic_min"]
                ].drop_duplicates()

                fig.add_trace(
                    go.Scatter(
                        x=call_counts_historical_plotting_min_max["month_start"],
                        y=call_counts_historical_plotting_min_max["historic_max"],
                        mode="lines",
                        showlegend=False,
                        line=dict(
                            color="rgba(0,0,0,0)"
                        ),  # Invisible line (just the area)
                    )
                )

                fig.add_trace(
                    go.Scatter(
                        x=call_counts_historical_plotting_min_max["month_start"],
                        y=call_counts_historical_plotting_min_max["historic_min"],
                        mode="lines",
                        name="Historical Range",
                        line=dict(
                            color="rgba(0,0,0,0)"
                        ),  # Invisible line (just the area)
                        fill="tonexty",  # Fill the area between this and the next trace
                        fillcolor="rgba(255, 164, 0, 0.15)",  # Semi-transparent fill color
                    )
                )

        # Adjust font to match DAA style
        if use_poppins:
            return fig.update_layout(
                font=dict(family="Poppins", size=18, color="black")
            )
        else:
            return fig

    def count_weekdays_in_month(self, year, month):
        """Returns a dictionary with the count of each weekday in a given month."""
        weekday_counts = {day: 0 for day in day_name}

        _, num_days = monthrange(year, month)  # Get total days in month
        for day in range(1, num_days + 1):
            weekday = day_name[datetime.datetime(year, month, day).weekday()]
            weekday_counts[weekday] += 1

        return weekday_counts

    def compute_average_calls(self, df):
        """Computes the average calls received per day of the week for each month."""
        results = []
        for _, row in df.iterrows():
            year, month = row["month"].year, row["month"].month
            weekday_counts = self.count_weekdays_in_month(year, month)

            averages = {day: row[day] / weekday_counts[day] for day in weekday_counts}
            averages["month"] = row["month"].strftime("%Y-%m")
            results.append(averages)

        return pd.DataFrame(results)

    ######
    # TODO: Update to pull historical data from hist data class
    #####
    def PLOT_daily_call_counts(
        self,
        box_plot=False,
        average_per_month=False,
        bar_colour="teal",
        title="Calls Per Day",
        use_poppins=False,
        error_bar_colour="charcoal",
        show_error_bars_bar=True,
        show_historical=True,
    ) -> Figure:
        # Create a blank figure to build on
        fig = go.Figure()

        ###########
        # Add historical data if option selected
        ###########
        if show_historical:
            jobs_per_day_historic = (
                self.historical_data.historical_monthly_totals_by_day_of_week.copy()
            )

            # print("===========jobs_per_day_historic============")
            # print(jobs_per_day_historic)
            # Compute the average calls per day
            jobs_per_day_historic = self.compute_average_calls(jobs_per_day_historic)
            jobs_per_day_historic["month"] = pd.to_datetime(
                jobs_per_day_historic["month"], format="ISO8601"
            )

            jobs_per_day_historic["year_numeric"] = jobs_per_day_historic[
                "month"
            ].apply(lambda x: x.year)
            jobs_per_day_historic["month_numeric"] = jobs_per_day_historic[
                "month"
            ].apply(lambda x: x.month)
            # print("======== jobs_per_day_historic - updated ========")
            # print(jobs_per_day_historic)

            jobs_per_day_historic_long = jobs_per_day_historic.melt(
                id_vars=["month", "month_numeric", "year_numeric"]
            )
            # jobs_per_day_historic_long["hour"] = jobs_per_day_historic_long['variable'].str.extract(r"(\d+)\s")
            jobs_per_day_historic_long.rename(columns={"variable": "day"}, inplace=True)
            # jobs_per_day_historic_long["hour"] = jobs_per_day_historic_long["hour"].astype('int')
            jobs_per_day_historic_long = jobs_per_day_historic_long[
                ~jobs_per_day_historic_long["value"].isna()
            ]

            if not average_per_month:
                jobs_per_day_historic_long["value"] = jobs_per_day_historic_long[
                    "value"
                ] * (float(get_param("sim_duration", self.params_df)) / 60 / 24 / 7)

            jobs_per_day_historic_agg = (
                jobs_per_day_historic_long.groupby(["day"])["value"].agg(
                    ["min", "max", q10, q90]
                )
            ).reset_index()

            fig.add_trace(
                go.Bar(
                    x=jobs_per_day_historic_agg["day"],
                    y=jobs_per_day_historic_agg["max"]
                    - jobs_per_day_historic_agg["min"],  # The range
                    base=jobs_per_day_historic_agg["min"],  # Starts from the minimum
                    name="Historical Range",
                    marker_color="rgba(100, 100, 255, 0.2)",  # Light blue with transparency
                    hoverinfo="skip",  # Hide hover info for clarity
                    showlegend=True,
                    width=1.0,  # Wider bars to make them contiguous
                    offsetgroup="historical",  # Grouping ensures alignment
                )
            )

            fig.add_trace(
                go.Bar(
                    x=jobs_per_day_historic_agg["day"],
                    y=jobs_per_day_historic_agg["q90"]
                    - jobs_per_day_historic_agg["q10"],  # The range
                    base=jobs_per_day_historic_agg["q10"],  # Starts from the minimum
                    name="Historical 80% Range",
                    marker_color="rgba(100, 100, 255, 0.3)",  # Light blue with transparency
                    hoverinfo="skip",  # Hide hover info for clarity
                    showlegend=True,
                    width=1.0,  # Wider bars to make them contiguous
                    offsetgroup="historical",  # Grouping ensures alignment
                )
            )

            fig.update_layout(
                xaxis=dict(dtick=1),
                barmode="overlay",  # Ensures bars overlay instead of stacking
                title="Comparison of Simulated and Historical Call Counts",
            )

        ################################
        # Add in the actual data
        ################################

        if box_plot:
            if average_per_month:
                self.daily_calls_per_run["average_per_month"] = (
                    self.daily_calls_per_run["count"]
                    / (float(get_param("sim_duration", self.params_df)) / 60 / 24 / 7)
                )
                y_column = "average_per_month"
                y_label = "Average Monthly Calls Per Day Across Simulation<br>Averaged Across Simulation Runs"
            else:
                y_column = "count"
                y_label = "Total Calls Per Hour Day Simulation<br>Averaged Across Simulation Runs"

            # Add box plot trace
            fig.add_trace(
                go.Box(
                    x=self.daily_calls_per_run["day"],
                    y=self.daily_calls_per_run[y_column],
                    name="Simulated Mean",
                    width=0.4,
                    marker=dict(color=COLORSCHEME[bar_colour]),
                    showlegend=True,
                    boxpoints="outliers",  # Show all data points
                )
            )

            # Update layout
            fig.update_layout(
                title=title, xaxis=dict(title="Day", dtick=1), yaxis=dict(title=y_label)
            )

        else:
            # Create required dataframe for simulation output display
            aggregated_data = (
                self.daily_calls_per_run.groupby("day")
                .agg(
                    mean_count=("count", "mean"),
                    # std_count=("count", "std")
                    se_count=(
                        "count",
                        lambda x: x.std() / np.sqrt(len(x)),
                    ),  # Standard Error
                )
                .reset_index()
            )

            if show_error_bars_bar:
                # error_y = aggregated_data["std_count"]
                error_y = aggregated_data["se_count"]
            else:
                error_y = None

            # Add the bar trace if plotting averages across
            if average_per_month:
                aggregated_data["mean_count"] = aggregated_data["mean_count"] / (
                    float(get_param("sim_duration", self.params_df)) / 60 / 24 / 7
                )
                # aggregated_data['std_count'] = (
                #     aggregated_data['std_count'] /
                #     (float(_processing_functions.get_param("sim_duration", params_df))/ 60 / 24/ 7)
                #     )

                aggregated_data["se_count"] = aggregated_data["se_count"] / (
                    float(get_param("sim_duration", self.params_df)) / 60 / 24 / 7
                )

                fig.add_trace(
                    go.Bar(
                        x=aggregated_data["day"],
                        y=aggregated_data["mean_count"],
                        name="Simulated Mean",
                        marker=dict(
                            color=COLORSCHEME[bar_colour]
                        ),  # Use your color scheme
                        error_y=dict(type="data", array=error_y, visible=True)
                        if error_y is not None
                        else None,
                        width=0.4,  # Narrower bars in front
                        offsetgroup="simulated",
                    )
                )

                fig.update_layout(
                    xaxis=dict(dtick=1),
                    barmode="overlay",  # Ensures bars overlay instead of stacking
                    title=title,
                    yaxis_title="Average Monthly Calls Per Day Across Simulation<br>Averaged Across Simulation Runs",
                    xaxis_title="Day",
                )

            # Add the bar trace if plotting total calls over the course of the simulation
            else:
                fig.add_trace(
                    go.Bar(
                        x=aggregated_data["day"],
                        y=aggregated_data["mean_count"],
                        name="Simulated Mean",
                        marker=dict(
                            color=COLORSCHEME[bar_colour]
                        ),  # Use your color scheme
                        error_y=dict(type="data", array=error_y, visible=True)
                        if error_y is not None
                        else None,
                        width=0.4,  # Narrower bars in front
                        offsetgroup="simulated",
                    )
                )

                fig.update_layout(
                    xaxis=dict(dtick=1),
                    barmode="overlay",  # Ensures bars overlay instead of stacking
                    title=title,
                    yaxis_title="Total Calls Per Day Across Simulation",
                    xaxis_title="Day",
                )

        if not box_plot:
            fig = fig.update_traces(error_y_color=COLORSCHEME[error_bar_colour])

        fig.update_xaxes(categoryorder="array", categoryarray=self.day_order)

        if use_poppins:
            return fig.update_layout(
                font=dict(family="Poppins", size=18, color="black")
            )
        else:
            return fig

    def PLOT_missed_jobs(
        self,
        show_proportions_per_hour=False,
        by_quarter=False,
    ) -> Figure:
        simulated_df_resource_preferred_outcome = self.run_results[
            self.run_results["event_type"] == "resource_preferred_outcome"
        ].copy()

        simulated_df_resource_preferred_outcome["outcome_simplified"] = (
            simulated_df_resource_preferred_outcome["time_type"].apply(
                lambda x: "No HEMS available"
                if "No HEMS resource available" in x
                else "HEMS (helo or car) available and sent"
            )
        )

        # Set up historical dataframe
        if not by_quarter:
            historical_df = (
                self.historical_data.historical_missed_calls_by_hour_df.copy()
            )

        else:
            historical_df = self.historical_data.historical_missed_calls_by_quarter_and_hour_df.copy()

        # Regardless of whether grouping by quarter or not, do some basic wrangling
        historical_df.rename(
            columns={"callsign_group_simplified": "outcome_simplified"},
            inplace=True,
        )
        historical_df["what"] = "Historical"

        # Now branch off for distinctive data wrangling
        if not by_quarter:
            simulated_df_counts = (
                simulated_df_resource_preferred_outcome.groupby(
                    ["outcome_simplified", "hour"]
                )[["P_ID"]]
                .count()
                .reset_index()
                .rename(columns={"P_ID": "count"})
            )
            simulated_df_counts["what"] = "Simulated"

            full_df = pd.concat([simulated_df_counts, historical_df])

            if not show_proportions_per_hour:
                fig = px.bar(
                    full_df,
                    x="hour",
                    y="count",
                    color="outcome_simplified",
                    barmode="stack",
                    facet_row="what",
                    facet_row_spacing=0.2,
                    labels={
                        "outcome_simplified": "Job Outcome",
                        "count": "Count of Jobs",
                        "hour": "Hour",
                    },
                )

                # Allow each y-axis to be independent
                fig.update_yaxes(matches=None)

                # Move facet row labels above each subplot, aligned left
                fig.for_each_annotation(
                    lambda a: a.update(
                        text=a.text.split("=")[-1],  # remove 'what='
                        x=0,  # align left
                        xanchor="left",
                        y=a.y + 0.35,  # move label above the plot
                        yanchor="top",
                        textangle=0,  # horizontal
                        font=dict(size=24),
                    )
                )

                # Ensure x axis tick labels appear on both facets
                fig.for_each_xaxis(
                    lambda xaxis: xaxis.update(
                        showticklabels=True, tickmode="linear", tick0=0, dtick=1
                    )
                )

                # Increase top margin to prevent overlap
                fig.update_layout(margin=dict(t=100))

                return fig
            else:
                # Compute proportions within each hour + source
                df_prop = (
                    full_df.groupby(["hour", "what"])
                    .apply(lambda d: d.assign(proportion=d["count"] / d["count"].sum()))
                    .reset_index(drop=True)
                )

                fig = px.bar(
                    df_prop,
                    x="what",
                    y="proportion",
                    color="outcome_simplified",
                    barmode="stack",
                    facet_col="hour",
                    category_orders={"hour": sorted(full_df["hour"].unique())},
                    title="Proportion of HEMS Outcomes by Hour and Data Source",
                    labels={"proportion": "Proportion", "what": ""},
                )

                fig.update_yaxes(range=[0, 1], matches="y")  # consistent y-axis
                fig.for_each_annotation(
                    lambda a: a.update(text=a.text.split("=")[-1])
                )  # clean facet labels

                fig.update_layout(
                    hovermode="x unified",
                    legend_title_text="Outcome",
                )

                fig.update_layout(
                    legend=dict(
                        orientation="h",  # horizontal layout
                        yanchor="bottom",
                        y=1.12,  # a bit above the plot
                        xanchor="center",
                        x=0.5,  # center aligned
                    )
                )

                # Increase spacing below title
                fig.update_layout(margin=dict(t=150))

                fig.for_each_xaxis(
                    lambda axis: axis.update(
                        # Rotate labels
                        tickangle=90,
                        # Force display of both labels even on narrow screens
                        showticklabels=True,
                        tickmode="linear",
                        tick0=0,
                        dtick=1,
                    )
                )

                return fig
        # if by_quarter
        else:
            simulated_df_resource_preferred_outcome.rename(
                columns={"qtr": "quarter"}, inplace=True
            )

            simulated_df_counts = (
                simulated_df_resource_preferred_outcome.groupby(
                    ["outcome_simplified", "quarter", "hour"]
                )[["P_ID"]]
                .count()
                .reset_index()
                .rename(columns={"P_ID": "count"})
            )
            simulated_df_counts["what"] = "Simulated"
            full_df = pd.concat([simulated_df_counts, historical_df])

            if not show_proportions_per_hour:
                fig = px.bar(
                    full_df,
                    x="hour",
                    y="count",
                    color="outcome_simplified",
                    barmode="stack",
                    facet_row="what",
                    facet_col="quarter",
                    facet_row_spacing=0.2,
                    labels={
                        "outcome_simplified": "Job Outcome",
                        "count": "Count of Jobs",
                        "hour": "Hour",
                    },
                )

                # Allow each y-axis to be independent
                fig.update_yaxes(matches=None)

                fig.for_each_xaxis(
                    lambda xaxis: xaxis.update(
                        showticklabels=True, tickmode="linear", tick0=0, dtick=1
                    )
                )

                # Increase top margin to prevent overlap
                fig.update_layout(margin=dict(t=100))

                fig.update_layout(
                    legend=dict(
                        orientation="h",  # horizontal layout
                        yanchor="bottom",
                        y=1.12,  # a bit above the plot
                        xanchor="center",
                        x=0.5,  # center aligned
                    )
                )

                return fig

            else:
                # Step 1: Compute proportions within each hour + source
                df_prop = (
                    full_df.groupby(["quarter", "hour", "what"])
                    .apply(lambda d: d.assign(proportion=d["count"] / d["count"].sum()))
                    .reset_index(drop=True)
                )

                # Step 2: Plot
                fig = px.bar(
                    df_prop,
                    x="what",
                    y="proportion",
                    color="outcome_simplified",
                    barmode="stack",
                    facet_col="hour",
                    facet_row="quarter",
                    category_orders={"hour": sorted(full_df["hour"].unique())},
                    title="Proportion of HEMS Outcomes by Hour and Data Source",
                    labels={"proportion": "Proportion", "what": ""},
                )

                fig.update_yaxes(range=[0, 1], matches="y")  # consistent y-axis
                fig.for_each_annotation(
                    lambda a: a.update(text=a.text.split("=")[-1])
                )  # clean facet labels

                fig.update_layout(
                    hovermode="x unified",
                    legend_title_text="Outcome",
                )

                fig.update_layout(
                    legend=dict(
                        orientation="h",  # horizontal layout
                        yanchor="bottom",
                        y=1.12,  # a bit above the plot
                        xanchor="center",
                        x=0.5,  # center aligned
                    )
                )

                fig.update_layout(margin=dict(t=150))

                fig.for_each_xaxis(
                    lambda axis: axis.update(
                        tickangle=90,
                        # showticklabels=True,
                        tickmode="linear",
                        tick0=0,
                        dtick=1,
                    )
                )

                return fig

    def PLOT_job_count_heatmap(
        self, normalise_per_day=False, simulated_days=None
    ) -> Figure:
        run_results = self.run_results[
            self.run_results["event_type"] == "resource_use"
        ][["P_ID", "run_number", "hour", "time_type"]].copy()

        # Unique values
        hours = list(range(24))  # 0–23 hours
        callsigns = run_results["time_type"].unique()

        # Cartesian product of all combinations to ensure we have a value on the x axis
        # for the hours with no data in
        full_index = pd.MultiIndex.from_product(
            [callsigns, hours], names=["time_type", "hour"]
        )

        # # # Group by callsign and hour, and count the number of occurrences
        heatmap_data = (
            run_results.groupby(["time_type", "hour"])
            .size()
            .reindex(full_index, fill_value=0)
            .reset_index(name="count")
        )

        # Normalise by number of runs
        heatmap_data["count"] = heatmap_data["count"] / len(
            run_results["run_number"].unique()
        )

        if normalise_per_day and simulated_days is not None:
            heatmap_data["count"] = heatmap_data["count"] / simulated_days

        # Create pivot table for heatmap matrix
        pivot = heatmap_data.pivot(
            index="time_type", columns="hour", values="count"
        ).fillna(0)

        fig = go.Figure(
            data=go.Heatmap(
                z=pivot.values,
                x=[str(h) for h in pivot.columns],
                y=pivot.index,
                colorscale="Blues",
                colorbar=dict(title="Count"),
                showscale=True,
                xgap=1,  # Gap between columns (simulates vertical borders)
                ygap=1,  # Gap between rows (simulates horizontal borders)
            )
        )

        if normalise_per_day and simulated_days:
            title = "Heatmap of Average Hourly Simulated Calls Responded to by Hour and Callsign"
        else:
            title = "Heatmap of Total Hourly Simulated Calls Responded to by Hour and Callsign (Run Average)"

        fig.update_layout(
            title=title,
            xaxis_title="Hour of Day",
            yaxis_title="Callsign",
            height=600,
            xaxis=dict(dtick=1),
        )

        return fig

    def PLOT_job_count_heatmap_monthly(
        self, normalise_per_day=False, simulated_days=None
    ) -> Figure:
        run_results = self.run_results[
            self.run_results["event_type"] == "resource_use"
        ][["P_ID", "run_number", "hour", "timestamp_dt", "time_type", "month"]].copy()

        # Prepare full grid of (month, hour, callsign)
        hours = list(range(24))
        months = self.month_order
        callsigns = sorted(run_results["time_type"].unique())

        full_index = pd.MultiIndex.from_product(
            [months, hours, callsigns], names=["month", "hour", "time_type"]
        )

        grouped = (
            run_results.groupby(["month", "hour", "time_type"], observed=False)
            .size()
            .reindex(full_index, fill_value=0)
            .reset_index(name="count")
        )

        # Normalize
        grouped["count"] = grouped["count"] / run_results["run_number"].nunique()
        if normalise_per_day and simulated_days:
            grouped["count"] = grouped["count"] / simulated_days

        # Pivot to 2D arrays for each callsign
        subplot_data = {
            callsign: grouped[grouped["time_type"] == callsign]
            .pivot(index="month", columns="hour", values="count")
            .reindex(index=self.month_order)  # Ensure full month order
            .fillna(0)
            for callsign in callsigns
        }

        # Create subplots
        fig = make_subplots(
            rows=len(callsigns),
            cols=1,
            shared_xaxes=True,
            shared_yaxes=False,
            subplot_titles=[f"Callsign: {c}" for c in callsigns],
            vertical_spacing=0.1,
        )

        for i, callsign in enumerate(callsigns, start=1):
            z = subplot_data[callsign].values
            x = [str(h) for h in subplot_data[callsign].columns]
            y = subplot_data[callsign].index.tolist()

            fig.add_trace(
                go.Heatmap(
                    z=z,
                    x=x,
                    y=y,
                    colorscale="Blues",
                    colorbar=dict(title="Count") if i == 1 else None,
                    showscale=(i == 1),
                    xgap=1,
                    ygap=1,
                ),
                row=i,
                col=1,
            )

        if normalise_per_day and simulated_days:
            title = "Average Hourly Job Count by Month and Callsign"
        else:
            title = "Total Hourly Job Count Across Simulated Period by Month and Callsign (Run Average)"

        fig.update_layout(
            height=350 * len(callsigns),
            title=title,
            # xaxis_title='Hour of Day',
            yaxis_title="Month",
        )

        fig.update_yaxes(autorange="reversed")

        # Make x-axis labels show on all subplots
        for i in range(1, len(callsigns) + 1):
            fig.update_xaxes(
                showticklabels=True, dtick=1, title_text="Hour of Day", row=i, col=1
            )

        return fig

    def PLOT_jobs_per_callsign(self):
        # Create a count of the number of days in the sim that each resource had that many jobs
        # i.e. how many days did CC70 have 0 jobs, 1 job, 2 jobs, etc.
        df = self.run_results.copy()

        df["date"] = pd.to_datetime(df["timestamp_dt"], format="ISO8601").dt.date
        all_counts = (
            df[df["event_type"] == "resource_use"]
            .groupby(["time_type", "date", "run_number"])["P_ID"]
            .count()
            .reset_index()
        )
        all_counts.rename(columns={"P_ID": "jobs_in_day"}, inplace=True)

        # We must assume any missing day in our initial count df is a 0 count
        # So generate a df with every possible combo
        all_combinations = pd.DataFrame(
            list(
                itertools.product(
                    all_counts["time_type"].unique(),
                    df["date"].unique(),
                    df["run_number"].unique(),
                )
            ),
            columns=["time_type", "date", "run_number"],
        )
        # Join this in
        merged = all_combinations.merge(
            all_counts, on=["time_type", "date", "run_number"], how="left"
        )
        # Fill na values with 0
        merged["jobs_in_day"] = merged["jobs_in_day"].fillna(0).astype(int)
        # Finally transform into pure counts
        sim_count_df = (
            merged.groupby(["time_type", "jobs_in_day"])[["date"]]
            .count()
            .reset_index()
            .rename(columns={"date": "count", "time_type": "callsign"})
        )

        sim_count_df["what"] = "Simulated"

        # Bring in historical data
        jobs_per_day_per_callsign_historical = (
            self.historical_data.historical_jobs_per_day_per_callsign.copy()
        )

        jobs_per_day_per_callsign_historical["what"] = "Historical"

        # Join the two together
        full_df = pd.concat([jobs_per_day_per_callsign_historical, sim_count_df])

        # Plot as histograms
        fig = px.histogram(
            full_df,
            x="jobs_in_day",
            y="count",
            color="what",
            facet_col="callsign",
            facet_row="what",
            histnorm="percent",
            #  barmode="overlay",
            #  opacity=1
        )

        return fig

    def get_care_cat_proportion_table(self):
        historical_counts_simple = (
            self.historical_data.care_cat_by_hour_historic.groupby("care_category")[
                "count"
            ]
            .sum()
            .reset_index()
            .rename(
                columns={
                    "count": "Historic Job Counts",
                    "care_category": "Care Category",
                }
            )
        )

        run_results = self.run_results.copy()

        # Amend care category to reflect the small proportion of regular jobs assumed to have
        # a helicopter benefit
        run_results.loc[
            (run_results["heli_benefit"] == "y") & (run_results["care_cat"] == "REG"),
            "care_cat",
        ] = "REG - helicopter benefit"

        care_cat_counts_sim = (
            run_results[run_results["event_type"] == "patient_helicopter_benefit"][
                ["P_ID", "run_number", "care_cat"]
            ]
            .reset_index()
            .groupby(["care_cat"])
            .size()
            .reset_index(name="count")
        ).copy()

        full_counts = historical_counts_simple.merge(
            care_cat_counts_sim.rename(
                columns={"care_cat": "Care Category", "count": "Simulated Job Counts"}
            ),
            how="outer",
            on="Care Category",
        )

        # Calculate proportions by column
        full_counts = full_counts[
            full_counts["Care Category"] != "Unknown - DAA resource did not attend"
        ].copy()

        full_counts["Historic Percentage"] = (
            full_counts["Historic Job Counts"]
            / full_counts["Historic Job Counts"].sum()
        )
        full_counts["Simulated Percentage"] = (
            full_counts["Simulated Job Counts"]
            / full_counts["Simulated Job Counts"].sum()
        )

        full_counts["Historic Percentage"] = full_counts["Historic Percentage"].apply(
            lambda x: f"{x:.1%}"
        )
        full_counts["Simulated Percentage"] = full_counts["Simulated Percentage"].apply(
            lambda x: f"{x:.1%}"
        )

        full_counts["Care Category"] = pd.Categorical(
            full_counts["Care Category"],
            ["CC", "EC", "REG - helicopter benefit", "REG"],
        )

        return full_counts.sort_values("Care Category")

    def get_care_cat_counts_plot_sim(self, show_proportions=False):
        run_results = self.run_results.copy()

        # Amend care category to reflect the small proportion of regular jobs assumed to have
        # a helicopter benefit
        run_results.loc[
            (run_results["heli_benefit"] == "y") & (run_results["care_cat"] == "REG"),
            "care_cat",
        ] = "REG - helicopter benefit"

        care_cat_by_hour = (
            run_results[run_results["event_type"] == "patient_helicopter_benefit"][
                ["P_ID", "run_number", "care_cat", "hour"]
            ]
            .reset_index()
            .groupby(["hour", "care_cat"])
            .size()
            .reset_index(name="count")
        ).copy()

        # Calculate total per hour
        total_per_hour = care_cat_by_hour.groupby("hour")["count"].transform("sum")
        # Add proportion column
        care_cat_by_hour["proportion"] = care_cat_by_hour["count"] / total_per_hour

        title = "Care Category of calls in simulation by hour of day with EC/CC/Regular - Heli Benefit/Regular"

        if not show_proportions:
            fig = px.bar(
                care_cat_by_hour,
                x="hour",
                y="count",
                color="care_cat",
                title=title,
                category_orders={
                    "care_cat": ["CC", "EC", "REG - helicopter benefit", "REG"]
                },
            )

        # if show_proportions
        else:
            fig = px.bar(
                care_cat_by_hour,
                x="hour",
                y="proportion",
                color="care_cat",
                title=title,
                category_orders={
                    "care_cat": ["CC", "EC", "REG - helicopter benefit", "REG"]
                },
            )

        fig.update_layout(xaxis=dict(dtick=1))

        return fig

    def get_preferred_outcome_by_hour(self, show_proportions=False):
        resource_preferred_outcome_by_hour = (
            self.run_results[
                self.run_results["event_type"] == "resource_preferred_outcome"
            ][["P_ID", "run_number", "care_cat", "time_type", "hour"]]
            .reset_index()
            .groupby(["time_type", "hour"])
            .size()
            .reset_index(name="count")
        ).copy()

        # Calculate total per hour
        total_per_hour = resource_preferred_outcome_by_hour.groupby("hour")[
            "count"
        ].transform("sum")
        # Add proportion column
        resource_preferred_outcome_by_hour["proportion"] = (
            resource_preferred_outcome_by_hour["count"] / total_per_hour
        )

        if not show_proportions:
            fig = px.bar(
                resource_preferred_outcome_by_hour,
                x="hour",
                y="count",
                color="time_type",
            )

        else:
            fig = px.bar(
                resource_preferred_outcome_by_hour,
                x="hour",
                y="proportion",
                color="time_type",
            )

        return fig

    def get_facet_plot_preferred_outcome_by_hour(self):
        resource_preferred_outcome_by_hour = (
            self.run_results[
                self.run_results["event_type"] == "resource_preferred_outcome"
            ][["P_ID", "run_number", "care_cat", "time_type", "hour"]]
            .reset_index()
            .groupby(["time_type", "hour"])
            .size()
            .reset_index(name="count")
        ).copy()

        # Calculate total per hour
        total_per_hour = resource_preferred_outcome_by_hour.groupby("hour")[
            "count"
        ].transform("sum")
        # Add proportion column
        resource_preferred_outcome_by_hour["proportion"] = (
            resource_preferred_outcome_by_hour["count"] / total_per_hour
        )

        resource_preferred_outcome_by_hour["time_type"] = (
            resource_preferred_outcome_by_hour["time_type"].apply(
                lambda x: textwrap.fill(x, width=25).replace("\n", "<br>")
            )
        )

        fig = px.bar(
            resource_preferred_outcome_by_hour,
            x="hour",
            y="proportion",
            facet_col="time_type",
            facet_col_wrap=4,
            height=800,
            facet_col_spacing=0.05,
            facet_row_spacing=0.13,
        )

        return fig

    def plot_patient_outcomes(
        self,
        group_cols="vehicle_type",
        outcome_col="hems_result",
        plot_counts=False,
        return_fig=True,
    ):
        patient_outcomes_df = (
            self.run_results[self.run_results["time_type"] == "HEMS call start"][
                [
                    "P_ID",
                    "run_number",
                    "heli_benefit",
                    "care_cat",
                    "vehicle_type",
                    "hems_result",
                    "outcome",
                ]
            ]
            .reset_index(drop=True)
            .copy()
        )

        def calculate_grouped_proportions(df, group_cols, outcome_col):
            """
            Calculate counts and proportions of an outcome column grouped by one or more columns.

            Parameters:
            df (pd.DataFrame): The input DataFrame.
            group_cols (str or list of str): Column(s) to group by (e.g., 'care_cat', 'vehicle_type').
            outcome_col (str): The name of the outcome column (e.g., 'hems_result').

            Returns:
            pd.DataFrame: A DataFrame with counts and proportions of outcome values per group.
            """
            if isinstance(group_cols, str):
                group_cols = [group_cols]

            count_df = (
                df.value_counts(group_cols + [outcome_col])
                .reset_index()
                .sort_values(group_cols + [outcome_col])
            )
            count_df.rename(columns={0: "count"}, inplace=True)

            # Calculate the total count per group (excluding the outcome column)
            total_per_group = count_df.groupby(group_cols)["count"].transform("sum")
            count_df["proportion"] = count_df["count"] / total_per_group

            return count_df

        patient_outcomes_df_grouped_counts = calculate_grouped_proportions(
            patient_outcomes_df, group_cols, outcome_col
        )

        if return_fig:
            if plot_counts:
                y = "count"
            else:
                y = "proportion"

            fig = px.bar(
                patient_outcomes_df_grouped_counts,
                color=group_cols,
                y=y,
                x=outcome_col,
                barmode="group",
            )

            return fig
        else:
            return patient_outcomes_df_grouped_counts

    def create_simulation_event_duration_df(self):
        order_of_events = [
            "HEMS call start",
            # 'No HEMS available',
            "HEMS allocated to call",
            # 'HEMS stand down before mobile',
            "HEMS mobile",
            # 'HEMS stand down en route',
            "HEMS on scene",
            # 'HEMS landed but no patient contact',
            "HEMS leaving scene",
            # 'HEMS patient treated (not conveyed)',
            "HEMS arrived destination",
            "HEMS clear",
        ]

        self.simulation_event_duration_df = self.run_results[
            self.run_results["time_type"].isin(order_of_events)
        ].copy()

        self.simulation_event_duration_df.time_type = (
            self.simulation_event_duration_df.time_type.astype("category")
        )

        self.simulation_event_duration_df.time_type = (
            self.simulation_event_duration_df.time_type.cat.set_categories(
                order_of_events
            )
        )

        self.simulation_event_duration_df = (
            self.simulation_event_duration_df.sort_values(
                ["run_number", "P_ID", "time_type"]
            )
        )

        # Calculate time difference within each group
        self.simulation_event_duration_df["time_elapsed"] = (
            self.simulation_event_duration_df.groupby(["P_ID", "run_number"])[
                "timestamp_dt"
            ].diff()
        )

        self.simulation_event_duration_df["time_elapsed_minutes"] = (
            self.simulation_event_duration_df["time_elapsed"].apply(
                lambda x: x.total_seconds() / 60.0 if pd.notna(x) else None
            )
        )

    def summarise_event_times(self):
        self.simulation_event_duration_df_summary = (
            self.simulation_event_duration_df.groupby("time_type", observed=False)[
                "time_elapsed_minutes"
            ]
            .agg(["mean", "median", "max", "min"])
            .round(1)
        )

    ##############
    # TODO: Confirm difference between run_results and event_log_df
    #############
    def resource_allocation_outcomes_run_variation(self):
        return (
            (
                self.run_results[
                    self.run_results["event_type"] == "resource_preferred_outcome"
                ]
                .groupby(["time_type", "run_number"])[["time_type"]]
                .count()
                / self.n_runs
            )
            .round(0)
            .astype("int")
            .rename(columns={"time_type": "Count"})
            .reset_index()
            .rename(
                columns={
                    "time_type": "Resource Allocation Attempt Outcome",
                    "run_number": "Run",
                }
            )
        ).copy()

    def get_perc_unattended_string(self):
        """
        Alternative to display_UNTATTENDED_calls_per_run

        This approach looks at instances where the resource_request_outcome
        was 'no resource available'
        """
        try:
            num_unattendable = len(
                self.run_results[
                    (self.run_results["event_type"] == "resource_request_outcome")
                    & (self.run_results["time_type"] == "No Resource Available")
                ]
            )

            # print(f"==get_perc_unattended_string - num_unattended: {num_unattendable}==")
        except:
            "Error"

        total_calls = len(
            self.run_results[
                (self.run_results["event_type"] == "resource_request_outcome")
            ]
        )

        try:
            perc_unattendable = num_unattendable / total_calls

            if perc_unattendable < 0.01:
                return f"{num_unattendable} of {total_calls} (< 0.1%)"
            else:
                return f"{num_unattendable} of {total_calls} ({perc_unattendable:.1%})"
        except:
            return "Error"

    def get_perc_unattended_string_normalised(self):
        """
        Alternative to display_UNTATTENDED_calls_per_run

        This approach looks at instances where the resource_request_outcome
        was 'no resource available'
        """
        try:
            num_unattendable = len(
                self.run_results[
                    (self.run_results["event_type"] == "resource_request_outcome")
                    & (self.run_results["time_type"] == "No Resource Available")
                ]
            )

            # print(f"==get_perc_unattended_string - num_unattended: {num_unattendable}==")
        except:
            "Error"

        total_calls = len(
            self.run_results[
                (self.run_results["event_type"] == "resource_request_outcome")
            ]
        )

        # print(f"==get_perc_unattended_string - total calls: {total_calls}==")

        try:
            perc_unattendable = num_unattendable / total_calls

            sim_duration_days = float(get_param("sim_duration_days", self.params_df))

            total_average_calls_missed_per_year = (
                num_unattendable / self.n_runs / sim_duration_days
            ) * 365
            total_average_calls_received_per_year = (
                total_calls / self.n_runs / sim_duration_days
            ) * 365

            if perc_unattendable < 0.01:
                return (
                    total_average_calls_received_per_year,
                    f"{(num_unattendable / self.n_runs):.0f} of {(total_calls / self.n_runs):.0f} (< 0.1%)",
                    f"This equates to around {total_average_calls_missed_per_year:.0f} of {total_average_calls_received_per_year:.0f} calls per year having no resource available to attend.",
                )
            else:
                return (
                    total_average_calls_received_per_year,
                    f"{(num_unattendable / self.n_runs):.0f} of {(total_calls / self.n_runs):.0f} ({perc_unattendable:.1%})",
                    f"This equates to around {total_average_calls_missed_per_year:.0f} of {total_average_calls_received_per_year:.0f} calls per year having no resource available to attend.",
                )
        except:
            return 2500, "Error", "Error"

    def get_missed_call_df(self):
        # Filter for relevant events

        resource_requests = self.run_results[
            self.run_results["event_type"] == "resource_request_outcome"
        ].copy()

        # Recode care_cat when helicopter benefit applies
        resource_requests["care_cat"] = resource_requests.apply(
            lambda x: "REG - Helicopter Benefit"
            if x["heli_benefit"] == "y" and x["care_cat"] == "REG"
            else x["care_cat"],
            axis=1,
        )

        # Group by care_cat, time_type, and run_number to get jobs per run
        jobs_per_run = (
            resource_requests.groupby(["care_cat", "time_type", "run_number"])
            .size()
            .reset_index(name="jobs")
        )

        sim_duration_days = float(get_param("sim_duration_days", self.params_df))

        # print(jobs_per_run.head())
        # print(jobs_per_run.jobs)
        # print(sim_duration_days)

        self.missed_jobs_per_run_breakdown = jobs_per_run.copy()

        self.missed_jobs_per_run_breakdown["jobs_per_year"] = (
            jobs_per_run["jobs"] / sim_duration_days
        ) * 365

        # Then aggregate to get average, min, and max per group
        self.missed_jobs_per_run_care_cat_summary = (
            jobs_per_run.groupby(["care_cat", "time_type"])
            .agg(
                jobs_average=("jobs", "mean"),
                jobs_min=("jobs", "min"),
                jobs_max=("jobs", "max"),
            )
            .reset_index()
        )

        # Add annualised average per year
        self.missed_jobs_per_run_care_cat_summary["jobs_per_year_average"] = (
            (
                self.missed_jobs_per_run_care_cat_summary["jobs_average"]
                / sim_duration_days
            )
            * 365
        ).round(0)

        self.missed_jobs_per_run_care_cat_summary["jobs_per_year_min"] = (
            (self.missed_jobs_per_run_care_cat_summary["jobs_min"] / sim_duration_days)
            * 365
        ).round(0)

        self.missed_jobs_per_run_care_cat_summary["jobs_per_year_max"] = (
            (
                self.missed_jobs_per_run_care_cat_summary["jobs_max"]
                / float(get_param("sim_duration_days", self.params_df))
            )
            * 365
        ).round(0)

    ###########################
    # TODO: NOT YET CONVERTED TO OO
    ##########################
    def make_SIMULATION_utilisation_variation_plot(
        self,
        historical_results_obj,
        car_colour="blue",
        helicopter_colour="red",
        use_poppins=False,
    ):
        """
        Creates a box plot to visualize the variation in resource utilization
        across all simulation runs, with historical mean and range indicators.

        Parameters
        ----------
        historical_results_obj

        Returns
        -------
        plotly.graph_objects.Figure
            A Plotly box plot.
        """
        utilisation_df_per_run = self.utilisation_df_per_run.reset_index()
        utilisation_df_per_run["vehicle_type"] = utilisation_df_per_run[
            "vehicle_type"
        ].str.title()
        utilisation_df_per_run["callsign"] = utilisation_df_per_run["callsign"].astype(
            str
        )

        sorted_callsigns = sorted(utilisation_df_per_run["callsign"].unique())

        # Force ordering for y-axis alignment
        utilisation_df_per_run["callsign"] = pd.Categorical(
            utilisation_df_per_run["callsign"],
            categories=sorted_callsigns,
            ordered=True,
        )
        utilisation_df_per_run = utilisation_df_per_run.sort_values("callsign")

        fig = px.box(
            utilisation_df_per_run,
            x="perc_time_in_use",
            y="callsign",
            color="vehicle_type",
            title="Variation in Resource Utilisation Across All Simulation Runs",
            labels={
                "callsign": "Callsign",
                "perc_time_in_use": "Average Percentage of Available<br>Time Spent in Use",
                "vehicle_type": "Vehicle Type",
            },
            color_discrete_map={
                "Car": COLORSCHEME.get(car_colour, "blue"),
                "Helicopter": COLORSCHEME.get(helicopter_colour, "red"),
            },
            category_orders={"callsign": sorted_callsigns},
        )

        fig.update_layout(xaxis={"tickformat": ".0%"})

        # Flip y-axis so our numeric y positions match callsign order
        fig.update_yaxes(
            categoryorder="array", categoryarray=sorted_callsigns, autorange="reversed"
        )

        # Add historical data
        historical_utilisation_df_summary = (
            historical_results_obj.historical_utilisation_df_summary.copy()
        )
        historical_utilisation_df_summary.index = (
            historical_utilisation_df_summary.index.astype(str)
        )

        historical_marker_height_fraction = 0.4

        for num_idx, callsign_str in enumerate(sorted_callsigns):
            if callsign_str in historical_utilisation_df_summary.index:
                row = historical_utilisation_df_summary.loc[callsign_str]
                min_val = row["min"] / 100.0
                max_val = row["max"] / 100.0
                mean_val = row["mean"] / 100.0

                y_pos_center = num_idx
                y_pos_start = y_pos_center - historical_marker_height_fraction
                y_pos_end = y_pos_center + historical_marker_height_fraction

                fig.add_shape(
                    type="rect",
                    yref="y",
                    xref="x",
                    y0=y_pos_start,
                    x0=min_val,
                    y1=y_pos_end,
                    x1=max_val,
                    fillcolor=COLORSCHEME.get(
                        "historical_box_fill", "rgba(0,0,0,0.08)"
                    ),
                    line=dict(color="rgba(0,0,0,0)"),
                    layer="below",
                )

                fig.add_shape(
                    type="line",
                    yref="y",
                    xref="x",
                    y0=y_pos_start,
                    x0=mean_val,
                    y1=y_pos_end,
                    x1=mean_val,
                    line=dict(
                        dash="dot",
                        color=COLORSCHEME.get("charcoal", "black"),
                        width=2,
                    ),
                    layer="below",
                )

                fig.add_trace(
                    go.Scatter(
                        x=[mean_val + 0.01],
                        y=[callsign_str],
                        text=[f"Hist. Mean: {mean_val:.0%}"],
                        mode="text",
                        textfont=dict(
                            color=COLORSCHEME.get("charcoal", "black"), size=10
                        ),
                        hoverinfo="skip",
                        showlegend=False,
                    )
                )

        fig.update_layout(
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )

        if use_poppins:
            fig.update_layout(
                font=dict(
                    family="Poppins",
                    size=18,
                    color=COLORSCHEME.get("charcoal", "black"),
                )
            )
        else:
            fig.update_layout(
                font=dict(
                    family="Arial, sans-serif",
                    size=12,
                    color=COLORSCHEME.get("charcoal", "black"),
                )
            )

        return fig

    def PLOT_SIMULATION_utilisation_summary(
        self,
        historical_results_obj,
        car_colour="blue",
        helicopter_colour="red",
        use_poppins=False,
    ):
        """
        Creates a bar plot to summarize the average resource utilization
        across all simulation runs.

        Parameters
        ----------
        historical_results_obj

        Returns
        -------
        plotly.graph_objects.Figure
            A Plotly bar plot showing the average utilization percentage
            for each resource, grouped by vehicle type.

        Notes
        -----
        - The `vehicle_type` column values are capitalized for consistency.
        - The y-axis values are formatted as percentages with no decimal places.
        - A custom color scheme is applied based on vehicle type:
        - "Car" is mapped to `COLORSCHEME["blue"]`.
        - "Helicopter" is mapped to `COLORSCHEME["red"]`.

        Example
        -------
        >>> fig = make_SIMULATION_utilisation_summary_plot(utilisation_df)
        >>> fig.show()
        """
        utilisation_df_overall = self.utilisation_df_overall.reset_index()
        utilisation_df_overall["vehicle_type"] = utilisation_df_overall[
            "vehicle_type"
        ].str.title()
        utilisation_df_overall["perc_time_formatted"] = utilisation_df_overall[
            "perc_time_in_use"
        ].apply(lambda x: f"Simulated: {x:.1%}")

        # Create base bar chart
        fig = px.bar(
            utilisation_df_overall,
            y="perc_time_in_use",
            x="callsign",
            color="vehicle_type",
            opacity=0.5,
            text="perc_time_formatted",
            title="Average Resource Utilisation Across All Simulation Runs",
            labels={
                "callsign": "Callsign",
                "perc_time_in_use": "Average Percentage of Available<br>Time Spent in Use",
                "vehicle_type": "Vehicle Type",
            },
            color_discrete_map={
                "Car": COLORSCHEME.get(car_colour, "blue"),  # Use .get for safety
                "Helicopter": COLORSCHEME.get(helicopter_colour, "red"),
            },
            # barmode='group' is default when color is used, which is good.
        )

        # Place actual label at the bottom of the bar
        fig.update_traces(textposition="inside", insidetextanchor="start")

        fig.update_layout(
            bargap=0.4,
            yaxis_tickformat=".0%",
            xaxis_type="category",  # Explicitly set x-axis to category
            # Ensure categories are ordered as per the sorted DataFrame
            # If callsigns are purely numeric but should be treated as categories, ensure they are strings
            xaxis={
                "categoryorder": "array",
                "categoryarray": sorted(utilisation_df_overall["callsign"].unique()),
            },
        )

        # Get the unique, sorted callsigns as they will appear on the x-axis
        # This order is now determined by the 'categoryarray' in layout or default sorting.
        x_axis_categories = sorted(utilisation_df_overall["callsign"].unique())

        # Define the width of the historical markers (box and line) relative to category slot
        # A category slot is 1 unit wide (e.g., from -0.5 to 0.5 around the category's integer index).
        # We'll make the markers 80% of this width.
        historical_marker_width_fraction = 0.3  # Half-width, so total width is 0.8

        # Iterate through the callsigns in the order they appear on the axis
        for num_idx, callsign_str in enumerate(x_axis_categories):
            if (
                callsign_str
                in historical_results_obj.historical_utilisation_df_summary.index
            ):
                row = historical_results_obj.historical_utilisation_df_summary.loc[
                    callsign_str
                ]

                min_val = row["min"] / 100.0  # Convert percentage to 0-1 scale
                max_val = row["max"] / 100.0
                mean_val = row["mean"] / 100.0

                # Calculate x-positions for the historical markers
                # num_idx is the integer position of the category (0, 1, 2, ...)
                x_pos_start = num_idx - historical_marker_width_fraction
                x_pos_end = num_idx + historical_marker_width_fraction

                # --- Min/Max shaded rectangle for historical range ---
                fig.add_shape(
                    type="rect",
                    xref="x",
                    yref="y",  # Refer to data coordinates
                    x0=x_pos_start,
                    y0=min_val,
                    x1=x_pos_end,
                    y1=max_val,
                    fillcolor=COLORSCHEME.get(
                        "historical_box_fill", "rgba(0,0,0,0.08)"
                    ),
                    line=dict(color="rgba(0,0,0,0)"),  # No border for the box
                    layer="below",  # Draw below the main bars
                )

                # --- Mean horizontal line for historical mean ---
                fig.add_shape(
                    type="line",
                    xref="x",
                    yref="y",
                    x0=x_pos_start,
                    y0=mean_val,
                    x1=x_pos_end,
                    y1=mean_val,
                    line=dict(
                        dash="dot",
                        color=COLORSCHEME.get("charcoal", "black"),
                        width=2,
                    ),
                    layer="below",  # Draw below main bars
                )

                # --- Mean value label (text annotation) ---
                # Using go.Scatter for text annotation, positioned at the center of the category
                fig.add_trace(
                    go.Scatter(
                        x=[callsign_str],  # Use the category name for x
                        y=[
                            mean_val + (0.01 if max_val < 0.95 else -0.01)
                        ],  # Adjust y to avoid overlap with top
                        text=[f"Historical: {mean_val * 100:.0f}%"],  # Simpler label
                        mode="text",
                        textfont=dict(
                            color=COLORSCHEME.get("charcoal", "black"), size=10
                        ),
                        hoverinfo="skip",
                        showlegend=False,
                    )
                )

        fig.update_layout(
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )

        if use_poppins:
            fig.update_layout(
                font=dict(
                    family="Poppins",
                    size=18,
                    color=COLORSCHEME.get("charcoal", "black"),
                )
            )
        else:  # Apply a default font for better appearance
            fig.update_layout(
                font=dict(
                    family="Arial, sans-serif",
                    size=12,
                    color=COLORSCHEME.get("charcoal", "black"),
                )
            )

        return fig

    # TODO: convert to use historical class
    # --- Helper function to display vehicle metric ---
    def display_vehicle_utilisation_metric(
        self,
        historical_data_class,
        st_column,
        callsign_to_display,
        vehicle_type_label,
        icon_unicode,
        current_quarto_string,
    ):
        """
        Displays the utilisation metrics for a given vehicle in a specified Streamlit column.
        Returns the updated quarto_string.
        """
        with st_column:
            with iconMetricContainer(
                key=f"{vehicle_type_label.lower()}_util_{callsign_to_display}",
                icon_unicode=icon_unicode,
                type="symbols",
            ):
                matched_sim = self.utilisation_df_overall[
                    self.utilisation_df_overall["callsign"] == callsign_to_display
                ]

                if not matched_sim.empty:
                    sim_util_fig = matched_sim["PRINT_perc"].values[0]
                    sim_util_display = f"{sim_util_fig}"
                else:
                    sim_util_fig = "N/A"
                    sim_util_display = "N/A"

                current_quarto_string += f"\n\nAverage simulated {callsign_to_display} Utilisation was {sim_util_fig}\n\n"

                st.metric(
                    f"Average Simulated {callsign_to_display} Utilisation",
                    sim_util_display,
                    border=True,
                )

            # Get historical data
            hist_util_value = historical_data_class.RETURN_hist_util_fig(
                callsign_to_display, "mean"
            )

            hist_util_value_display = (
                f"{hist_util_value}%"
                if isinstance(hist_util_value, (int, float))
                else hist_util_value
            )

            hist_util_caption = f"*The historical average utilisation of {callsign_to_display} was {hist_util_value_display}*\n\n"
            current_quarto_string += hist_util_caption
            current_quarto_string += "\n\n---\n\n"
            st.caption(hist_util_caption)

        return current_quarto_string

    def create_callsign_group_split_rwc_plot(
        self,
        historical_data_obj,
        x_is_callsign_group=False,
    ):
        jobs_by_callsign = (
            historical_data_obj.historical_monthly_totals_by_callsign.copy()
        )
        jobs_by_callsign["month"] = pd.to_datetime(
            jobs_by_callsign["month"], format="ISO8601"
        )
        jobs_by_callsign["quarter"] = jobs_by_callsign["month"].dt.quarter
        jobs_by_callsign = jobs_by_callsign.melt(id_vars=["month", "quarter"]).rename(
            columns={"variable": "callsign", "value": "jobs"}
        )
        jobs_by_callsign["callsign_group"] = jobs_by_callsign["callsign"].str.extract(
            r"(\d+)"
        )
        jobs_by_callsign_group_hist = (
            jobs_by_callsign.groupby(["callsign_group", "quarter"])["jobs"]
            .sum()
            .reset_index()
        )
        # Group by quarter and compute total jobs per quarter
        quarter_totals_hist = jobs_by_callsign_group_hist.groupby("quarter")[
            "jobs"
        ].transform("sum")
        # Calculate proportion
        jobs_by_callsign_group_hist["proportion"] = (
            jobs_by_callsign_group_hist["jobs"] / quarter_totals_hist
        )
        jobs_by_callsign_group_hist["what"] = "Historical"

        jobs_by_callsign_sim = self.resource_use_events_only_df[
            ["run_number", "P_ID", "time_type", "qtr"]
        ].copy()
        jobs_by_callsign_sim["callsign_group"] = jobs_by_callsign_sim[
            "time_type"
        ].str.extract(r"(\d+)")
        jobs_by_callsign_group_sim = (
            jobs_by_callsign_sim.groupby(["qtr", "callsign_group"])
            .size()
            .reset_index()
            .rename(columns={"qtr": "quarter", 0: "jobs"})
        )
        # Group by quarter and compute total jobs per quarter
        quarter_totals_sim = jobs_by_callsign_group_sim.groupby("quarter")[
            "jobs"
        ].transform("sum")
        # Calculate proportion
        jobs_by_callsign_group_sim["proportion"] = (
            jobs_by_callsign_group_sim["jobs"] / quarter_totals_sim
        )
        jobs_by_callsign_group_sim["what"] = "Simulated"

        full_df_callsign_group_counts = pd.concat(
            [jobs_by_callsign_group_hist, jobs_by_callsign_group_sim]
        )

        if not x_is_callsign_group:
            fig = px.bar(
                full_df_callsign_group_counts,
                title="Historical vs Simulated Split of Jobs Between Callsign Groups",
                color="callsign_group",
                y="proportion",
                x="what",
                barmode="stack",
                labels={
                    "what": "",
                    "proportion": "Percent of Jobs in Quarter",
                    "callsign_group": "Callsign Group",
                },
                facet_col="quarter",
                text=full_df_callsign_group_counts["proportion"].map(
                    lambda p: f"{p:.0%}"
                ),  # format as percent
            )

            fig.update_layout(yaxis_tickformat=".0%")

            fig.update_traces(
                textposition="inside"
            )  # You can also try 'auto' or 'outside'
            return fig

        else:
            fig = px.bar(
                full_df_callsign_group_counts,
                title="Historical vs Simulated Split of Jobs Between Callsign Groups",
                color="what",
                y="proportion",
                x="callsign_group",
                barmode="group",
                labels={
                    "what": "",
                    "proportion": "Percent of Jobs in Quarter",
                    "callsign_group": "Callsign Group",
                },
                facet_col="quarter",
                text=full_df_callsign_group_counts["proportion"].map(
                    lambda p: f"{p:.0%}"
                ),  # format as percent
            )

            fig.update_layout(yaxis_tickformat=".0%")

            fig.update_traces(
                textposition="inside"
            )  # You can also try 'auto' or 'outside'
            return fig

    def PLOT_UTIL_rwc_plot(self) -> Figure:
        #############
        # Prep real-world data
        #############

        jobs_by_callsign_long = (
            self.historical_data.historical_monthly_totals_by_callsign.melt(
                id_vars="month"
            )
            .rename(columns={"variable": "callsign", "value": "jobs"})
            .copy()
        )

        # print(jobs_by_callsign_long)

        all_combinations = pd.MultiIndex.from_product(
            [
                jobs_by_callsign_long["month"].unique(),
                jobs_by_callsign_long["callsign"].unique(),
            ],
            names=["month", "callsign"],
        )

        # Reindex the dataframe to include missing callsigns
        jobs_by_callsign_long = (
            jobs_by_callsign_long.set_index(["month", "callsign"])
            .reindex(all_combinations, fill_value=0)
            .reset_index()
        )

        jobs_by_callsign_long["callsign_group"] = jobs_by_callsign_long[
            "callsign"
        ].str.extract(r"(\d+)")

        jobs_by_callsign_long = jobs_by_callsign_long[
            ~jobs_by_callsign_long["callsign_group"].isna()
        ]

        jobs_by_callsign_long["vehicle_type"] = jobs_by_callsign_long["callsign"].apply(
            lambda x: "car" if "CC" in x else "helicopter"
        )

        # Compute total jobs per callsign_group per month
        jobs_by_callsign_long["total_jobs_per_group"] = jobs_by_callsign_long.groupby(
            ["month", "callsign_group"]
        )["jobs"].transform("sum")

        # Compute percentage of calls per row
        jobs_by_callsign_long["percentage_of_group"] = (
            jobs_by_callsign_long["jobs"]
            / jobs_by_callsign_long["total_jobs_per_group"]
        ) * 100

        # Handle potential division by zero (if total_jobs_per_group is 0)
        jobs_by_callsign_long["percentage_of_group"] = jobs_by_callsign_long[
            "percentage_of_group"
        ].fillna(0)

        # print(jobs_by_callsign_long)

        fig = go.Figure()

        # Bar chart (Simulation Averages)
        for idx, vehicle in enumerate(
            self.sim_averages_utilisation["vehicle_type"].unique()
        ):
            filtered_data = self.sim_averages_utilisation[
                self.sim_averages_utilisation["vehicle_type"] == vehicle
            ]

            fig.add_trace(
                go.Bar(
                    y=filtered_data["percentage_of_group"],
                    x=filtered_data["callsign_group"],
                    name=f"Simulated - {vehicle}",
                    marker=dict(color=list(COLORSCHEME.values())[idx]),
                    width=0.3,
                    opacity=0.6,  # Same opacity for consistency
                    text=[
                        "Simulated:<br>{:.1f}%".format(val)
                        for val in filtered_data["percentage_of_group"]
                    ],  # Correct way            textposition="inside",  # Places text just above the x-axis
                    insidetextanchor="start",  # Anchors text inside the bottom of the bar
                    textfont=dict(color="white"),
                )
            )

            fig.update_layout(
                title="<b>Comparison of Allocated Resources by Callsign Group</b><br>Simulation vs Historical Data",
                yaxis_title="Percentage of Jobs in Callsign Group<br>Tasked to Callsign",
                xaxis_title="Callsign Group",
                barmode="group",
                legend_title="Vehicle Type",
                height=600,
            )

        for callsign in jobs_by_callsign_long["callsign"].unique():
            filtered_data = (
                jobs_by_callsign_long[jobs_by_callsign_long["callsign"] == callsign]
                .groupby(["callsign_group", "vehicle_type", "callsign"])[
                    ["percentage_of_group"]
                ]
                .mean()
                .reset_index()
            )

            if filtered_data["callsign_group"].values[0] in ["70", "71"]:
                expected_x = float(filtered_data["callsign_group"].values[0])
                y_value = filtered_data["percentage_of_group"].values[0]
                expected_y = y_value - 1  # Position for the line

                if filtered_data["vehicle_type"].values[0] == "car":
                    x_start = expected_x - 0.4
                    x_end = expected_x
                else:
                    x_start = expected_x
                    x_end = expected_x + 0.4

                # Add dashed line
                fig.add_trace(
                    go.Scatter(
                        x=[x_start, x_end],
                        y=[expected_y, expected_y],
                        mode="lines",
                        name=f"Expected Level - {callsign}",
                        showlegend=False,
                        hoverinfo="all",
                        line=dict(dash="dash", color=COLORSCHEME["charcoal"]),
                    )
                )

                # Add text annotation above the line
                fig.add_trace(
                    go.Scatter(
                        x=[(x_start + x_end) / 2],  # Center the text horizontally
                        y=[expected_y + 5],  # Slightly above the line
                        text=[f"Historical:<br>{y_value:.1f}%"],
                        mode="text",
                        textfont=dict(color="black"),
                        showlegend=False,  # Don't show in legend
                    )
                )

        min_x = min(
            self.sim_averages_utilisation["callsign_group"].astype("int").values
        )
        max_x = max(
            self.sim_averages_utilisation["callsign_group"].astype("int").values
        )

        tick_vals = list(range(min_x, max_x + 1))  # Tick positions at integer values

        fig.update_layout(
            xaxis=dict(
                title=dict(font=dict(size=20)),
                tickfont=dict(size=25),
                tickmode="array",
                tickvals=tick_vals,  # Ensure ticks are at integer positions
                range=[
                    min_x - 0.5,
                    max_x + 0.5,
                ],  # Extend range to start 0.5 units earlier
            ),
            yaxis=dict(
                ticksuffix="%",
                title=dict(dict(font=dict(size=15))),
                tickfont=dict(size=20),
                range=[0, 100],
            ),
            legend=dict(font=dict(size=15)),
            legend_title=dict(font=dict(size=20)),
        )

        return fig

    def prep_util_df_from_call_df(self):
        self.call_df["timestamp_dt"] = pd.to_datetime(
            self.call_df["timestamp_dt"], format="ISO8601"
        )
        self.call_df["month_start"] = (
            self.call_df["timestamp_dt"].dt.to_period("M").dt.to_timestamp()
        )

        # print("==prep_util_df_from_call_df: call_df==")
        # print(call_df)

        jobs_counts_by_callsign_monthly_sim = self.call_df[
            ~self.call_df["callsign"].isna()
        ].copy()

        # print("==jobs_counts_by_callsign_monthly_sim - prior to aggregation==")
        # print(jobs_counts_by_callsign_monthly_sim)

        jobs_counts_by_callsign_monthly_sim["callsign_group"] = (
            jobs_counts_by_callsign_monthly_sim["callsign"].str.extract(r"(\d+)")
        )

        jobs_counts_by_callsign_monthly_sim = (
            jobs_counts_by_callsign_monthly_sim.groupby(
                [
                    "run_number",
                    "month_start",
                    "callsign",
                    "callsign_group",
                    "vehicle_type",
                ]
            )["P_ID"]
            .count()
            .reset_index()
            .rename(columns={"P_ID": "jobs"})
        )

        # print("==jobs_counts_by_callsign_monthly_sim==")
        # print(jobs_counts_by_callsign_monthly_sim)

        all_combinations = pd.MultiIndex.from_product(
            [
                jobs_counts_by_callsign_monthly_sim["month_start"].unique(),
                jobs_counts_by_callsign_monthly_sim["run_number"].unique(),
                jobs_counts_by_callsign_monthly_sim["callsign"].unique(),
            ],
            names=["month_start", "run_number", "callsign"],
        )

        # Reindex the dataframe to include missing callsigns
        jobs_counts_by_callsign_monthly_sim = (
            jobs_counts_by_callsign_monthly_sim.set_index(
                ["month_start", "run_number", "callsign"]
            )
            .reindex(all_combinations, fill_value=0)
            .reset_index()
        )

        jobs_counts_by_callsign_monthly_sim["callsign_group"] = (
            jobs_counts_by_callsign_monthly_sim["callsign"].str.extract(r"(\d+)")
        )
        jobs_counts_by_callsign_monthly_sim["vehicle_type"] = (
            jobs_counts_by_callsign_monthly_sim["callsign"].apply(
                lambda x: "car" if "C" in x else "helicopter"
            )
        )

        # Compute total jobs per callsign_group per month
        jobs_counts_by_callsign_monthly_sim["total_jobs_per_group"] = (
            jobs_counts_by_callsign_monthly_sim.groupby(
                ["month_start", "callsign_group", "run_number"]
            )["jobs"].transform("sum")
        )

        # Compute percentage of calls per row
        jobs_counts_by_callsign_monthly_sim["percentage_of_group"] = (
            jobs_counts_by_callsign_monthly_sim["jobs"]
            / jobs_counts_by_callsign_monthly_sim["total_jobs_per_group"]
        ) * 100

        # Handle potential division by zero (if total_jobs_per_group is 0)
        jobs_counts_by_callsign_monthly_sim["percentage_of_group"] = (
            jobs_counts_by_callsign_monthly_sim["percentage_of_group"].fillna(0)
        )

        self.sim_averages_utilisation = (
            jobs_counts_by_callsign_monthly_sim.groupby(
                ["callsign_group", "callsign", "vehicle_type"]
            )[["percentage_of_group"]]
            .mean()
            .reset_index()
        )

    def make_SIMULATION_stacked_callsign_util_plot(self):
        fig = px.bar(
            self.sim_averages_utilisation,
            x="percentage_of_group",
            y="callsign_group",
            color="callsign",
            height=300,
        )

        # Update axis labels and legend title
        fig.update_layout(
            yaxis=dict(
                title="Callsign Group",
                tickmode="linear",  # Ensures ticks appear at regular intervals
                dtick=1,  # Set tick spacing to 1 unit
            ),
            xaxis=dict(title="Utilisation % within Callsign Group"),
            legend_title="Callsign",
        )

        return fig

    def make_SIMULATION_utilisation_headline_figure(self, vehicle_type):
        """
        Options:
            - helicopter
            - solo car
            - helicopter backup car
        """

        if vehicle_type == "helicopter":
            return self.utilisation_df_overall[
                self.utilisation_df_overall["vehicle_type"] == "helicopter"
            ].mean(numeric_only=True)["perc_time_in_use"]

        else:
            # assume anything with >= 1 entries in a callsign group is helicopter + backup car
            # NOTE: This assumption may not hold forever! It assumes
            vehicles_per_callsign_group = self.utilisation_df_overall.groupby(
                "callsign_group"
            ).count()[["callsign"]]

            if vehicle_type == "solo car":
                car_only = vehicles_per_callsign_group[
                    vehicles_per_callsign_group["callsign"] == 1
                ].copy()

                return self.utilisation_df_overall[
                    self.utilisation_df_overall["callsign_group"].isin(
                        car_only.reset_index().callsign_group.values
                    )
                ].mean(numeric_only=True)["perc_time_in_use"]

            elif vehicle_type == "helicopter backup car":
                backupcar_only = vehicles_per_callsign_group[
                    vehicles_per_callsign_group["callsign"] == 2
                ].copy()

                cars_only = self.utilisation_df_overall[
                    self.utilisation_df_overall["vehicle_type"] == "car"
                ].copy()

                return cars_only[
                    cars_only["callsign_group"].isin(
                        backupcar_only.reset_index().callsign_group.values
                    )
                ].mean(numeric_only=True)["perc_time_in_use"]

            else:
                print(
                    "Invalid vehicle type entered. Please use 'helicopter', 'solo car' or 'helicopter backup car'"
                )

    def create_event_log(self):
        df = self.run_results[self.run_results["event_type"] == "queue"].copy()

        df["activity_id"] = df.groupby("run_number").cumcount() + 1

        # Duplicate rows and modify them
        df_start = df.copy()

        df_start["lifecycle_id"] = "start"

        df_end = df.copy()
        df_end["lifecycle_id"] = "complete"

        # Shift timestamps for 'end' rows
        df_end["timestamp"] = df_end["timestamp"].shift(-1)
        df_end["timestamp_dt"] = df_end["timestamp_dt"].shift(-1)

        # Combine and sort
        df_combined = pd.concat([df_start, df_end]).sort_index(kind="stable")

        # Drop last 'end' row (since there’s no next row to get a timestamp from)
        df_combined = df_combined[:-1]

        df_combined.to_csv("event_log.csv", index=False)

    def display_resource_use_exploration(
        self,
    ):
        """
        Displays the resource use exploration section including dataframes and plots.
        """

        st.subheader("Resource Use")

        # Accounting for odd bug being seen in streamlit community cloud
        # This check might be more robust if done before calling this function,
        # but keeping it here to match original logic if resource_use_events_only_df is passed directly.
        if "P_ID" not in self.resource_use_events_only_df.columns:
            self.resource_use_events_only_df = (
                self.resource_use_events_only_df.reset_index()
            )

        # The @st.fragment decorator is used to group widgets and outputs
        # that should be treated as a single unit for rerun behavior.
        # If you want this behavior, keep it. Otherwise, it can be removed
        # if the function is called within a fragment in the main app.
        # For this refactoring, we'll keep it to ensure similar behavior.
        @st.fragment
        def resource_use_exploration_plots_fragment():
            run_select_ruep = st.selectbox(
                "Choose the run to show",
                self.resource_use_events_only_df["run_number"].unique(),
                key="ruep_run_select",  # Added a key for uniqueness
            )

            # colour_by_cc_ec = st.toggle("Colour the plot by CC/EC/REG patient benefit",
            #                             value=True, key="ruep_color_toggle") # Added a key

            show_outline = st.toggle(
                "Show an outline to help debug overlapping calls",
                value=False,
                key="ruep_outline_toggle",
            )  # Added a key

            with st.expander("Click here to see the timings of resource use"):
                st.dataframe(
                    self.resource_use_events_only_df[
                        self.resource_use_events_only_df["run_number"]
                        == run_select_ruep
                    ]
                )

                st.dataframe(
                    self.resource_use_events_only_df[
                        self.resource_use_events_only_df["run_number"]
                        == run_select_ruep
                    ][["callsign", "callsign_group", "registration"]].value_counts()
                )

                st.dataframe(
                    self.resource_use_events_only_df[
                        self.resource_use_events_only_df["run_number"]
                        == run_select_ruep
                    ][["P_ID", "time_type", "timestamp_dt", "event_type"]]
                    .melt(
                        id_vars=["P_ID", "time_type", "event_type"],
                        value_vars="timestamp_dt",
                    )
                    .drop_duplicates()
                )

                resource_use_wide = (
                    self.resource_use_events_only_df[
                        self.resource_use_events_only_df["run_number"]
                        == run_select_ruep
                    ][
                        [
                            "P_ID",
                            "time_type",
                            "timestamp_dt",
                            "event_type",
                            "registration",
                            "care_cat",
                        ]
                    ]
                    .drop_duplicates()
                    .pivot(
                        columns="event_type",
                        index=["P_ID", "time_type", "registration", "care_cat"],
                        values="timestamp_dt",
                    )
                    .reset_index()
                ).copy()

                # get the number of resources and assign them a value
                resources = resource_use_wide.time_type.unique()
                resources = np.concatenate([resources, ["No Resource Available"]])
                resource_dict = {
                    resource: index for index, resource in enumerate(resources)
                }

                missed_job_events = self.run_results[
                    (
                        self.run_results["run_number"] == run_select_ruep
                    )  # Filter by selected run first
                    & (self.run_results["event_type"] == "resource_request_outcome")
                    & (self.run_results["time_type"] == "No Resource Available")
                ].copy()  # Use .copy() to avoid SettingWithCopyWarning if further modifications are made

                # Check if 'P_ID' is in columns, if not, reset_index (bug handling from original)
                if (
                    "P_ID" not in missed_job_events.columns
                    and not missed_job_events.empty
                ):
                    missed_job_events = missed_job_events.reset_index()

                missed_job_events = missed_job_events[
                    [
                        "P_ID",
                        "time_type",
                        "timestamp_dt",
                        "event_type",
                        "registration",
                        "care_cat",
                    ]
                ].drop_duplicates()
                missed_job_events["event_type"] = "resource_use"

                missed_job_events_end = missed_job_events.copy()
                missed_job_events_end["event_type"] = "resource_use_end"
                missed_job_events_end["timestamp_dt"] = pd.to_datetime(
                    missed_job_events_end["timestamp_dt"]
                ) + datetime.timedelta(minutes=5)

                missed_job_events_full = pd.concat(
                    [missed_job_events, missed_job_events_end]
                )
                missed_job_events_full["registration"] = (
                    "No Resource Available"  # Explicitly set for these events
                )

                if not missed_job_events_full.empty:
                    missed_job_events_full_wide = missed_job_events_full.pivot(
                        columns="event_type",
                        index=["P_ID", "time_type", "registration", "care_cat"],
                        values="timestamp_dt",
                    ).reset_index()
                    resource_use_wide = pd.concat(
                        [resource_use_wide, missed_job_events_full_wide]
                    ).reset_index(drop=True)
                else:
                    # Ensure columns match if missed_job_events_full_wide is empty
                    # This might need more robust handling based on expected columns
                    pass

                resource_use_wide["y_pos"] = resource_use_wide["time_type"].map(
                    resource_dict
                )

                resource_use_wide["resource_use_end"] = pd.to_datetime(
                    resource_use_wide["resource_use_end"]
                )
                resource_use_wide["resource_use"] = pd.to_datetime(
                    resource_use_wide["resource_use"]
                )

                resource_use_wide["duration"] = (
                    resource_use_wide["resource_use_end"]
                    - resource_use_wide["resource_use"]
                )
                resource_use_wide["duration_seconds"] = (
                    resource_use_wide["resource_use_end"]
                    - resource_use_wide["resource_use"]
                ).dt.total_seconds() * 1000
                resource_use_wide["duration_minutes"] = (
                    resource_use_wide["duration_seconds"] / 1000 / 60
                )
                resource_use_wide["duration_minutes"] = resource_use_wide[
                    "duration_minutes"
                ].round(1)

                resource_use_wide["callsign_group"] = resource_use_wide[
                    "time_type"
                ].str.extract(r"(\d+)")  # Added r for raw string

                resource_use_wide = resource_use_wide.sort_values(
                    ["callsign_group", "time_type"]
                )

                st.dataframe(resource_use_wide)

                service_schedule = self.simulation_inputs.service_dates_df.merge(
                    self.simulation_inputs.callsign_registration_lookup_df,
                    on="registration",
                )  # Specify merge key if different

                service_schedule["service_end_date"] = pd.to_datetime(
                    service_schedule["service_end_date"]
                )

                service_schedule["service_start_date"] = pd.to_datetime(
                    service_schedule["service_start_date"]
                )

                service_schedule["duration_seconds"] = (
                    (
                        service_schedule["service_end_date"]
                        - service_schedule["service_start_date"]
                    )
                    + datetime.timedelta(days=1)
                ).dt.total_seconds() * 1000

                service_schedule["duration_days"] = (
                    (service_schedule["duration_seconds"] / 1000) / 60 / 60 / 24
                )

                # Map y_pos using the comprehensive resource_dict
                service_schedule["y_pos"] = service_schedule["callsign"].map(
                    resource_dict
                )
                # Filter out entries that couldn't be mapped if necessary, or handle NaNs
                service_schedule = service_schedule.dropna(subset=["y_pos"])

                # Ensure resource_use_wide is not empty before accessing .min()/.max()
                if (
                    not resource_use_wide.empty
                    and "resource_use" in resource_use_wide.columns
                ):
                    min_date = resource_use_wide.resource_use.min()
                    max_date = resource_use_wide.resource_use.max()
                    service_schedule = service_schedule[
                        (service_schedule["service_start_date"] <= max_date)
                        & (service_schedule["service_end_date"] >= min_date)
                    ]
                else:  # Handle case where resource_use_wide might be empty or missing column
                    service_schedule = pd.DataFrame(
                        columns=service_schedule.columns
                    )  # Empty df with same columns

                st.dataframe(service_schedule)

            # Create figure
            resource_use_fig = go.Figure()

            # Add horizontal bars using actual datetime values
            # Ensure unique callsigns are taken from the sorted resource_use_wide for consistent y-axis order
            unique_time_types_sorted = resource_use_wide.time_type.unique()

            for idx, callsign in enumerate(unique_time_types_sorted):
                callsign_df = resource_use_wide[
                    resource_use_wide["time_type"] == callsign
                ]
                service_schedule_df = service_schedule[
                    service_schedule["callsign"] == callsign
                ]

                # Add in hatched boxes showing the servicing periods
                if not service_schedule_df.empty:
                    resource_use_fig.add_trace(
                        go.Bar(
                            x=service_schedule_df["duration_seconds"],
                            y=service_schedule_df["y_pos"],
                            base=service_schedule_df["service_start_date"],
                            orientation="h",
                            width=0.6,
                            marker_pattern_shape="x",
                            marker=dict(
                                color="rgba(63, 63, 63, 0.30)",
                                line=dict(color="black", width=1),
                            ),
                            name=f"Servicing = {callsign}",
                            customdata=service_schedule_df[
                                [
                                    "callsign",
                                    "duration_days",
                                    "service_start_date",
                                    "service_end_date",
                                    "registration",
                                ]
                            ],
                            hovertemplate="Servicing %{customdata[0]} (registration %{customdata[4]}) lasting %{customdata[1]:.1f} days<br>(%{customdata[2]|%a %-e %b %Y} to %{customdata[3]|%a %-e %b %Y})<extra></extra>",
                        )
                    )

                # if colour_by_cc_ec: # Logic for this needs COLORSCHEME and potentially cc_ec_reg_colour_lookup
                #     if not callsign_df.empty and 'care_cat' in callsign_df.columns:
                #         cc_ec_status = callsign_df["care_cat"].values[0] # This might need adjustment if multiple care_cat per callsign
                #         # cc_ec_reg_colour_lookup would also be needed here
                #         # marker_val = dict(color=list(COLORSCHEME.values())[cc_ec_reg_colour_lookup[cc_ec_status]])
                #     else:
                #         marker_val = dict(color=list(COLORSCHEME.values())[idx % len(COLORSCHEME)])
                # else: # Fallback or default coloring
                if show_outline:
                    marker_val = dict(
                        color=list(COLORSCHEME.values())[
                            idx % len(COLORSCHEME)
                        ],  # Use modulo for safety
                        line=dict(color="#FFA400", width=0.2),
                    )
                else:
                    marker_val = dict(
                        color=list(COLORSCHEME.values())[idx % len(COLORSCHEME)]
                    )  # Use modulo

                # Add in boxes showing the duration of individual calls
                if not callsign_df.empty:
                    resource_use_fig.add_trace(
                        go.Bar(
                            x=callsign_df["duration_seconds"],
                            y=callsign_df["y_pos"],
                            base=callsign_df["resource_use"],
                            orientation="h",
                            width=0.4,
                            marker=marker_val,
                            name=callsign,
                            customdata=callsign_df[
                                [
                                    "resource_use",
                                    "resource_use_end",
                                    "time_type",
                                    "duration_minutes",
                                    "registration",
                                    "care_cat",
                                ]
                            ],
                            hovertemplate="Response to %{customdata[5]} call from %{customdata[2]}<br>(registration %{customdata[4]}) lasting %{customdata[3]:.1f} minutes<br>(%{customdata[0]|%a %-e %b %Y %H:%M} to %{customdata[1]|%a %-e %b %Y %H:%M})<extra></extra>",
                        )
                    )

            # Layout tweaks
            resource_use_fig.update_layout(
                title_text="Resource Use Over Time",  # Changed from title
                barmode="overlay",
                xaxis=dict(
                    title_text="Time",  # Changed from title
                    type="date",
                ),
                yaxis=dict(
                    title_text="Callsign",  # Changed from title
                    tickmode="array",
                    tickvals=list(resource_dict.values()),
                    ticktext=list(
                        resource_dict.keys()
                    ),  # These should be the sorted unique callsigns
                    autorange="reversed",
                ),
                showlegend=True,
                height=700,
            )

            resource_use_fig.update_xaxes(
                rangeslider_visible=True,
                rangeselector=dict(
                    buttons=list(
                        [
                            dict(
                                count=1, label="1m", step="month", stepmode="backward"
                            ),
                            dict(
                                count=6, label="6m", step="month", stepmode="backward"
                            ),
                            dict(count=1, label="YTD", step="year", stepmode="todate"),
                            dict(count=1, label="1y", step="year", stepmode="backward"),
                            dict(step="all"),
                        ]
                    )
                ),
            )
            # Ensure the output directory exists
            # import os
            # os.makedirs("app/fig_outputs", exist_ok=True)
            # resource_use_fig.write_html("app/fig_outputs/resource_use_fig.html",full_html=False, include_plotlyjs='cdn')

            st.plotly_chart(
                resource_use_fig,
                use_container_width=True,  # Added for better responsiveness
            )

        # Call the fragment function to render its content
        resource_use_exploration_plots_fragment()

    def get_total_times_model(self, get_summary=False):
        if get_summary:
            utilisation_by_vehicle_summary = (
                self.utilisation_df_overall.groupby("vehicle_type")[
                    "resource_use_duration"
                ]
                .agg(["mean", "median", "min", "max", q10, q90])
                .round(1)
            )
            return utilisation_by_vehicle_summary

        else:
            return self.utilisation_df_overall

    def plot_historical_job_duration_vs_simulation_overall(
        self,
        use_poppins=True,
        write_to_html=False,
        html_output_filepath="fig_job_durations_historical.html",
        violin=False,
    ):
        fig = go.Figure()

        # print(self.historical_data.historical_job_durations_breakdown)
        historical_activity_times_overall = (
            self.historical_data.historical_job_durations_breakdown[
                self.historical_data.historical_job_durations_breakdown["name"]
                == "total_duration"
            ].copy()
        )

        historical_activity_times_overall["what"] = "Historical"

        self.resource_use_wide["what"] = "Simulated"

        # Force 'Simulated' to always appear first (left) and 'Historical' second (right)
        historical_activity_times_overall.rename(
            columns={"value": "resource_use_duration"}, inplace=True
        )

        full_activity_duration_df = pd.concat(
            [historical_activity_times_overall, self.resource_use_wide]
        )

        full_activity_duration_df["what"] = pd.Categorical(
            full_activity_duration_df["what"],
            categories=["Simulated", "Historical"],
            ordered=True,
        )

        if violin:
            fig = px.violin(
                full_activity_duration_df,
                x="vehicle_type",
                y="resource_use_duration",
                color="what",
                category_orders={"what": ["Simulated", "Historical"]},
            )
        else:
            fig = px.box(
                full_activity_duration_df,
                x="vehicle_type",
                y="resource_use_duration",
                color="what",
                category_orders={"what": ["Simulated", "Historical"]},
            )

        fig.update_layout(title="Resource Utilisation Duration vs Historical Averages")

        if write_to_html:
            fig.write_html(
                html_output_filepath, full_html=False, include_plotlyjs="cdn"
            )

        # Adjust font to match DAA style
        if use_poppins:
            fig.update_layout(font=dict(family="Poppins", size=18, color="black"))

        return fig

    def plot_total_times(self, by_run=False):
        if not by_run:
            fig = px.box(
                self.utilisation_df_overall,
                x="resource_use_duration",
                y="vehicle_type",
                color_discrete_sequence=list(COLORSCHEME.values()),
                labels={
                    "resource_use_duration": "Resource Use Duration (minutes)",
                    "vehicle_type": "Vehicle Type",
                },
            )
        else:
            fig = px.box(
                self.utilisation_df_overall,
                x="resource_use_duration",
                y="vehicle_type",
                color="run_number",
                color_discrete_sequence=list(COLORSCHEME.values()),
                labels={
                    "resource_use_duration": "Resource Use Duration (minutes)",
                    "vehicle_type": "Vehicle Type",
                    "run_number": "Run Number",
                },
            )

        return fig

    def plot_total_times_by_hems_or_pt_outcome(
        self, y, color, column_of_interest="hems_result", show_group_averages=True
    ):
        resource_use_wide = (
            self.resource_use_events_only_df[
                [
                    "P_ID",
                    "run_number",
                    "event_type",
                    "timestamp_dt",
                    "callsign_group",
                    "vehicle_type",
                    "callsign",
                    column_of_interest,
                ]
            ]
            .pivot(
                index=[
                    "P_ID",
                    "run_number",
                    "callsign_group",
                    "vehicle_type",
                    "callsign",
                    column_of_interest,
                ],
                columns="event_type",
                values="timestamp_dt",
            )
            .reset_index()
        ).copy()

        # If utilisation start time is missing, then set to start of model + warm-up time (if relevant)
        # as can assume this is a call that started before the warm-up period elapsed but finished
        # after the warm-up period elapsed
        # TODO: need to add in a check to ensure this only happens for calls at the end of the model,
        # not due to errors elsewhere that could fail to assign a resource end time
        resource_use_wide = fill_missing_values(
            resource_use_wide,
            "resource_use",
            get_param("warm_up_end_date", self.params_df),
        )

        # Calculate number of minutes the attending resource was in use on each call
        resource_use_wide["resource_use_duration"] = calculate_time_difference(
            resource_use_wide, "resource_use", "resource_use_end", unit="minutes"
        )

        # Calculate average duration per HEMS result
        mean_durations = (
            resource_use_wide.groupby(y)["resource_use_duration"]
            .mean()
            .sort_values(ascending=True)
        )

        # Create sorted list of HEMS results
        sorted_results = mean_durations.index.tolist()

        fig = px.box(
            resource_use_wide,
            x="resource_use_duration",
            y=y,
            color=color,
            color_discrete_sequence=list(COLORSCHEME.values()),
            category_orders={y: sorted_results},
            labels={
                "resource_use_duration": "Resource Use Duration (minutes)",
                "vehicle_type": "Vehicle Type",
                "hems_result": "HEMS Result",
                "outcome": "Patient Outcome",
                "callsign": "Callsign",
                "callsign_group": "Callsign Group",
            },
            height=900,
        )

        if show_group_averages:
            # Add vertical lines for group means
            # Map hems_result to its numerical position on the y-axis
            # Reversed mapping: top of plot gets highest numeric y-position
            result_order = sorted_results
            n = len(result_order)
            y_positions = {result: n - i - 1 for i, result in enumerate(result_order)}

            for result, avg_duration in mean_durations.items():
                y_center = y_positions[result]
                # Plot horizontal line centered at this group
                fig.add_shape(
                    type="line",
                    x0=avg_duration,
                    x1=avg_duration,
                    y0=y_center - 0.4,
                    y1=y_center + 0.4,
                    xref="x",
                    yref="y",
                    line=dict(color="black", dash="dash"),
                )

        return fig

    def calculate_ks_for_job_durations(
        self, historical_data_series, simulated_data_series, what="cars"
    ):
        statistic, p_value = ks_2samp(historical_data_series, simulated_data_series)

        if p_value > 0.05:
            st.success(f"""
There is no statistically significant difference between
the distributions of overall job durations for **{what}** in historical data and the
simulation (p = {format_sigfigs(p_value)})

This means that the pattern of total job durations produced by the simulation
matches the pattern seen in the real-world data —
for example, the average duration and variability of overall job durations
is sufficiently similar to what has been observed historically.
                        """)
        else:
            if p_value < 0.0001:
                p_value_formatted = "< 0.0001"
            else:
                p_value_formatted = format_sigfigs(p_value)

            ks_text_string_sig = f"""
    There is a statistically significant difference between the
    distributions of overall job durations from historical data and the
    simulation (p = {p_value_formatted}) for **{what}**.

    This means that the pattern of total job durations produced by the simulation
    does not match the pattern seen in the real-world data —
    for example, the average duration or variability of overall job durations
    may be different.

    The simulation may need to be adjusted to better
    reflect the patterns of job durations observed historically.

    """

            if statistic < 0.1:
                st.info(
                    ks_text_string_sig
                    + f"""Although the difference is
                        statistically significant, the actual magnitude
                        of the difference (D = {format_sigfigs(statistic, 3)}) is small.
                        This suggests the simulation's total job duration pattern is reasonably
                        close to reality.
                        """
                )

            elif statistic < 0.2:
                st.warning(
                    ks_text_string_sig
                    + f"""The KS statistic (D = {format_sigfigs(statistic, 3)})
                        indicates a moderate difference in
                        distribution. You may want to review the simulation model to
                        ensure it adequately reflects real-world variability.
                        """
                )

            else:
                st.error(
                    ks_text_string_sig
                    + f"""The KS statistic (D = {format_sigfigs(statistic, 3)})
                    suggests a large difference in overall job duration patterns.
                    The simulation may not accurately reflect historical
                    patterns and may need adjustment.
                    """
                )

    def PLOT_time_breakdown(self):
        job_times = [
            "time_allocation",
            "time_mobile",
            "time_to_scene",
            "time_on_scene",
            "time_to_hospital",
            "time_to_clear",
        ]

        run_results = self.run_results[self.run_results["event_type"].isin(job_times)][
            ["P_ID", "run_number", "time_type", "event_type", "vehicle_type"]
        ].copy()
        run_results["time_type"] = run_results["time_type"].astype("float")

        self.historical_data.historical_job_durations_breakdown["what"] = "Historical"
        run_results["what"] = "Simulated"

        full_job_duration_breakdown_df = pd.concat(
            [
                run_results.rename(
                    columns={"time_type": "value", "event_type": "name"}
                ).drop(columns=["P_ID", "run_number"]),
                self.historical_data.historical_job_durations_breakdown[
                    self.historical_data.historical_job_durations_breakdown["name"]
                    != "total_duration"
                ].drop(columns=["callsign", "job_identifier"]),
            ]
        )

        full_job_duration_breakdown_df["what"] = pd.Categorical(
            full_job_duration_breakdown_df["what"],
            categories=["Simulated", "Historical"],
            ordered=True,
        )

        full_job_duration_breakdown_df["name"] = (
            full_job_duration_breakdown_df["name"].str.replace("_", " ").str.title()
        )

        fig = px.box(
            full_job_duration_breakdown_df,
            y="value",
            x="name",
            color="what",
            facet_row="vehicle_type",
            category_orders={"what": ["Simulated", "Historical"]},
            labels={
                "value": "Duration (minutes)",
                # "vehicle_type": "Vehicle Type",
                "what": "Time Type (Historical Data vs Simulated Data)",
                "name": "Job Stage",
            },
            title="Comparison of Job Stage Durations by Vehicle Type",
            facet_row_spacing=0.2,
        )

        # Remove default facet titles
        fig.layout.annotations = [
            anno
            for anno in fig.layout.annotations
            if not anno.text.startswith("vehicle_type=")
        ]

        # Get the sorted unique vehicle types as used by Plotly (from top to bottom)
        # Plotly displays the first facet row (in terms of sorting) at the bottom
        vehicle_types = sorted(full_job_duration_breakdown_df["vehicle_type"].unique())

        n_rows = len(vehicle_types)
        row_heights = [1.0 - (i / n_rows) for i in range(n_rows)]

        for i, vehicle in enumerate(vehicle_types):
            fig.add_annotation(
                text=f"Vehicle Type: {vehicle.capitalize()}",
                xref="paper",
                yref="paper",
                x=0.5,
                y=row_heights[i] + 0.02,  # slightly above the subplot
                showarrow=False,
                font=dict(size=14, color="black"),
                xanchor="center",
            )

        # Increase spacing and top margin
        fig.update_layout(
            margin=dict(t=120),
            title_y=0.95,
            height=200 + 300 * n_rows,  # Adjust height based on number of rows
        )

        del run_results
        gc.collect()

        return fig

    def RETURN_prediction_cc_patients_sent_ec_resource(self) -> tuple:
        counts_df = (
            self.run_results[self.run_results["event_type"] == "resource_use"][
                ["run_number", "hems_res_category", "care_cat"]
            ]
            .value_counts()
            .reset_index()
        ).copy()

        counts_df_summary = (
            counts_df.groupby(["hems_res_category", "care_cat"])["count"]
            .agg(["mean", "min", "max"])
            .reset_index()
        )

        row_of_interest = counts_df_summary[
            (counts_df_summary["hems_res_category"] != "CC")
            & (counts_df_summary["care_cat"] == "CC")
        ]

        run_duration_days = float(get_param("sim_duration_days", self.params_df))

        return (
            (row_of_interest["mean"].values[0] / run_duration_days) * 365,
            (row_of_interest["min"].values[0] / run_duration_days) * 365,
            (row_of_interest["max"].values[0] / run_duration_days) * 365,
        )

    def RETURN_prediction_heli_benefit_patients_sent_car(self) -> tuple:
        counts_df = (
            self.run_results[self.run_results["event_type"] == "resource_use"][
                ["run_number", "vehicle_type", "heli_benefit"]
            ]
            .value_counts()
            .reset_index()
        ).copy()

        counts_df_summary = (
            counts_df.groupby(["vehicle_type", "heli_benefit"])["count"]
            .agg(["mean", "min", "max"])
            .reset_index()
        )

        row_of_interest = counts_df_summary[
            (counts_df_summary["vehicle_type"] == "car")
            & (counts_df_summary["heli_benefit"] == "y")
        ]

        run_duration_days = float(get_param("sim_duration_days", self.params_df))

        return (
            (row_of_interest["mean"].values[0] / run_duration_days) * 365,
            (row_of_interest["min"].values[0] / run_duration_days) * 365,
            (row_of_interest["max"].values[0] / run_duration_days) * 365,
        )

    def plot_missed_calls_boxplot(
        self,
        historical_results_obj,
        what="breakdown",
        historical_yearly_missed_calls_estimate=None,
    ):
        self.missed_jobs_per_run_breakdown["what"] = "Simulation"

        historical_results_obj.SIM_hist_missed_jobs_care_cat_breakdown["what"] = (
            "Historical (Simulated with Historical Rotas)"
        )

        full_df = pd.concat(
            [
                self.missed_jobs_per_run_breakdown,
                historical_results_obj.SIM_hist_missed_jobs_care_cat_breakdown,
            ]
        )

        full_df_no_resource_avail = full_df[
            full_df["time_type"] == "No Resource Available"
        ]

        if what == "breakdown":
            category_order = ["CC", "EC", "REG - Helicopter Benefit", "REG"]

            fig = px.box(
                full_df_no_resource_avail,
                x="jobs_per_year",
                y="care_cat",
                color="what",
                points="all",  # or "suspectedoutliers"
                boxmode="group",
                height=800,
                labels={
                    "jobs_per_year": "Estimated Average Jobs per Year",
                    "care_cat": "Care Category",
                    "what": "Simulation Results vs Simulated Historical Data",
                },
                category_orders={"care_cat": category_order},
            )

            fig.update_layout(
                legend=dict(orientation="h", y=1.1, x=0.5, xanchor="center")
            )

        if what == "summary":
            full_df_no_resource_avail_per_run = (
                full_df_no_resource_avail.groupby(["run_number", "what"])[
                    ["jobs_per_year"]
                ]
                .sum()
                .reset_index()
            )

            # Compute data bounds for x-axis
            x_min = full_df_no_resource_avail_per_run["jobs_per_year"].min()
            x_max = full_df_no_resource_avail_per_run["jobs_per_year"].max()
            padding = 0.20 * (x_max - x_min)
            x_range = [x_min - padding, x_max + padding]

            fig = px.box(
                full_df_no_resource_avail_per_run,
                x="jobs_per_year",
                y="what",
                color="what",
                points="all",
                boxmode="group",
                height=400,
            )

            # Update x-axis range
            fig.update_layout(xaxis_range=x_range, showlegend=False)

            if historical_yearly_missed_calls_estimate is not None:
                # Add the dotted vertical line
                fig.add_vline(
                    x=historical_yearly_missed_calls_estimate,
                    line_dash="dot",
                    line_color="black",
                    line_width=2,
                    annotation_text=f"Historical Estimate: {historical_yearly_missed_calls_estimate:.0f}",
                    annotation_position="top",
                )

            # Step 1: Compute Q1 and Q3
            q_df = (
                full_df_no_resource_avail_per_run.groupby("what")["jobs_per_year"]
                .quantile([0.25, 0.5, 0.75])
                .unstack()
                .reset_index()
                .rename(columns={0.25: "q1", 0.5: "median", 0.75: "q3"})
            )

            # Step 2: Calculate IQR and upper whisker cap
            q_df["iqr"] = q_df["q3"] - q_df["q1"]
            q_df["upper_whisker_cap"] = q_df["q3"] + 1.5 * q_df["iqr"]

            # Step 3: Find the max non-outlier per group
            max_non_outliers = full_df_no_resource_avail_per_run.merge(
                q_df[["what", "upper_whisker_cap"]], on="what"
            )
            max_non_outliers = (
                max_non_outliers[
                    max_non_outliers["jobs_per_year"]
                    <= max_non_outliers["upper_whisker_cap"]
                ]
                .groupby("what")["jobs_per_year"]
                .max()
                .reset_index()
                .rename(columns={"jobs_per_year": "max_non_outlier"})
            )

            # Step 4: Merge with median data
            annot_df = pd.merge(q_df[["what", "median"]], max_non_outliers, on="what")

            # Step 5: Add annotations just to the right of the whisker
            for _, row in annot_df.iterrows():
                fig.add_annotation(
                    x=row["max_non_outlier"] + padding * 0.1,
                    y=row["what"],
                    text=f"Median: {row['median']:.0f}",
                    showarrow=True,
                    arrowhead=2,
                    ax=0,
                    ay=0,
                    font=dict(size=12, color="gray"),
                    bgcolor="white",
                    bordercolor="gray",
                    borderwidth=1,
                )

        return fig

    def RETURN_missed_jobs_fig(self, care_category, what="average"):
        row = self.missed_jobs_per_run_care_cat_summary[
            (self.missed_jobs_per_run_care_cat_summary["care_cat"] == care_category)
            & (
                self.missed_jobs_per_run_care_cat_summary["time_type"]
                == "No Resource Available"
            )
        ]
        if what == "average":
            return row["jobs_per_year_average"].values[0]
        elif what == "min":
            return row["jobs_per_year_min"].values[0]
        elif what == "max":
            return row["jobs_per_year_max"].values[0]

    def PLOT_days_with_job_count_hist_ks(self):
        daily_call_counts = (
            self.call_df.groupby(["run_number", "day_date"])["P_ID"]
            .agg("count")
            .reset_index()
            .rename(columns={"P_ID": "Calls per Day"})
        )

        # Create histogram with two traces
        call_count_hist = go.Figure()

        # Simulated data
        call_count_hist.add_trace(
            go.Histogram(
                x=daily_call_counts["Calls per Day"],
                name="Simulated",
                histnorm="percent",
                xbins=dict(  # bins used for histogram
                    start=0.0,
                    end=max(daily_call_counts["Calls per Day"]) + 1,
                    size=1.0,
                ),
                opacity=0.75,
            )
        )

        # Historical data
        call_count_hist.add_trace(
            go.Histogram(
                x=self.historical_data.historical_daily_calls_breakdown["calls_in_day"],
                xbins=dict(  # bins used for histogram
                    start=0.0,
                    end=max(
                        self.historical_data.historical_daily_calls_breakdown[
                            "calls_in_day"
                        ]
                    )
                    + 1,
                    size=1.0,
                ),
                name="Historical",
                histnorm="percent",
                opacity=0.75,
            )
        )

        # Update layout
        call_count_hist.update_layout(
            title="Distribution of Jobs Per Day: Simulated vs Historical",
            barmode="overlay",
            bargap=0.03,
            xaxis=dict(tickmode="linear", tick0=0, dtick=1),
        )

        # Save and display
        call_count_hist.write_html(
            "app/fig_outputs/daily_calls_dist_histogram.html",
            full_html=False,
            include_plotlyjs="cdn",
        )

        call_count_hist.update_layout(
            font=dict(family="Poppins", size=18, color="black")
        )

        st.plotly_chart(call_count_hist)

        st.caption("""
This plot looks at the number of days across all repeats of the simulation where each given number of calls was observed (i.e. on how many days was one call received, two calls, three calls, and so on).
                        """)

        statistic, p_value = ks_2samp(
            daily_call_counts["Calls per Day"],
            self.historical_data.historical_daily_calls_breakdown["calls_in_day"],
        )

        if p_value > 0.05:
            st.success(f"""There is no statistically significant difference between
                                the distributions of call data from historical data and the
                                simulation (p = {format_sigfigs(p_value)})

                                This means that the pattern of calls produced by the simulation
                                matches the pattern seen in the real-world data —
                                for example, the frequency or variability of daily calls
                                is sufficiently similar to what has been observed historically.
                                """)
        else:
            ks_text_string_sig = f"""
There is a statistically significant difference between the
distributions of call data from historical data and
the simulation (p = {format_sigfigs(p_value)}).

This means that the pattern of calls produced by the simulation
does not match the pattern seen in the real-world data —
for example, the frequency or variability of daily calls
may be different.

The simulation may need to be adjusted to better
reflect the patterns of demand observed historically.

"""

            if statistic < 0.1:
                st.info(
                    ks_text_string_sig
                    + f"""Although the difference is
                                statistically significant, the actual magnitude
                                of the difference (D = {format_sigfigs(statistic)}) is small.
                                This suggests the simulation's call volume pattern is reasonably
                                close to reality.
                                """
                )

            elif statistic < 0.2:
                st.warning(
                    ks_text_string_sig
                    + f"""The KS statistic (D = {format_sigfigs(statistic)})
                                indicates a moderate difference in
                                distribution. You may want to review the simulation model to
                                ensure it adequately reflects real-world variability.
                                """
                )

            else:
                st.error(
                    ks_text_string_sig
                    + f"""The KS statistic (D = {format_sigfigs(statistic)})
                                suggests a large difference in call volume patterns.
                                The simulation may not accurately reflect historical
                                demand and may need adjustment.
                                """
                )

    def PLOT_daily_availability(self):
        return px.bar(
            self.daily_availability_df,
            x="month",
            y="theoretical_availability",
            facet_row="callsign",
        )

    def PLOT_events_over_time(self, runs=None):
        events_over_time_df = self.run_results[
            self.run_results["run_number"].isin(runs)
        ].copy()

        # Fix to deal with odd community cloud indexing bug
        if "P_ID" not in events_over_time_df.columns:
            events_over_time_df = events_over_time_df.reset_index()

        events_over_time_df["time_type"] = events_over_time_df["time_type"].astype(
            "str"
        )

        fig = px.scatter(
            events_over_time_df,
            x="timestamp_dt",
            y="time_type",
            # facet_row="run_number",
            # showlegend=False,
            color="time_type",
            height=800,
            title="Events Over Time - By Run",
        )

        fig.update_traces(marker=dict(size=3, opacity=0.5))

        fig.update_layout(
            yaxis_title="",  # Remove y-axis label
            yaxis_type="category",
            showlegend=False,
        )
        # Remove facet labels
        fig.for_each_annotation(lambda x: x.update(text=""))

        return fig

    def PLOT_cumulative_arrivals_per_run(self):
        return px.line(
            self.run_results[self.run_results["time_type"] == "arrival"],
            x="timestamp_dt",
            y="P_ID",
            color="run_number",
            height=800,
            title="Cumulative Arrivals Per Run",
        )

    def get_event_counts(self):
        self.event_counts_df = (
            pd.DataFrame(self.run_results[["run_number", "time_type"]].value_counts())
            .reset_index()
            .pivot(index="run_number", columns="time_type", values="count")
        )
        self.event_counts_long = self.event_counts_df.reset_index(drop=False).melt(
            id_vars="run_number"
        )

    def PLOT_event_funnel_plot(self, hems_events, run_select):
        return px.funnel(
            self.event_counts_long[
                (self.event_counts_long["time_type"].isin(hems_events))
                & (self.event_counts_long["run_number"].isin(run_select))
            ],
            facet_col="run_number",
            x="value",
            y="time_type",
            category_orders={"time_type": hems_events[::-1]},
        )

    def PLOT_per_patient_events(self, patient_df):
        fig = px.scatter(patient_df, x="timestamp_dt", y="time_type", color="time_type")

        fig.update_layout(yaxis_type="category")

        return fig

    def PLOT_outcome_variation_across_day(self, y_col):
        hourly_hems_result_counts = (
            self.run_results[self.run_results["time_type"] == "HEMS call start"]
            .groupby(["hems_result", "hour"])
            .size()
            .reset_index(name="count")
        ).copy()

        total_per_group = hourly_hems_result_counts.groupby("hour")["count"].transform(
            "sum"
        )

        hourly_hems_result_counts["proportion"] = (
            hourly_hems_result_counts["count"] / total_per_group
        )

        return px.bar(
            hourly_hems_result_counts,
            x="hour",
            y=y_col,
            color="hems_result",
        )
****************************************

****************************************
air_ambulance_des\des_hems.py
****************************************
import math
import os, simpy
from random import expovariate, uniform
from sim_tools.time_dependent import NSPPThinning
import pandas as pd

# from random import expovariate
from utils import Utils, SeededDistribution
from class_patient import Patient

# Revised class for HEMS availability
from air_ambulance_des.class_hems_availability import HEMSAvailability
from air_ambulance_des.class_hems import HEMS
from air_ambulance_des.class_ambulance import Ambulance
from datetime import timedelta
import warnings
import numpy as np
from math import floor

warnings.filterwarnings("ignore", category=RuntimeWarning)
# import all distributions
import ast
from numpy.random import SeedSequence
from typing import List, Dict, Tuple
from numpy.random import SeedSequence, default_rng
import random

import logging

logging.basicConfig(filename="log.txt", filemode="w", level=logging.DEBUG, format="")


class DES_HEMS:
    """
    # The DES_HEMS class

    This class contains everything require to undertake a single DES run for multiple patients
    over the specified duration.

    NOTE: The unit of sim is minutes

    """

    def __init__(
        self,
        run_number: int,
        sim_duration: int,
        warm_up_duration: int,
        sim_start_date: str,
        amb_data: bool,
        random_seed: int,
        demand_increase_percent: float,
        activity_duration_multiplier: float,
        print_debug_messages: bool,
    ):
        self.run_number = run_number + 1  # Add 1 so we don't have a run_number 0
        self.sim_duration = sim_duration
        self.warm_up_duration = warm_up_duration
        self.sim_start_date = sim_start_date

        self.random_seed = random_seed
        self.random_seed_sequence = SeedSequence(self.random_seed)

        self.print_debug_messages = print_debug_messages

        self.utils = Utils(
            master_seed=self.random_seed_sequence.spawn(1)[0],
            print_debug_messages=self.print_debug_messages,
        )

        self.utils.setup_seeds()

        self.demand_increase_percent = demand_increase_percent

        # Option for sampling calls per day by season or quarter
        self.daily_calls_by_quarter_or_season = "quarter"

        # Option to include/exclude ambulance service cases in addition to HEMS
        self.amb_data = amb_data
        # self.debug(f"Ambulance data values is {self.amb_data}")

        self.all_results_location = self.utils.ALL_RESULTS_CSV
        self.run_results_location = self.utils.RUN_RESULTS_CSV

        self.env = simpy.Environment()
        self.patient_counter = 0
        self.calls_today = 0
        self.new_day = pd.to_datetime("1900-01-01").date
        self.new_hour = -1

        self.hems_resources = HEMSAvailability(
            env=self.env,
            sim_start_date=sim_start_date,
            sim_duration=sim_duration,
            print_debug_messages=self.print_debug_messages,
            master_seed=self.random_seed_sequence,
            utility=self.utils,
        )

        # Set up empty list to store results prior to conversion to dataframe
        self.results_list = []

        # Set up data frame to capture time points etc. during the simulation
        # We might not need all of these, but simpler to capture them all for now.

        # This might be better as a separate 'data collection' class of some sort.
        # I am thinking that each resource will need its own entry for any given
        # patient to account for dual response (ambulance service and HEMS)
        # stand downs etc.
        self.results_df = None

        # self.inter_arrival_times_df = pd.read_csv('distribution_data/inter_arrival_times.csv')

        self.activity_duration_multiplier = activity_duration_multiplier

        # self.seeded_dists = self.utils.build_seeded_distributions(
        #     self.utils.activity_time_distr,
        #     master_seed=self.random_seed
        #     )

    def debug(self, message: str):
        if self.print_debug_messages:
            logging.debug(message)
            # print(message)

    def calls_per_hour(self, quarter: int) -> dict:
        """
        Function to return a dictionary of keys representing the hour of day
        and value representing the number of calls in that hour
        """

        # self.debug(f"There are going to be {self.calls_today} calls today and the current hour is {current_hour}")

        hourly_activity = self.utils.hourly_arrival_by_qtr_probs_df
        hourly_activity_for_qtr = hourly_activity[
            hourly_activity["quarter"] == quarter
        ][["hour", "proportion"]]

        calls_in_hours = []

        for i in range(0, self.calls_today):
            hour = pd.Series.sample(
                hourly_activity_for_qtr["hour"],
                weights=hourly_activity_for_qtr["proportion"],
                random_state=self.utils.rngs["calls_per_hour"],
            ).iloc[0]
            # self.debug(f"Chosen hour is {hour}")
            calls_in_hours.append(hour)

        calls_in_hours.sort()

        # self.debug(calls_in_hours)

        d = {}

        hours, counts = np.unique(calls_in_hours, return_counts=True)

        for i in range(len(hours)):
            d[hours[i]] = counts[i]

        return d

    def predetermine_call_arrival(self, current_hour: int, quarter: int) -> list:
        """
        Function to determine the number of calls in
        24 hours and the inter-arrival rate for calls in that period
        Returns a list of times that should be used in the yield timeout statement
        in a patient generator

        """

        hourly_activity = self.utils.hourly_arrival_by_qtr_probs_df
        hourly_activity_for_qtr = hourly_activity[
            hourly_activity["quarter"] == quarter
        ][["hour", "proportion"]]

        d = self.calls_per_hour(quarter)

        ia_time = []

        for index, row in hourly_activity_for_qtr.iterrows():
            hour = row["hour"]
            if hour >= current_hour and hour in d:
                count = d[hour]
                if count > 0:
                    scale = 60 / count  # mean of exponential = 1 / rate
                    calc_ia_time = self.utils.rngs[
                        "predetermine_call_arrival"
                    ].exponential(scale=scale)
                    tmp_ia_time = ((hour - current_hour) * 60) + calc_ia_time
                    current_hour += floor(tmp_ia_time / 60)
                    ia_time.append(tmp_ia_time)

        return ia_time

    def generate_calls(self):
        """
        **Call generator**

        Generate calls (and patients) until current time equals sim_duration + warm_up_duration.
        This method calculates number of calls per day and then distributes them according to distributions determined
        from historic data

        """

        while self.env.now < (self.sim_duration + self.warm_up_duration):
            # Get current day of week and hour of day
            [dow, hod, weekday, month, qtr, current_dt] = self.utils.date_time_of_call(
                self.sim_start_date, self.env.now
            )

            if self.new_hour != hod:
                self.new_hour = hod

                # self.debug("new hour")

            # If it is a new day, need to calculate how many calls
            # in the next 24 hours
            if self.new_day != current_dt.date():
                # self.debug("It's a new day")
                # self.debug(dow)
                # self.debug(f"{self.new_day} and {current_dt.date}")

                # Now have additional option of determining calls per day by quarter instead of season
                # self.calls_today = int(self.utils.inc_per_day(qtr, self.daily_calls_by_quarter_or_season) * (self.demand_increase_percent))

                # Try with actual sampled values instead
                self.calls_today = int(
                    self.utils.inc_per_day_samples(
                        qtr, self.daily_calls_by_quarter_or_season
                    )
                    * (self.demand_increase_percent)
                )

                # self.debug(f"{current_dt.date()} There will be {self.calls_today} calls today")
                # self.debug(f"{current_dt.date()} There will be {self.calls_today} calls today")

                self.new_day = current_dt.date()

                ia_dict = {}
                ia_dict = self.calls_per_hour(qtr)

                # self.debug(ia_dict)

                # Also run scripts to check HEMS resources to see whether they are starting/finishing service
                yield self.env.process(
                    self.hems_resources.daily_servicing_check(current_dt, hod, month)
                )

            if self.calls_today > 0:
                if hod in ia_dict.keys():
                    minutes_elapsed = 0
                    for i in range(0, ia_dict[hod]):
                        if minutes_elapsed < 59:
                            # Determine remaining time and sample a wait time
                            remaining_minutes = 59 - minutes_elapsed
                            # wait_time = random.randint(0, remaining_minutes)
                            wait_time = int(
                                self.utils.rngs["call_iat"].integers(
                                    0, remaining_minutes + 1
                                )
                            )

                            yield self.env.timeout(wait_time)
                            minutes_elapsed += wait_time

                            self.env.process(
                                self.generate_patient(
                                    dow, hod, weekday, month, qtr, current_dt
                                )
                            )

                            [dow, hod, weekday, month, qtr, current_dt] = (
                                self.utils.date_time_of_call(
                                    self.sim_start_date, self.env.now
                                )
                            )

                next_hr = current_dt.floor("h") + pd.Timedelta("1h")
                yield self.env.timeout(
                    math.ceil(
                        pd.to_timedelta(next_hr - current_dt).total_seconds() / 60
                    )
                )

            # self.debug('Out of loop')
            else:
                # Skip to tomorrow

                self.debug("Skip to tomorrow")

                [dow, hod, weekday, month, qtr, current_dt] = (
                    self.utils.date_time_of_call(self.sim_start_date, self.env.now)
                )
                next_day = current_dt.floor("d") + pd.Timedelta(days=1)
                self.debug("next day is {next_day}")
                yield self.env.timeout(
                    math.ceil(
                        pd.to_timedelta(next_hr - current_dt).total_seconds() / 60
                    )
                )

    def generate_patient(self, dow, hod, weekday, month, qtr, current_dt):
        self.patient_counter += 1

        # Create a new caller/patient
        pt = Patient(self.patient_counter)

        # Update patient instance with time-based values so the current time is known
        pt.day = dow
        pt.hour = hod
        pt.weekday = weekday
        pt.month = month
        pt.qtr = qtr
        pt.current_dt = current_dt

        # self.debug(f"Patient {pt.id} incident date: {pt.current_dt}")

        # Update patient instance with age, sex, AMPDS card, whether they are a HEMS' patient and if so, the HEMS result,
        pt.ampds_card = self.utils.ampds_code_selection(pt.hour)
        # self.debug(f"AMPDS card is {pt.ampds_card}")
        pt.age = self.utils.age_sampling(pt.ampds_card, 115)
        pt.sex = self.utils.sex_selection(pt.ampds_card)
        hems_cc_or_ec = self.utils.care_category_selection(pt.ampds_card)
        pt.hems_cc_or_ec = hems_cc_or_ec
        self.debug(
            f"{pt.current_dt}: {pt.id} Pt allocated to {pt.hems_cc_or_ec} from AMPDS {pt.ampds_card}"
        )

        pt.pt_outcome = self.utils.pt_outcome_selection(pt.hems_cc_or_ec, int(qtr))
        self.debug(
            f"{pt.current_dt}: {pt.id} Pt allocated to patient outcome: {pt.pt_outcome}"
        )

        self.add_patient_result_row(pt, "arrival", "arrival_departure")

        if self.amb_data:
            # TODO: We'll need the logic to decide whether it is an ambulance or HEMS case
            # if ambulance data is being collected too.
            self.debug("Ambulance case")
            pt.hems_case = (
                1
                if self.utils.rngs["hems_case"].uniform(0, 1) <= 0.5
                else pt.hems_case == 0
            )
        else:
            pt.hems_case = 1

        if pt.hems_case == 1:
            # self.debug(f"Going to callsign_group_selection with hour {pt.hour} and AMPDS {pt.ampds_card}")
            # pt.hems_pref_callsign_group = self.utils.callsign_group_selection(pt.ampds_card)
            # pt.hems_pref_callsign_group = self.utils.callsign_group_selection(pt.hems_cc_or_ec)
            pt.hems_pref_callsign_group = self.utils.callsign_group_selection()
            # self.debug(f"Callsign is {pt.hems_pref_callsign_group}")

            # Some % of calls have a helicopter benefit
            # Default to y for all patients
            helicopter_benefit = "n"

            # Update for REG patients based on historically observed patterns
            with open(
                "distribution_data/proportion_jobs_heli_benefit.txt", "r"
            ) as file:
                expected_prop_heli_benefit_jobs_reg = float(file.read().strip())
            if pt.hems_cc_or_ec == "REG":
                helicopter_benefit = (
                    "y"
                    if self.utils.rngs["helicopter_benefit_from_reg"].uniform(0, 1)
                    <= expected_prop_heli_benefit_jobs_reg
                    else "n"
                )

            # Update for CC patients based on historically observed patterns
            with open(
                "distribution_data/proportion_jobs_heli_benefit_cc.txt", "r"
            ) as file:
                expected_prop_heli_benefit_jobs_cc = float(file.read().strip())
            if pt.hems_cc_or_ec == "CC":
                helicopter_benefit = (
                    "y"
                    if self.utils.rngs["helicopter_benefit_from_cc"].uniform(0, 1)
                    <= expected_prop_heli_benefit_jobs_cc
                    else "n"
                )

            # Update for EC patients based on historically observed patterns
            with open(
                "distribution_data/proportion_jobs_heli_benefit_ec.txt", "r"
            ) as file:
                expected_prop_heli_benefit_jobs_ec = float(file.read().strip())
            if pt.hems_cc_or_ec == "EC":
                helicopter_benefit = (
                    "y"
                    if self.utils.rngs["helicopter_benefit_from_ec"].uniform(0, 1)
                    <= expected_prop_heli_benefit_jobs_ec
                    else "n"
                )

            # Following conversation with HT 21/5
            # Also count as having had a helicopter benefit if the patient is conveyed by HEMS
            # This is consistent with how things are done in the golden codes paper
            # https://static-content.springer.com/esm/art%3A10.1186%2Fs13049-023-01094-w/MediaObjects/13049_2023_1094_MOESM1_ESM.pdf
            # while having not always been coded in the historic dataset
            # (TODO: SR 30/5 I have commented this out for now and we should revisit this at some point -
            # as we might be slightly overestimating heli benefit patients as a result? These need to be
            # more closely/cleverly linked or just covered by the historic dataset instead)

            if pt.hems_result == "Patient Conveyed by HEMS":
                helicopter_benefit = "y"

            pt.hems_helicopter_benefit = helicopter_benefit
            self.add_patient_result_row(pt, pt.hems_cc_or_ec, "patient_care_category")
            self.add_patient_result_row(
                pt, pt.hems_helicopter_benefit, "patient_helicopter_benefit"
            )

            # pt.hems_pref_vehicle_type = self.utils.vehicle_type_selection(pt.hems_pref_callsign_group)
            pt.hems_pref_vehicle_type = self.utils.vehicle_type_selection_qtr(
                pt.hems_pref_callsign_group, int(qtr)
            )
            # pt.hems_pref_vehicle_type = 'helicopter'
            # pt.hems_pref_callsign_group = '70'
            # pt.hems_helicopter_benefit = 'y'

            self.add_patient_result_row(
                pt, pt.hems_pref_callsign_group, "resource_preferred_resource_group"
            )
            self.add_patient_result_row(
                pt, pt.hems_pref_vehicle_type, "resource_preferred_vehicle_type"
            )

            if (
                pt.hems_cc_or_ec == "CC" or pt.hems_cc_or_ec == "EC"
            ) and self.utils.rngs["know_cc_ec_benefit"].uniform(0, 1) <= 0.5:
                hems_res_list: list[
                    HEMS | None, str, HEMS | None
                ] = yield self.hems_resources.allocate_resource(pt)
            else:
                # Separate function to determine HEMS resource based on the preferred callsign group
                # (which is sampled from historical data), the preferred vehicle type
                hems_res_list: list[
                    HEMS | None, str, HEMS | None
                ] = yield self.hems_resources.allocate_regular_resource(pt)

            self.debug(f"{pt.id} hems_res_list: {hems_res_list}")

            hems_allocation = hems_res_list[0]

            # This will either contain the other resource in a callsign_group and HEMS category (EC/CC) or None
            hems_group_resource_allocation = hems_res_list[2]

            self.add_patient_result_row(
                pt, hems_res_list[1], "resource_preferred_outcome"
            )

            if hems_allocation is not None:
                # self.debug(f"allocated {hems_allocation.callsign}")

                self.env.process(
                    self.patient_journey(
                        hems_allocation, pt, hems_group_resource_allocation
                    )
                )
            else:
                # self.debug(f"{pt.current_dt} No HEMS resource available - non-DAAT land crew sent")
                self.env.process(self.patient_journey(None, pt, None))

    def patient_journey(
        self, hems_res: HEMS | None, patient: Patient, secondary_hems_res: HEMS | None
    ):
        """
        Send patient on their journey!
        """

        # self.debug(f"Patient journey triggered for {patient.id}")
        # self.debug(f"patient journey Time: {self.env.now}")
        # self.debug(hems_res.callsign)

        try:
            patient_enters_sim = self.env.now

            # Note that 'add_patient_result_row' does its own check for whether it's currently within
            # the warm-up period, so this does not need to be checked manually when adding result
            # rows in this section
            # not_in_warm_up_period = False if self.env.now < self.warm_up_duration else True

            # Add variables for quick determination of whether certain parts of the process should
            # be included per case
            hems_case = True if patient.hems_case == 1 else False
            hems_avail = True if hems_res is not None else False

            if not hems_avail:
                self.debug(f"Patient {patient.id}: No HEMS available")
                self.add_patient_result_row(patient, "No HEMS available", "queue")
                self.add_patient_result_row(
                    patient, "No Resource Available", "resource_request_outcome"
                )

            # if hems_avail
            else:
                # Record selected vehicle type and callsign group in patient object
                patient.hems_vehicle_type = hems_res.vehicle_type
                patient.hems_registration = hems_res.registration
                patient.callsign = hems_res.callsign

                self.add_patient_result_row(
                    patient, patient.hems_vehicle_type, "selected_vehicle_type"
                )
                self.add_patient_result_row(
                    patient, patient.hems_callsign_group, "selected_callsign_group"
                )

                # patient.hems_result = self.utils.hems_result_by_callsign_group_and_vehicle_type_selection(patient.hems_callsign_group, patient.hems_vehicle_type)
                # self.debug(f"{patient.hems_cc_or_ec} and {patient.hems_helicopter_benefit}")
                # patient.hems_result = self.utils.hems_result_by_care_category_and_helicopter_benefit_selection(patient.hems_cc_or_ec, patient.hems_helicopter_benefit)

                # Determine outcome
                # If we know a resource has been allocated, we can determine the output from historical patterns
                if hems_res:
                    patient.hems_result = self.utils.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs(
                        patient.pt_outcome,
                        int(patient.qtr),
                        patient.hems_vehicle_type,
                        patient.hems_callsign_group,
                        int(patient.hour),
                    )
                else:
                    # Default to what is recorded when no resource sent
                    patient.hems_result = "Unknown"

                self.debug(
                    f"{patient.current_dt}: PT_ID:{patient.id} Pt allocated to HEMS result: {patient.hems_result}"
                )
                self.add_patient_result_row(
                    patient, "HEMS Resource Available", "resource_request_outcome"
                )
                self.add_patient_result_row(patient, hems_res.callsign, "resource_use")
                self.debug(
                    f"{patient.current_dt} Patient {patient.id} (preferred callsign group {patient.hems_pref_callsign_group}, preferred resource type {patient.hems_pref_vehicle_type}) sent resource {hems_res.callsign}"
                )
                self.add_patient_result_row(
                    patient, hems_res.callsign, "callsign_group_resource_use"
                )

                # Check if HEMS result indicates that resource stood down before going mobile or en route
                no_HEMS_at_scene = (
                    True if patient.hems_result in ["Stand Down"] else False
                )
                # Check if HEMS result indicates no leaving scene/at hospital times
                no_HEMS_hospital = (
                    True
                    if patient.hems_result
                    in [
                        "Stand Down",
                        "Landed but no patient contact",
                        "Patient Treated but not conveyed by HEMS",
                    ]
                    else False
                )

            # self.debug('Inside hems_avail')
            if self.amb_data:
                ambulance = Ambulance()

            patient.time_in_sim = self.env.now - patient_enters_sim

            if hems_case and hems_avail:
                # self.debug(f"Adding result row with csg {patient.hems_callsign_group}")
                self.add_patient_result_row(patient, "HEMS call start", "queue")

            if self.amb_data:
                self.add_patient_result_row(patient, "AMB call start", "queue")

            # Allocation time --------------
            # Allocation will always take place if a resource is found

            if hems_case and hems_avail:
                # Calculate min and max permitted times.
                allocation_time = (
                    self.utils.activity_time(
                        patient.hems_vehicle_type, "time_allocation"
                    )
                    * self.activity_duration_multiplier
                )
                self.add_patient_result_row(patient, allocation_time, "time_allocation")
                self.add_patient_result_row(patient, "HEMS allocated to call", "queue")
                # self.debug(f"Vehicle type {patient.hems_vehicle_type} and allocation time is {allocation_time}")
                yield self.env.timeout(allocation_time)

            if self.amb_data:
                # self.debug('Ambulance allocation time')
                yield self.env.timeout(180)

            patient.time_in_sim = self.env.now - patient_enters_sim

            # Mobilisation time ---------------

            # Calculate mobile to time at scene (or stood down before)
            if hems_case and hems_avail:
                mobile_time = (
                    self.utils.activity_time(patient.hems_vehicle_type, "time_mobile")
                    * self.activity_duration_multiplier
                )
                self.add_patient_result_row(patient, mobile_time, "time_mobile")
                self.add_patient_result_row(patient, "HEMS mobile", "queue")
                yield self.env.timeout(mobile_time)

            if self.amb_data:
                # Determine allocation time for ambulance
                # Yield until done
                # self.debug('Ambulance time to going mobile')
                self.debug("Ambulance mobile")
                yield self.env.timeout(1)

            patient.time_in_sim = self.env.now - patient_enters_sim

            # On scene ---------------

            if hems_case and hems_avail and not no_HEMS_at_scene:
                tts_time = (
                    self.utils.activity_time(patient.hems_vehicle_type, "time_to_scene")
                    * self.activity_duration_multiplier
                )
                self.add_patient_result_row(patient, tts_time, "time_to_scene")
                self.add_patient_result_row(patient, "HEMS on scene", "queue")
                yield self.env.timeout(tts_time)

            if self.amb_data:
                # Determine allocation time for ambulance
                # Yield until done
                # self.debug('Ambulance time to scene')
                yield self.env.timeout(20)

            patient.time_in_sim = self.env.now - patient_enters_sim

            if self.amb_data:
                self.debug("Ambulance stand down en route")

            # Leaving scene ------------

            if hems_case and hems_avail and not no_HEMS_at_scene:
                tos_time = (
                    self.utils.activity_time(patient.hems_vehicle_type, "time_on_scene")
                    * self.activity_duration_multiplier
                )
                self.add_patient_result_row(patient, tos_time, "time_on_scene")
                yield self.env.timeout(tos_time)

                if no_HEMS_hospital:
                    self.add_patient_result_row(
                        patient, f"HEMS {patient.hems_result.lower()}", "queue"
                    )
                else:
                    self.add_patient_result_row(patient, "HEMS leaving scene", "queue")

            if self.amb_data:
                # self.debug('Ambulance on scene duration')
                yield self.env.timeout(120)
                self.debug("Ambulance leaving scene time")

            patient.time_in_sim = self.env.now - patient_enters_sim

            # Arrived destination time ------------

            if hems_case and hems_avail and not no_HEMS_hospital:
                travel_time = (
                    self.utils.activity_time(
                        patient.hems_vehicle_type, "time_to_hospital"
                    )
                    * self.activity_duration_multiplier
                )
                self.add_patient_result_row(patient, travel_time, "time_to_hospital")
                self.add_patient_result_row(
                    patient, "HEMS arrived destination", "queue"
                )
                yield self.env.timeout(travel_time)

            if self.amb_data:
                # self.debug('Ambulance travel time')
                yield self.env.timeout(30)
                self.debug("Ambulance at destination time")

            patient.time_in_sim = self.env.now - patient_enters_sim

            # Handover time ---------------

            # Not currently available

            # Clear time ------------

            if hems_case and hems_avail:
                clear_time = (
                    self.utils.activity_time(patient.hems_vehicle_type, "time_to_clear")
                    * self.activity_duration_multiplier
                )
                self.add_patient_result_row(patient, clear_time, "time_to_clear")
                self.add_patient_result_row(patient, "HEMS clear", "queue")
                yield self.env.timeout(clear_time)

            if self.amb_data:
                # self.debug('Ambulance clear time')
                yield self.env.timeout(60)

            patient.time_in_sim = self.env.now - patient_enters_sim

            if self.amb_data:
                self.debug("Ambulance clear time")

            # self.debug(f"Depart for patient {patient.id} on run {self.run_number}")

            self.add_patient_result_row(patient, "depart", "arrival_departure")
        finally:
            # Always return the resource at the end of the patient journey.
            if hems_res is not None:
                self.hems_resources.return_resource(hems_res, secondary_hems_res)
                self.debug(
                    f"Attempting to return {hems_res} and {secondary_hems_res} to resource store"
                )
                self.add_patient_result_row(
                    patient, hems_res.callsign, "resource_use_end"
                )

    def add_patient_result_row(
        self, patient: Patient, time_type: str, event_type: str, **kwargs
    ) -> None:
        """
        Convenience function to create a row of data for the results table

        """

        results = {
            "P_ID": patient.id,
            "run_number": self.run_number,
            "time_type": time_type,  # e.g. mobile, at scene, leaving scene etc.
            "event_type": event_type,  # for animation: arrival_departure, queue, resource_use, resource_use_end
            "timestamp": self.env.now,
            "timestamp_dt": self.sim_start_date + timedelta(minutes=self.env.now),
            "day": patient.day,
            "hour": patient.hour,
            "weekday": patient.weekday,
            "month": patient.month,
            "qtr": patient.qtr,
            "registration": patient.hems_registration,  # registration of allocated/attending vehicle
            "callsign": patient.callsign,  # callsign of allocated/attending vehicle
            "callsign_group": patient.hems_callsign_group,  # callsign group of allocated/attending vehicle
            "vehicle_type": patient.hems_vehicle_type,  # vehicle type (car/helicopter) of allocated/attending vehicle
            "hems_res_category": patient.hems_category,
            "ampds_card": patient.ampds_card,
            "age": patient.age,
            "sex": patient.sex,
            "care_cat": patient.hems_cc_or_ec,
            "heli_benefit": patient.hems_helicopter_benefit,
            "hems_result": patient.hems_result,
            "outcome": patient.pt_outcome,
            "hems_reg": patient.hems_registration,
        }

        # self.debug(results)

        # Add any additional items passed in **kwargs
        for key, value in kwargs.items():
            results[key] = value

        if self.env.now >= self.warm_up_duration:
            self.store_patient_results(results)

    def store_patient_results(self, results: dict) -> None:
        """
        Adds a row of data to the Class' `result_df` dataframe
        """

        self.results_list.append(results)

    def convert_results_to_df(self, results: dict) -> None:
        self.results_df = pd.DataFrame(results)
        try:
            self.results_df.set_index("P_ID", inplace=True)
        # TODO - Improve error handling here
        except KeyError:
            pass

    def write_all_results(self) -> None:
        """
        Writes the content of `result_df` to a csv file
        """
        # https://stackoverflow.com/a/30991707/3650230

        # Check if file exists...if it does, append data, otherwise create a new file with headers matching
        # the column names of `results_df`
        if not os.path.isfile(self.all_results_location):
            self.results_df.to_csv(self.all_results_location, header="column_names")
        else:  # else it exists so append without writing the header
            self.results_df.to_csv(self.all_results_location, mode="a", header=False)

    def write_run_results(self) -> None:
        """
        Writes the content of `result_dfs` to csv files that contains only the results from the
        single run

        Note that this cannot be done in a similar manner to write_all_results due to the impacts of
        the parallisation approach that is taken with joblib - depending on process timings it can lead to
        not all
        """

        self.results_df.to_csv(
            f"{Utils.RESULTS_FOLDER}/output_run_{self.run_number}.csv",
            header="column_names",
            index=True,
            encoding="utf-8-sig",
        )

    def run(self) -> None:
        """
        Function to start the simulation.

        """
        self.debug(
            f"HEMS class initialised with the following: run {self.run_number}, duration {self.sim_duration}, warm-up {self.warm_up_duration}, start date {self.sim_start_date}, demand increase multiplier {self.demand_increase_percent}, activity duration multiplier {self.activity_duration_multiplier}"
        )

        # Start entity generators
        self.env.process(self.generate_calls())

        # Run simulation
        self.env.run(until=(self.sim_duration + self.warm_up_duration))

        # Convert results to a dataframe
        self.convert_results_to_df(results=self.results_list)

        # Write run results to file
        self.write_all_results()
        self.write_run_results()
****************************************

****************************************
air_ambulance_des\des_parallel_process.py
****************************************
from datetime import datetime, timedelta
import logging
import time
import glob
import os
import sys
import pandas as pd
import multiprocessing as mp
from joblib import Parallel, delayed
from numpy.random import SeedSequence
from pathlib import Path

from air_ambulance_des.utils import Utils
from air_ambulance_des.des_hems import DES_HEMS


def write_run_params(model) -> None:
    """
    Write the simulation parameters used in a DES model run to a CSV file.

    Extracts key configuration parameters from the given model instance and writes them
    to a CSV file (`run_params_used.csv`) in the designated results folder. This provides
    a record of the conditions under which the simulation was executed.

    SR NOTE: It would also be good to add some sort of identifier to both the run results csv
        and this csv so you can confirm that they came from the same model execution (to avoid
        issues with calculations being incorrect if e.g. it was not possible to write one of the
        outputs due to an error, write protection, etc.)

    Parameters
    ----------
    model : DES_HEMS
        The simulation model instance from which to extract run parameters. Must have attributes
        including `sim_start_date`, `sim_duration`, `warm_up_duration`, `amb_data`, and
        `activity_duration_multiplier`.

    Returns
    -------
    None

    Notes
    -----
    - The function calculates `sim_end_date` and `warm_up_end_date` based on the provided
      `sim_start_date` and durations.
    - Output CSV includes timing and configuration values such as:
        - Simulation duration and warm-up duration
        - Simulation start, end, and warm-up end datetimes
        - Whether ambulance data was used
        - Activity duration multiplier
        - Model execution timestamp
        - Assumed summer and winter period start dates
    - The output CSV is saved to `Utils.RESULTS_FOLDER/run_params_used.csv`.
    - Only supports a single simulation's parameters at a time.
    - Future improvements may include adding a unique identifier for linking this file
      with the corresponding simulation results.

    See Also
    --------
    runSim : Runs an individual simulation and optionally calls this function.
    parallelProcessJoblib : Executes multiple `runSim` runs in parallel.
    """
    # Ensure sim_start_date is a datetime object
    sim_start_date = model.sim_start_date
    if isinstance(sim_start_date, str):
        sim_start_date = datetime.fromisoformat(
            sim_start_date
        )  # Convert string to datetime

    sim_end_date = sim_start_date + timedelta(minutes=model.sim_duration)
    warm_up_end_date = sim_start_date + timedelta(minutes=model.warm_up_duration)

    params_df = pd.DataFrame.from_dict(
        {
            "sim_duration": [model.sim_duration],  # model time unit is minutes
            "sim_duration_hours": [model.sim_duration / 60],
            "sim_duration_days": [model.sim_duration / 60 / 24],
            "warm_up_duration": [model.warm_up_duration],
            "sim_start_date": [sim_start_date],
            "sim_end_date": [sim_end_date],
            "warm_up_end_date": [warm_up_end_date],
            "amb_data": [model.amb_data],
            "model_exec_time": [datetime.now()],
            # Assuming summer hours are quarters 2 and 3 i.e. April-September
            # This is defined in class_hems and will need updating here too
            "summer_start_date": [f"{sim_start_date.year}-04-01"],
            "winter_start_date": [f"{sim_start_date.year}-10-01"],
            "activity_duration_multiplier": [model.activity_duration_multiplier],
        },
        orient="index",
        columns=["value"],
    )

    params_df.index.name = "parameter"

    params_df.to_csv(
        f"{Utils.RESULTS_FOLDER}/run_params_used.csv", header="column_names"
    )


try:
    __file__
except NameError:
    __file__ = sys.argv[0]


def runSim(
    run: int,
    total_runs: int,
    sim_duration: int,
    warm_up_time: int,
    sim_start_date: datetime,
    amb_data: bool,
    random_seed: int = 101,
    save_params_csv: bool = True,
    demand_increase_percent: float = 1.0,
    activity_duration_multiplier: float = 1.0,
    print_debug_messages: bool = False,
):
    """
    Run a single discrete event simulation (DES) for the specified configuration.

    This function initializes and runs a DES_HEMS simulation model for a given run number,
    logs performance and configuration details, and optionally saves simulation parameters.

    Parameters
    ----------
    run : int
        The index of the current simulation run (starting from 0).
    total_runs : int
        Total number of simulation runs being executed.
    sim_duration : int
        The total simulation duration (excluding warm-up) in minutes or other time unit.
    warm_up_time : int
        The warm-up period to discard before recording results.
    sim_start_date : datetime
        The datetime representing the start of the simulation.
    amb_data : bool
        Flag indicating whether ambulance-specific data should be generated in the simulation.
    save_params_csv : bool, optional
        If True, simulation parameters will be saved to CSV (only on the first run). Default is True.
    demand_increase_percent : float, optional
        Factor by which demand is increased (e.g., 1.10 for a 10% increase). Default is 1.0.
    activity_duration_multiplier : float, optional
        Multiplier to adjust generated durations of activities (e.g., 1.10 for a 10% increase). Default is 1.0.
    print_debug_messages : bool, optional
        If True, enables additional debug message output. Default is False.

    Returns
    -------
    pandas.DataFrame
        A DataFrame containing the simulation results.

    Notes
    -----
    - Only the first run (i.e., `run == 0`) will trigger the saving of run parameters if `save_params_csv` is True.
    - Timing information and configuration details are printed and logged for transparency.
    """
    # print(f"Inside runSim and {sim_start_date} and {what_if_sim_run}")

    print(
        f"{Utils.current_time()}: Demand increase set to {demand_increase_percent * 100}%"
    )
    logging.debug(
        f"{Utils.current_time()}: Demand increase set to {demand_increase_percent * 100}%"
    )

    start = time.process_time()

    print(f"{Utils.current_time()}: Run {run + 1} of {total_runs}")
    logging.debug(f"{Utils.current_time()}: Run {run + 1} of {total_runs}")

    # print(f"Sim start date is {sim_start_date}")
    daa_model = DES_HEMS(
        run_number=run,
        sim_duration=sim_duration,
        warm_up_duration=warm_up_time,
        sim_start_date=sim_start_date,
        amb_data=amb_data,
        random_seed=random_seed,
        demand_increase_percent=demand_increase_percent,
        activity_duration_multiplier=activity_duration_multiplier,
        print_debug_messages=print_debug_messages,
    )
    daa_model.run()

    print(
        f"{Utils.current_time()}: Run {run + 1} took {round((time.process_time() - start) / 60, 1)} minutes to run"
    )
    logging.debug(
        f"{Utils.current_time()}: Run {run + 1} took {round((time.process_time() - start) / 60, 1)} minutes to run"
    )

    # SR NOTE: This could cause issues if we decide to use 1 as the starting number of runs
    if (run == 0) and (save_params_csv):
        write_run_params(daa_model)

    return daa_model.results_df


def collateRunResults() -> None:
    """
    Collates results from a series of runs into a single csv
    """
    matching_files = glob.glob(os.path.join(Utils.RESULTS_FOLDER, "output_run_*.csv"))

    combined_df = pd.concat([pd.read_csv(f) for f in matching_files], ignore_index=True)

    combined_df.to_csv(Utils.RUN_RESULTS_CSV, index=True, encoding="utf-8-sig")

    for file in matching_files:
        os.remove(file)


def removeExistingResults(remove_run_results_csv=False) -> None:
    """
    Removes results from previous simulation runs
    """
    matching_files = glob.glob(os.path.join(Utils.RESULTS_FOLDER, "output_run_*.csv"))

    for file in matching_files:
        os.remove(file)

    all_results_file_path = os.path.join(Utils.RESULTS_FOLDER, "all_results.csv")
    if os.path.isfile(all_results_file_path):
        os.unlink(all_results_file_path)

    if remove_run_results_csv:
        run_results_file_path = os.path.join(Utils.RESULTS_FOLDER, "run_results.csv")
        if os.path.isfile(run_results_file_path):
            os.unlink(run_results_file_path)


def parallelProcessJoblib(
    total_runs: int,
    sim_duration: int,
    warm_up_time: int,
    sim_start_date: datetime,
    amb_data: bool,
    save_params_csv: bool = True,
    demand_increase_percent: float = 1.0,
    activity_duration_multiplier: float = 1.0,
    print_debug_messages: bool = False,
    master_seed=42,
    n_cores=-1,
):
    """
    Execute multiple simulation runs in parallel using joblib.

    Parameters
    ----------
    total_runs : int
        The total number of simulation runs to execute.
    sim_duration : int
        The duration of each simulation (excluding warm-up).
    warm_up_time : int
        The warm-up period to discard before recording results.
    sim_start_date : datetime
        The datetime representing the start of the simulation.
    amb_data : bool
        Flag indicating whether ambulance-specific data should be generated in the simulation.
    save_params_csv : bool, optional
        If True, simulation parameters will be saved to CSV during the first run. Default is True.
    demand_increase_percent : float, optional
        Factor by which demand is increased (e.g., 1.10 for a 10% increase). Default is 1.0.
    activity_duration_multiplier : float, optional
        Multiplier to adjust generated durations of activities (e.g., 1.10 for a 10% increase). Default is 1.0.
    print_debug_messages : bool, optional
        If True, enables additional debug message output during each run. Default is False.
    master_seed : int, optional
        Master seed used to generate the uncorrelated random number streams for replication consistency
    n_cores : int, optional
        Determines how many parallel simulations will be run at a time (which is equivalent to the
        number of cores). Default is -1, which means all available cores will be utilised.

    Returns
    -------
    list of pandas.DataFrame
        A list of DataFrames, each containing the results of an individual simulation run.

    Notes
    -----
    - This function distributes simulation runs across available CPU cores using joblib's
    `Parallel` and `delayed` utilities. Each run is executed with the `runSim` function,
    with the given configuration parameters.
    - Runs are distributed across all available CPU cores (`n_jobs=-1`).
    - Only the first run will save parameter data if `save_params_csv` is True.
    - If a single output csv is required, use of this function must be followed by
      collateRunResults()
    """

    # seeds = Utils.get_distribution_seeds(master_seed=master_seed, n_replications=total_runs,
    #                                      n_dists_per_rep=30)

    # Generate a number of uncorrelated seeds that will always be the same given the same
    # master seed (which is determined as a parameter)
    # We start with a SeedSequence from the master seed, and then generate a number of
    # child SeedSequences equal to the total number of runs
    seed_sequence = SeedSequence(master_seed).spawn(total_runs)
    # We then turn these seeds into integer random numbers, and we will pass a different seed
    # into each run of the simulation.
    seeds = [i.generate_state(1)[0] for i in seed_sequence]

    # Run the simulation in parallel, using all available cores
    return Parallel(n_jobs=n_cores)(
        delayed(runSim)(
            run=run,
            total_runs=total_runs,
            sim_duration=sim_duration,
            warm_up_time=warm_up_time,
            sim_start_date=sim_start_date,
            amb_data=amb_data,
            random_seed=seeds[run],
            save_params_csv=save_params_csv,
            demand_increase_percent=demand_increase_percent,
            activity_duration_multiplier=activity_duration_multiplier,
            print_debug_messages=print_debug_messages,
        )
        for run in range(total_runs)
    )
****************************************

****************************************
air_ambulance_des\distribution_fit_utils.py
****************************************
from csv import QUOTE_ALL
import glob
import math
import os
import sys
import numpy as np
import pandas as pd
import json
import itertools
from fitter import Fitter, get_common_distributions
from datetime import timedelta
from utils import Utils
from des_parallel_process import (
    parallelProcessJoblib,
    collateRunResults,
    removeExistingResults,
)
from datetime import datetime

# TODO: Important fix for rerunning with new input data
# import visualisation._job_outcome_calculation as _job_outcome_calculation

from pathlib import Path

PACKAGE_ROOT = Path(__file__).resolve().parent  # air_ambulance_des/
REPO_ROOT = PACKAGE_ROOT.parent
DATA_PATH = REPO_ROOT / "data"


class DistributionFitUtils:
    """
    # The DistributionFitUtils classa

    This class will import a CSV, undertake some light
    wrangling and then determine distributions and probabilities required
    for the Discrete Event Simulation

    example usage:
        my_data = DistributionFitUtils('data/my_data.csv')
        my_data.import_and_wrangle()

    """

    def __init__(
        self, file_path: str, calculate_school_holidays=False, school_holidays_years=0
    ):
        self.file_path = file_path
        self.df = pd.DataFrame()

        # The number of additional years of school holidays
        # that will be calculated over that maximum date in the provided dataset
        self.school_holidays_years = school_holidays_years
        self.calculate_school_holidays = calculate_school_holidays

        self.times_to_fit = [
            {
                "hems_result": "Patient Treated but not conveyed by HEMS",
                "times_to_fit": [
                    "time_allocation",
                    "time_mobile",
                    "time_to_scene",
                    "time_on_scene",
                    "time_to_clear",
                ],
            },
            {
                "hems_result": "Patient Conveyed by HEMS",
                "times_to_fit": [
                    "time_allocation",
                    "time_mobile",
                    "time_to_scene",
                    "time_on_scene",
                    "time_to_hospital",
                    "time_to_clear",
                ],
            },
            {
                "hems_result": "Patient Conveyed by land with HEMS",
                "times_to_fit": [
                    "time_allocation",
                    "time_mobile",
                    "time_to_scene",
                    "time_on_scene",
                    "time_to_hospital",
                    "time_to_clear",
                ],
            },
            {
                "hems_result": "Stand Down",
                "times_to_fit": ["time_allocation", "time_mobile", "time_to_clear"],
            },
            {
                "hems_result": "Landed but no patient contact",
                "times_to_fit": [
                    "time_allocation",
                    "time_mobile",
                    "time_to_scene",
                    "time_on_scene",
                    "time_to_clear",
                ],
            },
        ]

        self.sim_tools_distr_plus = [
            "poisson",
            "bernoulli",
            "triang",
            "erlang",
            "weibull_min",
            "expon_weib",
            "betabinom",
            "pearson3",
            "cauchy",
            "chi2",
            "expon",
            "exponpow",
            "gamma",
            "lognorm",
            "norm",
            "powerlaw",
            "rayleigh",
            "uniform",
            "neg_binomial",
            "zip",
        ]
        # SR 16-04-2025 Have hardcoded the common distributions
        # to make setup for random number generation more robust
        # + get_common_distributions()

    def removeExistingResults(self, folder: str) -> None:
        """
        Removes results from previous fitting
        """

        matching_files = glob.glob(os.path.join(folder, "*.*"))

        print(matching_files)

        for file in matching_files:
            os.remove(file)

    def getBestFit(self, q_times, distr=get_common_distributions(), show_summary=False):
        """

        Convenience function for Fitter.
        Returns model and parameters that is considered
        the 'best fit'.

        TODO: Determine how Fitter works this out

        """

        if q_times.size > 0:
            if len(distr) > 0:
                f = Fitter(q_times, timeout=60, distributions=distr)
            else:
                f = Fitter(q_times, timeout=60)
            f.fit()
            if show_summary == True:
                f.summary()
            return f.get_best()
        else:
            return {}

    def import_and_wrangle(self):
        """

        Function to import CSV, add additional columns that are required
        and then sequentially execute other class functions to generate
        the probabilities and distributions required for the DES.

        TODO: Additional logic is required to check the imported CSV
        for missing values, incorrect columns names etc.

        """

        try:
            df = pd.read_csv(self.file_path, quoting=QUOTE_ALL)
            self.df = df

            # Perhaps run some kind of checking function here.

        except FileNotFoundError:
            print(f"Cannot locate that file")

        # If everything is okay, crack on...
        self.df["inc_date"] = pd.to_datetime(self.df["inc_date"])
        self.df["date_only"] = pd.to_datetime(df["inc_date"].dt.date)
        self.df["hour"] = self.df["inc_date"].dt.hour  # Hour of the day
        self.df["day_of_week"] = self.df[
            "inc_date"
        ].dt.day_name()  # Day of the week (e.g., Monday)
        self.df["month"] = self.df["inc_date"].dt.month
        self.df["quarter"] = self.df["inc_date"].dt.quarter
        self.df["first_day_of_month"] = (
            self.df["inc_date"].to_numpy().astype("datetime64[M]")
        )

        # Replacing a upper quartile limit on job cycle times and
        # instead using a manually specified time frame.
        # This has the advantage of allowing manual amendment of the falues
        # on the front-end
        # self.max_values_df = self.upper_allowable_time_bounds()
        self.min_max_values_df = pd.read_csv(
            "actual_data/upper_allowable_time_bounds.csv"
        )
        # print(self.min_max_values_df)

        # This will be needed for other datasets, but has already been computed for DAA
        # self.df['ampds_card'] = self.df['ampds_code'].str[:2]

        self.removeExistingResults(Utils.HISTORICAL_FOLDER)
        self.removeExistingResults(Utils.DISTRIBUTION_FOLDER)

        # get proportions of AMPDS card by hour of day
        self.hour_by_ampds_card_probs()

        # Determine 'best' distributions for time-based data
        self.activity_time_distributions()

        # Calculate probability patient will be female based on AMPDS card
        self.sex_by_ampds_card_probs()

        # Determine 'best' distributions for age ranges straitifed by AMPDS card
        self.age_distributions()

        # Alternative approach to IA times. Start with probabilty of call at given hour stratified by quarter
        self.hourly_arrival_by_qtr_probs()

        # Calculates the mean and standard deviation of the number of incidents per day stratified by quarter
        self.incidents_per_day()
        self.incidents_per_day_samples()

        # Calculate probability of enhanced or critical care being required based on AMPDS card
        self.enhanced_or_critical_care_by_ampds_card_probs()

        # Calculate HEMS result
        self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs()

        # Calculate probability of callsign being allocated to a job based on AMPDS card and hour of day
        # self.callsign_group_by_ampds_card_and_hour_probs()
        # self.callsign_group_by_ampds_card_probs()
        # self.callsign_group_by_care_category()
        self.callsign_group()

        # Calculate probability of a particular vehicle type based on callsign group and month of year
        # self.vehicle_type_by_month_probs()
        self.vehicle_type_by_quarter_probs()
        # self.vehicle_type_probs() # Similar to previous but without monthly stratification since ad hoc unavailability should account for this.

        # Calculate the patient outcome (conveyed, deceased, unknown)
        self.patient_outcome_by_care_category_and_quarter_probs()

        # ============= ARCHIVED CODE ================= #
        # Calculate the mean inter-arrival times stratified by yearly quarter and hour of day
        # self.inter_arrival_times()
        # ============= END ARCHIVED CODE ================= #

        # ============= ARCHIVED CODE ================= #
        # Calculate probably of patient outcome
        # Note - this still needs to be run to support another one?
        # Calculate probability of a specific patient outcome being allocated to a job based on HEMS result and callsign
        # self.pt_outcome_by_hems_result_and_care_category_probs()
        # ============= END ARCHIVED CODE ================= #

        # ============= ARCHIVED CODE ================= #
        # self.hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_probs()
        # ============= END ARCHIVED CODE ================= #

        # ============= ARCHIVED CODE ================= #
        # Calculate probabily of HEMS result being allocated to a job based on callsign and hour of day
        # self.hems_result_by_callsign_group_and_vehicle_type_probs()
        # ============= END ARCHIVED CODE ================= #

        # ============= ARCHIVED CODE ================= #
        # Calculate probability of HEMS result being allocated to a job based on care category and helicopter benefit
        # self.hems_result_by_care_cat_and_helicopter_benefit_probs()
        # ============= END ARCHIVED CODE ================= #

        # Calculate school holidays since servicing schedules typically avoid these dates
        if self.calculate_school_holidays:
            self.school_holidays()

        # Calculate historical data
        self.historical_monthly_totals()
        self.historical_monthly_totals_by_callsign()
        self.historical_monthly_totals_by_day_of_week()
        self.historical_median_time_of_activities_by_month_and_resource_type()
        self.historical_monthly_totals_by_hour_of_day()
        self.historical_monthly_resource_utilisation()
        self.historical_monthly_totals_all_calls()
        self.historical_daily_calls_breakdown()
        self.historical_job_durations_breakdown()
        self.historical_missed_jobs()
        self.historical_jobs_per_day_per_callsign()
        self.historical_care_cat_counts()

        # Calculate proportions of ad hoc unavailability
        try:
            # self.ad_hoc_unavailability()
            self.ad_hoc_unavailability(
                period_start="2022-08-01", period_end="2024-07-31"
            )
        except FileNotFoundError:
            print("Couldn't find ad-hoc unavailability file")

    def hour_by_ampds_card_probs(self):
        """

        Calculates the proportions of calls that are triaged with
        a specific AMPDS card. This is stratified by hour of day

        TODO: Determine whether this should also be stratified by yearly quarter

        """
        category_counts = (
            self.df.groupby(["hour", "ampds_card"]).size().reset_index(name="count")
        )
        total_counts = category_counts.groupby("hour")["count"].transform("sum")
        category_counts["proportion"] = round(
            category_counts["count"] / total_counts, 4
        )

        # category_counts['ampds_card'] = category_counts['ampds_card'].apply(lambda x: str(x).zfill(2))

        category_counts.to_csv(
            "distribution_data/hour_by_ampds_card_probs.csv", mode="w+"
        )

    def sex_by_ampds_card_probs(self):
        """

        Calculates the probability that the patient will be female
        stratified by AMPDS card.

        """
        age_df = self.df
        category_counts = (
            age_df.groupby(["ampds_card", "sex"]).size().reset_index(name="count")
        )
        total_counts = category_counts.groupby("ampds_card")["count"].transform("sum")
        category_counts["proportion"] = round(
            category_counts["count"] / total_counts, 3
        )

        category_counts[category_counts["sex"] == "Female"].to_csv(
            "distribution_data/sex_by_ampds_card_probs.csv", mode="w+"
        )

    def activity_time_distributions(self):
        """

        Determine the 'best' distribution for each phase of a call
        i.e. Allocation time, Mobilisation time, Time to scene
        Time on scene, Travel time to hospital and handover, Time to clear.
        Not all times will apply to all cases, so the class 'times_to_fit'
        variable is a list of dictionaries, which contains the times to fit

        The data is currently stratitied by HEMS_result and vehicle type fields.

        """

        vehicle_type = self.df["vehicle_type"].dropna().unique()

        # We'll need to make sure that where a distribution is missing that the time is set to 0 in the model.
        # Probably easier than complicated logic to determine what times should be available based on hems_result

        final_distr = []

        for row in self.times_to_fit:
            # print(row)
            for ttf in row["times_to_fit"]:
                for vt in vehicle_type:
                    # print(f"HEMS result is {row['hems_result']} times_to_fit is {ttf} and vehicle type is  {vt}")

                    # This line might not be required if data quality is determined when importing the data
                    max_time = self.min_max_values_df[
                        self.min_max_values_df["time"] == ttf
                    ].max_value_mins.iloc[0]
                    min_time = self.min_max_values_df[
                        self.min_max_values_df["time"] == ttf
                    ].min_value_mins.iloc[0]

                    # print(f"Max time is {max_time} and Min time is {min_time}")

                    if ttf == "time_on_scene":
                        # There is virtually no data for HEMS_result other than patient conveyed
                        # which is causing issues with fitting. For time on scene, will
                        # use a simplified fitting ignoring hems_result as a category
                        fit_times = self.df[
                            (self.df.vehicle_type == vt)
                            & (self.df[ttf] >= min_time)
                            & (self.df[ttf] <= max_time)
                        ][ttf]
                    else:
                        fit_times = self.df[
                            (self.df.vehicle_type == vt)
                            & (self.df[ttf] >= min_time)
                            & (self.df[ttf] <= max_time)
                            & (self.df.hems_result == row["hems_result"])
                        ][ttf]
                    # print(fit_times[:10])
                    best_fit = self.getBestFit(
                        fit_times, distr=self.sim_tools_distr_plus
                    )
                    # print(best_fit)

                    return_dict = {
                        "vehicle_type": vt,
                        "time_type": ttf,
                        "best_fit": best_fit,
                        "hems_result": row["hems_result"],
                        "n": len(fit_times),
                    }
                    # print(return_dict)
                    final_distr.append(return_dict)

        with open(
            "distribution_data/activity_time_distributions.txt", "w+"
        ) as convert_file:
            convert_file.write(json.dumps(final_distr))
        convert_file.close()

    def age_distributions(self):
        """

        Determine the 'best' distribution for age stratified by
        AMPDS card

        """

        age_distr = []

        age_df = self.df[["age", "ampds_card"]].dropna()
        ampds_cards = age_df["ampds_card"].unique()
        print(ampds_cards)

        for card in ampds_cards:
            fit_ages = age_df[age_df["ampds_card"] == card]["age"]
            best_fit = self.getBestFit(fit_ages, distr=self.sim_tools_distr_plus)
            return_dict = {
                "ampds_card": str(card),
                "best_fit": best_fit,
                "n": len(fit_ages),
            }
            age_distr.append(return_dict)

        with open("distribution_data/age_distributions.txt", "w+") as convert_file:
            convert_file.write(json.dumps(age_distr))
        convert_file.close()

    # def inter_arrival_times(self):
    #     """

    #         Calculate the mean inter-arrival times for patients
    #         stratified by hour, and and yearly quarter

    #     """

    #     ia_df = self.df[['date_only', 'quarter', 'hour']].dropna()

    #     count_df = ia_df.groupby(['hour', 'date_only', 'quarter']).size().reset_index(name='n')

    #     ia_times_df = (
    #         count_df.groupby(['hour', 'quarter'])
    #         .agg(
    #             # max_arrivals_per_hour=('n', lambda x: round(60 / np.max(x), 3)),
    #             # min_arrivals_per_hour=('n', lambda x: round(60 / np.min(x),3)),
    #             mean_cases=('n', lambda x: round(x.mean(), 1)),
    #             # sd_cases=('n', lambda x: round(x.std(), 3)),
    #             mean_iat=('n', lambda x: 60 / x.mean())
    #             # n=('n', 'size')
    #         )
    #         .reset_index()
    #     )
    #     # Additional column for NSPPThinning
    #     ia_times_df['t'] = ia_times_df['hour']
    #     ia_times_df['arrival_rate'] = ia_times_df['mean_iat'].apply(lambda x: 1/x)

    #     ia_times_df.to_csv('distribution_data/inter_arrival_times.csv', mode='w+')

    def incidents_per_day(self):
        """
        Fit distributions for number of incidents per day using actual daily counts,
        applying year-based weighting to reflect trends (e.g., 2024 busier than 2023),
        stratified by season and quarter.
        """
        import math
        import json
        import numpy as np

        inc_df = self.df[["inc_date", "date_only", "quarter"]].copy()
        inc_df["year"] = inc_df["date_only"].dt.year

        # Daily incident counts
        inc_per_day = (
            inc_df.groupby("date_only").size().reset_index(name="jobs_per_day")
        )
        inc_per_day["year"] = inc_per_day["date_only"].dt.year

        # Merge quarter and season from self.df
        date_info = self.df[["date_only", "quarter"]].drop_duplicates()

        if "season" not in self.df.columns:
            date_info["season"] = date_info["quarter"].map(
                lambda q: "winter" if q in [1, 4] else "summer"
            )
        else:
            date_info = date_info.merge(
                self.df[["date_only", "season"]].drop_duplicates(),
                on="date_only",
                how="left",
            )

        inc_per_day = inc_per_day.merge(date_info, on="date_only", how="left")

        # Weight settings - simple implementation rather than biased mean thing
        year_weights = {
            2023: 1.0,
            2024: 4.0,  # 10% more weight to 2024
        }

        # ========== SEASONAL DISTRIBUTIONS ==========
        jpd_distr = []

        for season in inc_per_day["season"].dropna().unique():
            filtered = inc_per_day[inc_per_day["season"] == season].copy()
            filtered["weight"] = filtered["year"].map(year_weights).fillna(1.0)

            # Repeat rows proportionally by weight
            replicated = filtered.loc[
                filtered.index.repeat((filtered["weight"] * 10).round().astype(int))
            ]["jobs_per_day"]

            best_fit = self.getBestFit(
                np.array(replicated), distr=self.sim_tools_distr_plus
            )

            jpd_distr.append(
                {
                    "season": season,
                    "best_fit": best_fit,
                    "min_n_per_day": int(replicated.min()),
                    "max_n_per_day": int(replicated.max()),
                    "mean_n_per_day": float(replicated.mean()),
                }
            )

        with open("distribution_data/inc_per_day_distributions.txt", "w+") as f:
            json.dump(jpd_distr, f)

        # ========== QUARTERLY DISTRIBUTIONS ==========
        jpd_qtr_distr = []

        for quarter in inc_per_day["quarter"].dropna().unique():
            filtered = inc_per_day[inc_per_day["quarter"] == quarter].copy()
            filtered["weight"] = filtered["year"].map(year_weights).fillna(1.0)

            replicated = filtered.loc[
                filtered.index.repeat((filtered["weight"] * 10).round().astype(int))
            ]["jobs_per_day"]

            best_fit = self.getBestFit(
                np.array(replicated), distr=self.sim_tools_distr_plus
            )

            jpd_qtr_distr.append(
                {
                    "quarter": int(quarter),
                    "best_fit": best_fit,
                    "min_n_per_day": int(replicated.min()),
                    "max_n_per_day": int(replicated.max()),
                    "mean_n_per_day": float(replicated.mean()),
                }
            )

        with open("distribution_data/inc_per_day_qtr_distributions.txt", "w+") as f:
            json.dump(jpd_qtr_distr, f)

    def incidents_per_day_samples(self, weight_map=None, scale_factor=10):
        """
        Create weighted empirical samples of incidents per day by season and quarter.
        """

        inc_df = self.df[["date_only", "quarter"]].copy()
        inc_df["year"] = inc_df["date_only"].dt.year
        inc_df["season"] = inc_df["quarter"].map(
            lambda q: "winter" if q in [1, 4] else "summer"
        )

        # Get raw counts per day
        daily_counts = (
            inc_df.groupby("date_only").size().reset_index(name="jobs_per_day")
        )
        daily_counts["year"] = daily_counts["date_only"].dt.year

        # Merge back in season/quarter info
        meta_info = self.df[["date_only", "quarter"]].drop_duplicates()
        if "season" in self.df.columns:
            meta_info = meta_info.merge(
                self.df[["date_only", "season"]].drop_duplicates(),
                on="date_only",
                how="left",
            )
        else:
            meta_info["season"] = meta_info["quarter"].map(
                lambda q: "winter" if q in [1, 4] else "summer"
            )

        daily_counts = daily_counts.merge(meta_info, on="date_only", how="left")

        # Year weight map
        if weight_map is None:
            weight_map = {2023: 1.0, 2024: 1.1}

        # Compute weights
        daily_counts["weight"] = daily_counts["year"].map(weight_map).fillna(1.0)

        # Storage
        empirical_samples = {}

        # Season-based
        for season in daily_counts["season"].dropna().unique():
            filtered = daily_counts[daily_counts["season"] == season].copy()
            repeated = filtered.loc[
                filtered.index.repeat(
                    (filtered["weight"] * scale_factor).round().astype(int)
                )
            ]["jobs_per_day"].tolist()

            empirical_samples[season] = repeated

        # Quarter-based
        for quarter in daily_counts["quarter"].dropna().unique():
            filtered = daily_counts[daily_counts["quarter"] == quarter].copy()
            repeated = filtered.loc[
                filtered.index.repeat(
                    (filtered["weight"] * scale_factor).round().astype(int)
                )
            ]["jobs_per_day"].tolist()

            empirical_samples[f"Q{int(quarter)}"] = repeated

        with open("distribution_data/inc_per_day_samples.json", "w") as f:
            json.dump(empirical_samples, f)

    def enhanced_or_critical_care_by_ampds_card_probs(self):
        """

        Calculates the probabilty of enhanced or critical care resource beign required
        based on the AMPDS card

        """

        ec_df = self.df[["ampds_card", "ec_benefit", "cc_benefit"]].copy()

        def assign_care_category(row):
            # There are some columns with both EC and CC benefit selected
            # this function will allocate to only 1
            if row["cc_benefit"] == "y":
                return "CC"
            elif row["ec_benefit"] == "y":
                return "EC"
            else:
                return "REG"

        ec_df["care_category"] = ec_df.apply(assign_care_category, axis=1)

        care_cat_counts = (
            ec_df.groupby(["ampds_card", "care_category"])
            .size()
            .reset_index(name="count")
        )
        total_counts = care_cat_counts.groupby("ampds_card")["count"].transform("sum")

        care_cat_counts["proportion"] = round(
            care_cat_counts["count"] / total_counts, 3
        )

        care_cat_counts.to_csv(
            "distribution_data/enhanced_or_critical_care_by_ampds_card_probs.csv",
            mode="w+",
            index=False,
        )

    def patient_outcome_by_care_category_and_quarter_probs(self):
        """

        Calculates the probabilty of a patient outcome based on care category and yearly quarter

        """

        po_df = self.df[["quarter", "ec_benefit", "cc_benefit", "pt_outcome"]].copy()

        def assign_care_category(row):
            # There are some columns with both EC and CC benefit selected
            # this function will allocate to only 1
            if row["cc_benefit"] == "y":
                return "CC"
            elif row["ec_benefit"] == "y":
                return "EC"
            else:
                return "REG"

        # There are some values that are missing e.g. CC quarter 1 Deceased
        # I think we've had problems when trying to sample from this kind of thing before
        # As a fallback, ensure that 'missing' combinations are given a count and proportion of 0
        outcomes = po_df["pt_outcome"].unique()
        care_categories = ["CC", "EC", "REG"]
        quarters = po_df["quarter"].unique()

        all_combinations = pd.DataFrame(
            list(itertools.product(outcomes, care_categories, quarters)),
            columns=["pt_outcome", "care_category", "quarter"],
        )

        po_df["care_category"] = po_df.apply(assign_care_category, axis=1)

        po_cat_counts = (
            po_df.groupby(["pt_outcome", "care_category", "quarter"])
            .size()
            .reset_index(name="count")
        )

        merged = pd.merge(
            all_combinations,
            po_cat_counts,
            on=["pt_outcome", "care_category", "quarter"],
            how="left",
        ).fillna({"count": 0})
        merged["count"] = merged["count"].astype(int)

        total_counts = merged.groupby(["care_category", "quarter"])["count"].transform(
            "sum"
        )
        merged["proportion"] = round(merged["count"] / total_counts.replace(0, 1), 3)

        merged.to_csv(
            "distribution_data/patient_outcome_by_care_category_and_quarter_probs.csv",
            mode="w+",
            index=False,
        )

    # def hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_probs(self):
    #     """

    #         Calculates the probabilty of a given HEMS result based on
    #         patient outcome, yearly quarter, vehicle type and callsign group

    #     """

    #     hr_df = self.df[['hems_result', 'quarter', 'pt_outcome', 'vehicle_type', 'callsign_group']].copy()

    #     # There are some values that are missing e.g. CC quarter 1 Deceased
    #     # I think we've had problems when trying to sample from this kind of thing before
    #     # As a fallback, ensure that 'missing' combinations are given a count and proportion of 0
    #     # hems_results = hr_df['hems_result'].unique()
    #     # outcomes = hr_df['pt_outcome'].unique()
    #     # vehicle_categories = [x for x in hr_df['vehicle_type'].unique() if pd.notna(x)]
    #     # callsign_group_categories = hr_df['callsign_group'].unique()
    #     # quarters = hr_df['quarter'].unique()

    #     # all_combinations = pd.DataFrame(list(itertools.product(hems_results, outcomes, vehicle_categories, callsign_group_categories, quarters)),
    #     #                             columns=['hems_result', 'pt_outcome', 'vehicle_type', 'callsign_group', 'quarter'])

    #     hr_cat_counts = hr_df.groupby(['hems_result', 'pt_outcome', 'vehicle_type', 'callsign_group', 'quarter']).size().reset_index(name='count')

    #     # merged = pd.merge(all_combinations, hr_cat_counts,
    #     #               on=['hems_result', 'pt_outcome', 'vehicle_type', 'callsign_group', 'quarter'],
    #     #               how='left').fillna({'count': 0})
    #     # merged['count'] = merged['count'].astype(int)

    #     merged = hr_cat_counts

    #     total_counts = merged.groupby(['pt_outcome', 'vehicle_type', 'callsign_group', 'quarter'])['count'].transform('sum')
    #     merged['proportion'] = round(merged['count'] / total_counts.replace(0, 1), 3)

    #     merged.to_csv('distribution_data/hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_probs.csv', mode = "w+", index = False)

    def hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs(
        self,
    ):
        """

        Calculates the probabilty of a given HEMS result based on
            - patient outcome
            - yearly quarter
            - time of day (7am - 6pm, 7pm - 6am)
            - vehicle type
            - and callsign group

        """
        self.df["inc_date"] = pd.to_datetime(self.df["inc_date"])
        self.df["hour"] = self.df["inc_date"].dt.hour
        self.df["time_of_day"] = self.df["hour"].apply(
            lambda x: "day" if x >= 7 and x <= 18 else "night"
        )

        hr_df = self.df[
            [
                "hems_result",
                "quarter",
                "pt_outcome",
                "vehicle_type",
                "callsign_group",
                "time_of_day",
            ]
        ].copy()

        # There are some values that are missing e.g. CC quarter 1 Deceased
        # I think we've had problems when trying to sample from this kind of thing before
        # As a fallback, ensure that 'missing' combinations are given a count and proportion of 0
        # hems_results = hr_df['hems_result'].unique()
        # outcomes = hr_df['pt_outcome'].unique()
        # vehicle_categories = [x for x in hr_df['vehicle_type'].unique() if pd.notna(x)]
        # callsign_group_categories = hr_df['callsign_group'].unique()
        # quarters = hr_df['quarter'].unique()

        # all_combinations = pd.DataFrame(list(itertools.product(hems_results, outcomes, vehicle_categories, callsign_group_categories, quarters)),
        #                             columns=['hems_result', 'pt_outcome', 'vehicle_type', 'callsign_group', 'quarter'])

        hr_cat_counts = (
            hr_df.groupby(
                [
                    "hems_result",
                    "pt_outcome",
                    "vehicle_type",
                    "callsign_group",
                    "quarter",
                    "time_of_day",
                ]
            )
            .size()
            .reset_index(name="count")
        )

        # merged = pd.merge(all_combinations, hr_cat_counts,
        #               on=['hems_result', 'pt_outcome', 'vehicle_type', 'callsign_group', 'quarter'],
        #               how='left').fillna({'count': 0})
        # merged['count'] = merged['count'].astype(int)

        merged = hr_cat_counts

        total_counts = merged.groupby(
            ["pt_outcome", "vehicle_type", "callsign_group", "quarter", "time_of_day"]
        )["count"].transform("sum")
        merged["proportion"] = round(merged["count"] / total_counts.replace(0, 1), 3)

        merged.to_csv(
            "distribution_data/hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs.csv",
            mode="w+",
            index=False,
        )

    def hourly_arrival_by_qtr_probs(self):
        """

        Calculates the proportions of calls arriving in any given hour
        stratified by yearly quarter

        """

        ia_df = self.df[["quarter", "hour"]].dropna()

        hourly_counts = (
            ia_df.groupby(["hour", "quarter"]).size().reset_index(name="count")
        )
        total_counts = hourly_counts.groupby(["quarter"])["count"].transform("sum")
        hourly_counts["proportion"] = round(hourly_counts["count"] / total_counts, 4)

        hourly_counts.sort_values(by=["quarter", "hour"]).to_csv(
            "distribution_data/hourly_arrival_by_qtr_probs.csv", mode="w+"
        )

    def callsign_group_by_ampds_card_and_hour_probs(self):
        """

        Calculates the probabilty of a specific callsign being allocated to
        a call based on the AMPDS card category and hour of day

        """
        callsign_counts = (
            self.df.groupby(["ampds_card", "hour", "callsign_group"])
            .size()
            .reset_index(name="count")
        )

        total_counts = callsign_counts.groupby(["ampds_card", "hour"])[
            "count"
        ].transform("sum")
        callsign_counts["proportion"] = round(
            callsign_counts["count"] / total_counts, 4
        )

        callsign_counts.to_csv(
            "distribution_data/callsign_group_by_ampds_card_and_hour_probs.csv",
            mode="w+",
            index=False,
        )

    def callsign_group_by_ampds_card_probs(self):
        """

        Calculates the probabilty of a specific callsign being allocated to
        a call based on the AMPDS card category

        """

        callsign_df = self.df[self.df["callsign_group"] != "Other"]

        callsign_counts = (
            callsign_df.groupby(["ampds_card", "callsign_group"])
            .size()
            .reset_index(name="count")
        )

        total_counts = callsign_counts.groupby(["ampds_card"])["count"].transform("sum")
        callsign_counts["proportion"] = round(
            callsign_counts["count"] / total_counts, 4
        )

        callsign_counts.to_csv(
            "distribution_data/callsign_group_by_ampds_card_probs.csv",
            mode="w+",
            index=False,
        )

    def callsign_group(self):
        """
        Calculates the probabilty of a specific callsign being allocated to
        a call
        """
        df = self.df.copy()

        # Convert time fields to numeric
        time_fields = [
            "time_allocation",
            "time_mobile",
            "time_to_scene",
            "time_on_scene",
            "time_to_hospital",
            "time_to_clear",
        ]
        for col in time_fields:
            df[col] = pd.to_numeric(df[col], errors="coerce")

        # Calculate total job duration in minutes
        df["job_duration_min"] = df[time_fields].sum(axis=1, skipna=True)

        # Compute job start and end times
        df["start_time"] = df["inc_date"]
        df["end_time"] = df["start_time"] + pd.to_timedelta(
            df["job_duration_min"], unit="m"
        )

        # Sort jobs by start time
        df = df.sort_values(by="start_time").reset_index(drop=True)

        # Set to hold indices of jobs that overlap (but only the later-starting ones)
        overlapping = set()

        # Check for overlaps
        for i in range(len(df)):
            this_end = df.at[i, "end_time"]

            # Compare only to later jobs
            for j in range(i + 1, len(df)):
                next_start = df.at[j, "start_time"]
                if next_start >= this_end:
                    break  # No more possible overlaps
                # If it starts before i's job ends, it's overlapping
                overlapping.add(j)

        # Mark the overlaps in the dataframe
        df["overlaps"] = df.index.isin(overlapping)

        # Filter out overlapping jobs
        df_no_overlap = df[~df["overlaps"]]

        # We will use the ad-hoc unavailability to remove any instances where we already know one of
        # the vehicles to be recorded as offline

        data = df_no_overlap.copy()

        # TODO: Ideally we'd also remove any instances where we know one of the helos to have been
        # off for servicing if that data is available
        ad_hoc = pd.read_csv(
            "external_data/ad_hoc.csv", parse_dates=["offline", "online"]
        )
        ad_hoc["aircraft"] = ad_hoc["aircraft"].str.lower()

        data["inc_date"] = pd.to_datetime(data["inc_date"], format="ISO8601")
        data["vehicle"] = data["vehicle"].str.lower()

        # Create a cross-join between data and ad_hoc
        data["key"] = 1
        ad_hoc["key"] = 1
        merged = data.merge(ad_hoc, on="key")

        # Keep rows where inc_date falls within the offline period
        overlap = merged[
            (merged["inc_date"] >= merged["offline"])
            & (merged["inc_date"] <= merged["online"])
        ]

        # Filter out those rows from the original data
        df_no_overlap = data[~data["inc_date"].isin(overlap["inc_date"])].drop(
            columns="key"
        )

        callsign_df = df_no_overlap.assign(
            helicopter_benefit=np.select(
                [
                    df_no_overlap["cc_benefit"] == "y",
                    df_no_overlap["ec_benefit"] == "y",
                    df_no_overlap["hems_result"].isin(
                        [
                            "Stand Down En Route",
                            "Landed but no patient contact",
                            "Stand Down Before Mobile",
                        ]
                    ),
                ],
                ["y", "y", "n"],
                default=df_no_overlap["helicopter_benefit"],
            ),
            care_category=np.select(
                [
                    df_no_overlap["cc_benefit"] == "y",
                    df_no_overlap["ec_benefit"] == "y",
                ],
                ["CC", "EC"],
                default="REG",
            ),
        )

        callsign_df = callsign_df[callsign_df["callsign_group"] != "Other"]

        callsign_counts = (
            callsign_df.groupby(["callsign_group"]).size().reset_index(name="count")
        )

        total_counts = len(callsign_df)
        callsign_counts["proportion"] = round(
            callsign_counts["count"] / total_counts, 4
        )

        callsign_counts.to_csv(
            "distribution_data/callsign_group_probs.csv", mode="w+", index=False
        )

    # def callsign_group_by_care_category(self):
    #     """

    #         Calculates the probabilty of a specific callsign being allocated to
    #         a call based on the care category

    #     """
    #     df = self.df.copy()

    #     # Convert time fields to numeric
    #     time_fields = [
    #         "time_allocation", "time_mobile", "time_to_scene",
    #         "time_on_scene", "time_to_hospital", "time_to_clear"
    #     ]
    #     for col in time_fields:
    #         df[col] = pd.to_numeric(df[col], errors="coerce")

    #     # Calculate total job duration in minutes
    #     df["job_duration_min"] = df[time_fields].sum(axis=1, skipna=True)

    #     # Compute job start and end times
    #     df["start_time"] = df["inc_date"]
    #     df["end_time"] = df["start_time"] + pd.to_timedelta(df["job_duration_min"], unit="m")

    #     # Sort jobs by start time
    #     df = df.sort_values(by="start_time").reset_index(drop=True)

    #     # Set to hold indices of jobs that overlap (but only the later-starting ones)
    #     overlapping = set()

    #     # Check for overlaps
    #     for i in range(len(df)):
    #         this_end = df.at[i, "end_time"]

    #         # Compare only to later jobs
    #         for j in range(i + 1, len(df)):
    #             next_start = df.at[j, "start_time"]
    #             if next_start >= this_end:
    #                 break  # No more possible overlaps
    #             # If it starts before i's job ends, it's overlapping
    #             overlapping.add(j)

    #     # Mark the overlaps in the dataframe
    #     df["overlaps"] = df.index.isin(overlapping)

    #     # Filter out overlapping jobs
    #     df_no_overlap = df[~df["overlaps"]]

    #     callsign_df = (
    #         df_no_overlap
    #         .assign(
    #             helicopter_benefit=np.select(
    #                 [
    #                     df_no_overlap["cc_benefit"] == "y",
    #                     df_no_overlap["ec_benefit"] == "y",
    #                     df_no_overlap["hems_result"].isin([
    #                         "Stand Down En Route",
    #                         "Landed but no patient contact",
    #                         "Stand Down Before Mobile"
    #                     ])
    #                 ],
    #                 ["y", "y", "n"],
    #                 default=df_no_overlap["helicopter_benefit"]
    #             ),
    #             care_category=np.select(
    #                 [
    #                     df_no_overlap["cc_benefit"] == "y",
    #                     df_no_overlap["ec_benefit"] == "y"
    #                 ],
    #                 ["CC", "EC"],
    #                 default="REG"
    #             )
    #         )
    #     )

    #     callsign_df = callsign_df[callsign_df['callsign_group'] != 'Other']

    #     callsign_counts = callsign_df.groupby(['care_category', 'callsign_group']).size().reset_index(name='count')

    #     total_counts = callsign_counts.groupby(['care_category'])['count'].transform('sum')
    #     callsign_counts['proportion'] = round(callsign_counts['count'] / total_counts, 4)

    #     callsign_counts.to_csv('distribution_data/callsign_group_by_care_category_probs.csv', mode = "w+", index=False)

    # ========== ARCHIVED CODE ============ #
    # def vehicle_type_by_month_probs(self):
    #     """

    #         Calculates the probabilty of a car/helicopter being allocated to
    #         a call based on the callsign group and month of the year

    #     """
    #     callsign_counts = self.df.groupby(['callsign_group', 'month', 'vehicle_type']).size().reset_index(name='count')

    #     total_counts = callsign_counts.groupby(['callsign_group', 'month'])['count'].transform('sum')
    #     callsign_counts['proportion'] = round(callsign_counts['count'] / total_counts, 4)

    #     callsign_counts.to_csv('distribution_data/vehicle_type_by_month_probs.csv', mode = "w+")
    # ========== END ARCHIVED CODE ============ #
    def vehicle_type_by_quarter_probs(self):
        """

        Calculates the probabilty of a car/helicopter being allocated to
        a call based on the callsign group and quarter of the year

        Quarter accounts for seasonal variation without being as affected by

        """
        data = self.df.copy()

        # We will use the ad-hoc unavailability to remove any instances where we already know one of
        # the vehicles to be recorded as offline

        # TODO: Ideally we'd also remove any instances where we know one of the helos to have been
        # off for servicing if that data is available
        ad_hoc = pd.read_csv(
            "external_data/ad_hoc.csv", parse_dates=["offline", "online"]
        )
        ad_hoc["aircraft"] = ad_hoc["aircraft"].str.lower()

        data["inc_date"] = pd.to_datetime(data["inc_date"], format="ISO8601")
        data["vehicle"] = data["vehicle"].str.lower()

        # Create a cross-join between data and ad_hoc
        data["key"] = 1
        ad_hoc["key"] = 1
        merged = data.merge(ad_hoc, on="key")

        # Keep rows where inc_date falls within the offline period
        overlap = merged[
            (merged["inc_date"] >= merged["offline"])
            & (merged["inc_date"] <= merged["online"])
        ]

        # Filter out those rows from the original data
        filtered_data = data[~data["inc_date"].isin(overlap["inc_date"])].drop(
            columns="key"
        )

        # First, calculate overall props
        callsign_counts = (
            filtered_data.groupby(["callsign_group", "vehicle_type"])
            .size()
            .reset_index(name="count")
        )

        total_counts = callsign_counts.groupby(["callsign_group"])["count"].transform(
            "sum"
        )
        callsign_counts["proportion"] = round(
            callsign_counts["count"] / total_counts, 4
        )

        callsign_counts.to_csv("distribution_data/vehicle_type_probs.csv", mode="w+")

        # Then, redo by quarter
        callsign_counts = (
            filtered_data.groupby(["callsign_group", "quarter", "vehicle_type"])
            .size()
            .reset_index(name="count")
        )

        total_counts = callsign_counts.groupby(["callsign_group", "quarter"])[
            "count"
        ].transform("sum")
        callsign_counts["proportion"] = round(
            callsign_counts["count"] / total_counts, 4
        )

        callsign_counts.to_csv(
            "distribution_data/vehicle_type_by_quarter_probs.csv", mode="w+"
        )

    def vehicle_type_probs(self):
        """

        Calculates the probabilty of a car/helicopter being allocated to
        a call based on the callsign group

        """

        callsign_counts = (
            self.df.groupby(["callsign_group", "vehicle_type"])
            .size()
            .reset_index(name="count")
        )

        total_counts = callsign_counts.groupby(["callsign_group"])["count"].transform(
            "sum"
        )
        callsign_counts["proportion"] = round(
            callsign_counts["count"] / total_counts, 4
        )

        callsign_counts.to_csv("distribution_data/vehicle_type_probs.csv", mode="w+")

    def hems_result_by_callsign_group_and_vehicle_type_probs(self):
        """

        Calculates the probabilty of a specific HEMS result being allocated to
        a call based on the callsign group and hour of day

        TODO: These probability calculation functions could probably be refactored into a single
        function and just specify columns and output name

        """
        hems_counts = (
            self.df.groupby(["hems_result", "callsign_group", "vehicle_type"])
            .size()
            .reset_index(name="count")
        )

        total_counts = hems_counts.groupby(["callsign_group", "vehicle_type"])[
            "count"
        ].transform("sum")
        hems_counts["proportion"] = round(hems_counts["count"] / total_counts, 4)

        hems_counts.to_csv(
            "distribution_data/hems_result_by_callsign_group_and_vehicle_type_probs.csv",
            mode="w+",
            index=False,
        )

    # ========== ARCHIVED CODE ============ #
    # def hems_result_by_care_cat_and_helicopter_benefit_probs(self):
    #     """

    #         Calculates the probabilty of a specific HEMS result being allocated to
    #         a call based on the care category amd whether a helicopter is beneficial

    #     """

    #     # Wrangle the data...trying numpy for a change

    #     hems_df = (
    #         self.df
    #         .assign(
    #             helicopter_benefit=np.select(
    #                 [
    #                     self.df["cc_benefit"] == "y",
    #                     self.df["ec_benefit"] == "y",
    #                     self.df["hems_result"].isin([
    #                         "Stand Down En Route",
    #                         "Landed but no patient contact",
    #                         "Stand Down Before Mobile"
    #                     ])
    #                 ],
    #                 ["y", "y", "n"],
    #                 default=self.df["helicopter_benefit"]
    #             ),
    #             care_cat=np.select(
    #                 [
    #                     self.df["cc_benefit"] == "y",
    #                     self.df["ec_benefit"] == "y"
    #                 ],
    #                 ["CC", "EC"],
    #                 default="REG"
    #             )
    #         )
    #     )

    #     hems_counts = hems_df.groupby(['hems_result', 'care_cat', 'helicopter_benefit']).size().reset_index(name='count')

    #     hems_counts['total'] = hems_counts.groupby(['care_cat', 'helicopter_benefit'])['count'].transform('sum')
    #     hems_counts['proportion'] = round(hems_counts['count'] / hems_counts['total'], 4)

    #     hems_counts.to_csv('distribution_data/hems_result_by_care_cat_and_helicopter_benefit_probs.csv', mode = "w+", index=False)
    # ========== END ARCHIVED CODE ============ #

    # ========== ARCHIVED CODE ============ #
    # def pt_outcome_by_hems_result_and_care_category_probs(self):
    #     """

    #         Calculates the probabilty of a specific patient outcome based on HEMS result

    #     """

    #     hems_df = (
    #         self.df
    #         .assign(
    #             helicopter_benefit=np.select(
    #                 [
    #                     self.df["cc_benefit"] == "y",
    #                     self.df["ec_benefit"] == "y",
    #                     self.df["hems_result"].isin([
    #                         "Stand Down En Route",
    #                         "Landed but no patient contact",
    #                         "Stand Down Before Mobile"
    #                     ])
    #                 ],
    #                 ["y", "y", "n"],
    #                 default=self.df["helicopter_benefit"]
    #             ),
    #             care_category=np.select(
    #                 [
    #                     self.df["cc_benefit"] == "y",
    #                     self.df["ec_benefit"] == "y"
    #                 ],
    #                 ["CC", "EC"],
    #                 default="REG"
    #             )
    #         )
    #     )

    #     po_counts = hems_df.groupby(['pt_outcome', 'hems_result', 'care_category']).size().reset_index(name='count')

    #     po_counts['total'] = po_counts.groupby(['hems_result', 'care_category'])['count'].transform('sum')
    #     po_counts['proportion'] = round(po_counts['count'] / po_counts['total'], 4)

    #     po_counts.to_csv('distribution_data/pt_outcome_by_hems_result_and_care_category_probs.csv', mode = "w+")
    # ========== END ARCHIVED CODE ============ #

    def school_holidays(self) -> None:
        """ "
        Function to generate a CSV file containing schoole holiday
        start and end dates for a given year. The Year range is determined
        by the submitted data (plus a year at the end of the study for good measure)
        """

        min_date = self.df.inc_date.min()
        max_date = self.df.inc_date.max() + timedelta(
            weeks=(52 * self.school_holidays_years)
        )

        u = Utils()

        years_of_holidays_list = u.years_between(min_date, max_date)

        sh = pd.DataFrame(columns=["year", "start_date", "end_date"])

        for i, year in enumerate(years_of_holidays_list):
            tmp = u.calculate_term_holidays(year)

            if i == 0:
                sh = tmp
            else:
                sh = pd.concat([sh, tmp])

        sh.to_csv("actual_data/school_holidays.csv", index=False)

    # These functions are to wrangle historical data to provide comparison against the simulation outputs

    def historical_jobs_per_day_per_callsign(self):
        df = self.df

        df["date"] = pd.to_datetime(df["inc_date"]).dt.date
        all_counts_hist = (
            df.groupby(["date", "callsign"])["job_id"].count().reset_index()
        )
        all_counts_hist.rename(columns={"job_id": "jobs_in_day"}, inplace=True)

        all_combinations = pd.DataFrame(
            list(itertools.product(df["date"].unique(), df["callsign"].unique())),
            columns=["date", "callsign"],
        ).dropna()

        merged = all_combinations.merge(
            all_counts_hist, on=["date", "callsign"], how="left"
        )
        merged["jobs_in_day"] = merged["jobs_in_day"].fillna(0).astype(int)

        all_counts = (
            merged.groupby(["callsign", "jobs_in_day"])
            .count()
            .reset_index()
            .rename(columns={"date": "count"})
        )
        all_counts.to_csv(
            "historical_data/historical_jobs_per_day_per_callsign.csv", index=False
        )

    def historical_care_cat_counts(self):
        """
        Process historical incident data to categorize care types and compute hourly counts.

        This method performs the following steps:
        - Converts incident dates to datetime format.
        - Extracts month start and hour from the incident date.
        - Categorizes each incident into care categories based on benefit flags and attendance.
        - Counts the number of incidents by hour and care category.
        - Outputs these counts to a CSV file.
        - Computes and writes the proportion of regular care jobs with a helicopter benefit
        (excluding those not attended by a DAA resource) to a text file.

        Outputs:
        - CSV file: 'historical_data/historical_care_cat_counts.csv'
        - Text file: 'distribution_data/proportion_jobs_heli_benefit.txt'
        """

        df_historical = self.df

        df_historical["inc_date"] = pd.to_datetime(df_historical["inc_date"])
        # Extract the first day of the month and the hour of each incident
        df_historical["month_start"] = df_historical.inc_date.dt.strftime("%Y-%m-01")
        df_historical["hour"] = df_historical.inc_date.dt.hour

        conditions = [
            df_historical["cc_benefit"] == "y",
            df_historical["ec_benefit"] == "y",
            df_historical["helicopter_benefit"] == "y",
            df_historical["callsign_group"] == "Other",
        ]

        choices = [
            "CC",
            "EC",
            "REG - helicopter benefit",
            "Unknown - DAA resource did not attend",
        ]
        # Assign care category to each record
        # If the case did not meet any of the criteria in 'conditions', it will default
        # to being labelled as a 'regular/REG' case (i.e there was no benefit recorded)
        df_historical["care_category"] = np.select(conditions, choices, default="REG")

        # Count occurrences grouped by hour and care category
        historical_value_counts_by_hour = df_historical.value_counts(
            ["hour", "care_category"]
        ).reset_index(name="count")
        # Output to CSV for use in tests and visualisations
        (
            historical_value_counts_by_hour.sort_values(
                ["hour", "care_category"]
            ).to_csv("historical_data/historical_care_cat_counts.csv")
        )

        # Also output the % of regular (not cc/ec) jobs with a helicopter benefit
        # These are the regular jobs we will make an assumption follow different logic due to having an obvious expected
        # patient benefit of having a helicopter allocated to them that we will have to assume is apparent at the time
        # of the call being placed (such as the casualty being located in a remote location, or )

        numerator = historical_value_counts_by_hour[
            historical_value_counts_by_hour["care_category"]
            == "REG - helicopter benefit"
        ]["count"].sum()

        denominator = historical_value_counts_by_hour[
            (
                historical_value_counts_by_hour["care_category"]
                == "REG - helicopter benefit"
            )
            | (historical_value_counts_by_hour["care_category"] == "REG")
        ]["count"].sum()

        with open(
            "distribution_data/proportion_jobs_heli_benefit.txt", "w+"
        ) as heli_benefit_file:
            heli_benefit_file.write(json.dumps((numerator / denominator).round(4)))

        # Count occurrences grouped by hour and care category
        historical_value_counts_by_hour_cc_ec = df_historical.value_counts(
            ["hour", "care_category", "helicopter_benefit"]
        ).reset_index(name="count")

        # Output to CSV for use in tests and visualisations
        # (historical_value_counts_by_hour_cc_ec
        #  .sort_values(["hour", "care_category", "helicopter_benefit"])
        #  .to_csv("historical_data/historical_care_cat_counts_cc_ec.csv"))

        numerator_cc = historical_value_counts_by_hour_cc_ec[
            (historical_value_counts_by_hour_cc_ec["care_category"] == "CC")
            & (historical_value_counts_by_hour_cc_ec["helicopter_benefit"] == "y")
        ]["count"].sum()

        denominator_cc = historical_value_counts_by_hour_cc_ec[
            (historical_value_counts_by_hour_cc_ec["care_category"] == "CC")
        ]["count"].sum()

        with open(
            "distribution_data/proportion_jobs_heli_benefit_cc.txt", "w+"
        ) as heli_benefit_file:
            heli_benefit_file.write(
                json.dumps((numerator_cc / denominator_cc).round(4))
            )

        numerator_ec = historical_value_counts_by_hour_cc_ec[
            (historical_value_counts_by_hour_cc_ec["care_category"] == "EC")
            & (historical_value_counts_by_hour_cc_ec["helicopter_benefit"] == "y")
        ]["count"].sum()

        denominator_ec = historical_value_counts_by_hour_cc_ec[
            (historical_value_counts_by_hour_cc_ec["care_category"] == "EC")
        ]["count"].sum()

        with open(
            "distribution_data/proportion_jobs_heli_benefit_ec.txt", "w+"
        ) as heli_benefit_file:
            heli_benefit_file.write(
                json.dumps((numerator_ec / denominator_ec).round(4))
            )

    def historical_monthly_totals(self):
        """
        Calculates monthly incident totals from provided dataset of historical data
        """

        # Multiple resources can be sent to the same job.
        monthly_df = self.df[
            ["inc_date", "first_day_of_month", "hems_result", "vehicle_type"]
        ].drop_duplicates(subset="inc_date", keep="first")

        is_stand_down = monthly_df["hems_result"].str.contains("Stand Down")
        monthly_df["stand_down_car"] = (
            (monthly_df["vehicle_type"] == "car") & is_stand_down
        ).astype(int)
        monthly_df["stand_down_helicopter"] = (
            (monthly_df["vehicle_type"] == "helicopter") & is_stand_down
        ).astype(int)

        monthly_totals_df = (
            monthly_df.groupby("first_day_of_month")
            .agg(
                stand_down_car=("stand_down_car", "sum"),
                stand_down_helicopter=("stand_down_helicopter", "sum"),
                total_jobs=("vehicle_type", "size"),
            )
            .reset_index()
        )

        monthly_totals_df.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_jobs_per_month.csv", mode="w+", index=False
        )

    def historical_monthly_totals_all_calls(self):
        """
        Calculates monthly incident totals from provided dataset of historical data stratified by callsign
        """

        # Multiple resources can be sent to the same job.
        monthly_df = self.df[["inc_date", "first_day_of_month"]].dropna()

        monthly_totals_df = (
            monthly_df.groupby(["first_day_of_month"]).count().reset_index()
        )

        monthly_totals_df.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_monthly_totals_all_calls.csv",
            mode="w+",
            index=False,
        )

    def historical_monthly_totals_by_callsign(self):
        """
        Calculates monthly incident totals from provided dataset of historical data stratified by callsign
        """

        # Multiple resources can be sent to the same job.
        monthly_df = self.df[["inc_date", "first_day_of_month", "callsign"]].dropna()

        monthly_totals_df = (
            monthly_df.groupby(["first_day_of_month", "callsign"]).count().reset_index()
        )

        # print(monthly_totals_df.head())

        monthly_totals_pivot_df = (
            monthly_totals_df.pivot(
                index="first_day_of_month", columns="callsign", values="inc_date"
            )
            .fillna(0)
            .reset_index()
            .rename_axis(None, axis=1)
        )

        # print(monthly_totals_pivot_df.head())

        monthly_totals_pivot_df.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_monthly_totals_by_callsign.csv",
            mode="w+",
            index=False,
        )

    def historical_monthly_totals_by_hour_of_day(self):
        """
        Calculates monthly incident totals from provided dataset of historical data stratified by hour of the day
        """

        # Multiple resources can be sent to the same job.
        monthly_df = (
            self.df[["inc_date", "first_day_of_month", "hour"]]
            .dropna()
            .drop_duplicates(subset="inc_date", keep="first")
        )

        monthly_totals_df = (
            monthly_df.groupby(["first_day_of_month", "hour"]).count().reset_index()
        )

        # print(monthly_totals_df.head())

        monthly_totals_pivot_df = (
            monthly_totals_df.pivot(
                index="first_day_of_month", columns="hour", values="inc_date"
            )
            .fillna(0)
            .reset_index()
            .rename_axis(None, axis=1)
        )

        # print(monthly_totals_pivot_df.head())

        monthly_totals_pivot_df.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_monthly_totals_by_hour_of_day.csv",
            mode="w+",
            index=False,
        )

    def historical_monthly_totals_by_day_of_week(self):
        """
        Calculates number of incidents per month stratified by day of the week
        """

        # Multiple resources can be sent to the same job.
        monthly_df = (
            self.df[["inc_date", "first_day_of_month", "day_of_week"]]
            .dropna()
            .drop_duplicates(subset="inc_date", keep="first")
        )

        monthly_totals_df = (
            monthly_df.groupby(["first_day_of_month", "day_of_week"])
            .count()
            .reset_index()
        )

        # print(monthly_totals_df.head())

        monthly_totals_pivot_df = (
            monthly_totals_df.pivot(
                index="first_day_of_month", columns="day_of_week", values="inc_date"
            )
            .fillna(0)
            .reset_index()
            .rename_axis(None, axis=1)
        )

        # print(monthly_totals_pivot_df.head())

        monthly_totals_pivot_df.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_monthly_totals_by_day_of_week.csv",
            mode="w+",
            index=False,
        )

    def historical_median_time_of_activities_by_month_and_resource_type(self):
        """
        Calculate the median time for each of the job cycle phases stratified by month and vehicle type
        """

        median_df = self.df[
            [
                "first_day_of_month",
                "time_allocation",
                "time_mobile",
                "time_to_scene",
                "time_on_scene",
                "time_to_hospital",
                "time_to_clear",
                "vehicle_type",
            ]
        ].copy()

        median_df["total_job_time"] = median_df[
            [
                "time_allocation",
                "time_mobile",
                "time_to_scene",
                "time_on_scene",
                "time_to_hospital",
                "time_to_clear",
            ]
        ].sum(axis=1, skipna=True)

        # Replacing zeros with NaN to exclude from median calculation
        # since if an HEMS result is Stood down en route, then time_on_scene would be zero and affect the median
        # median_df.replace(0, np.nan, inplace=True)

        # Grouping by month and resource_type, calculating medians
        median_times = (
            median_df.groupby(["first_day_of_month", "vehicle_type"])
            .median(numeric_only=True)
            .reset_index()
        )

        pivot_data = median_times.pivot_table(
            index="first_day_of_month",
            columns="vehicle_type",
            values=[
                "time_allocation",
                "time_mobile",
                "time_to_scene",
                "time_on_scene",
                "time_to_hospital",
                "time_to_clear",
                "total_job_time",
            ],
        )

        pivot_data.columns = [f"median_{col[1]}_{col[0]}" for col in pivot_data.columns]
        pivot_data = pivot_data.reset_index()

        pivot_data.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_median_time_of_activities_by_month_and_resource_type.csv",
            mode="w+",
            index=False,
        )

    def historical_monthly_resource_utilisation(self):
        """
        Calculates number of, and time spent on, incidents per month stratified by callsign
        """

        # Multiple resources can be sent to the same job.
        monthly_df = self.df[
            [
                "inc_date",
                "first_day_of_month",
                "callsign",
                "time_allocation",
                "time_mobile",
                "time_to_scene",
                "time_on_scene",
                "time_to_hospital",
                "time_to_clear",
            ]
        ].copy()

        monthly_df["total_time"] = monthly_df.filter(regex=r"^time_").sum(axis=1)

        monthly_totals_df = monthly_df.groupby(
            ["callsign", "first_day_of_month"], as_index=False
        ).agg(n=("callsign", "size"), total_time=("total_time", "sum"))

        monthly_totals_pivot_df = monthly_totals_df.pivot(
            index="first_day_of_month", columns="callsign", values=["n", "total_time"]
        )

        monthly_totals_pivot_df.columns = [
            f"{col[0]}_{col[1]}" for col in monthly_totals_pivot_df.columns
        ]
        monthly_totals_pivot_df = monthly_totals_pivot_df.reset_index()

        monthly_totals_pivot_df.rename(columns={"first_day_of_month": "month"}).to_csv(
            "historical_data/historical_monthly_resource_utilisation.csv",
            mode="w+",
            index=False,
        )

    def historical_daily_calls_breakdown(self):
        df = self.df
        # Convert inc_date to date only (remove time)
        df["date"] = pd.to_datetime(df["inc_date"]).dt.date

        # Count number of calls per day
        calls_in_day_breakdown = (
            df.groupby("date").size().reset_index(name="calls_in_day")
        )

        # Save the daily call counts with a 'day' index column
        calls_in_day_breakdown_with_day = calls_in_day_breakdown.copy()
        calls_in_day_breakdown_with_day.insert(
            0, "day", range(1, len(calls_in_day_breakdown) + 1)
        )
        calls_in_day_breakdown_with_day.drop(columns="date").to_csv(
            "historical_data/historical_daily_calls_breakdown.csv", index=False
        )

        # Count how many days had the same number of calls
        calls_per_day_summary = (
            calls_in_day_breakdown["calls_in_day"].value_counts().reset_index()
        )
        calls_per_day_summary.columns = ["calls_in_day", "days"]
        calls_per_day_summary.to_csv(
            "historical_data/historical_daily_calls.csv", index=False
        )

    def historical_missed_jobs(self):
        df = self.df
        df["date"] = pd.to_datetime(df["inc_date"])
        df["hour"] = df["date"].dt.hour
        df["month_start"] = df["date"].dt.strftime("%Y-%m-01")
        df["callsign_group_simplified"] = df["callsign_group"].apply(
            lambda x: "No HEMS available"
            if x == "Other"
            else "HEMS (helo or car) available and sent"
        )
        df["quarter"] = df["inc_date"].dt.quarter

        # By month
        count_df_month = (
            df[["callsign_group_simplified", "month_start"]]
            .value_counts()
            .reset_index(name="count")
            .sort_values(["callsign_group_simplified", "month_start"])
        )
        count_df_month.to_csv(
            "historical_data/historical_missed_calls_by_month.csv", index=False
        )

        # By hour
        count_df = (
            df[["callsign_group_simplified", "hour"]]
            .value_counts()
            .reset_index(name="count")
            .sort_values(["callsign_group_simplified", "hour"])
        )
        count_df.to_csv(
            "historical_data/historical_missed_calls_by_hour.csv", index=False
        )

        # By quarter and hour
        count_df_quarter = (
            df[["callsign_group_simplified", "quarter", "hour"]]
            .value_counts()
            .reset_index(name="count")
            .sort_values(["quarter", "callsign_group_simplified", "hour"])
        )
        count_df_quarter.to_csv(
            "historical_data/historical_missed_calls_by_quarter_and_hour.csv",
            index=False,
        )

    def upper_allowable_time_bounds(self):
        """
        Calculates the maximum permissable time for each phase on an incident based on supplied historical data.
        This is currently set to 1.5x the upper quartile of the data distribution
        """

        median_df = self.df[
            [
                "time_allocation",
                "time_mobile",
                "time_to_scene",
                "time_on_scene",
                "time_to_hospital",
                "time_to_clear",
                "vehicle_type",
            ]
        ]

        # Replacing zeros with NaN to exclude from median calculation
        # since if an HEMS result is Stood down en route, then time_on_scene
        # would be zero and affect the median
        median_df.replace(0, np.nan, inplace=True)

        print(median_df.quantile(0.75))
        # pivot_data.rename(columns={'first_day_of_month': 'month'}).to_csv('historical_data/historical_median_time_of_activities_by_month_and_resource_type.csv', mode="w+", index=False)

    def historical_job_durations_breakdown(self):
        df = self.df

        cols = [
            "callsign",
            "vehicle_type",
            "time_allocation",
            "time_mobile",
            "time_to_scene",
            "time_on_scene",
            "time_to_hospital",
            "time_to_clear",
        ]
        df2 = df[cols].copy()

        # 2. Add a 1-based row identifier
        df2["job_identifier"] = range(1, len(df2) + 1)

        # 3. Compute total_duration as the row-wise sum of the time columns
        time_cols = [
            "time_allocation",
            "time_mobile",
            "time_to_scene",
            "time_on_scene",
            "time_to_hospital",
            "time_to_clear",
        ]
        df2["total_duration"] = df2[time_cols].sum(axis=1, skipna=True)

        # print(df2.head())

        # 4. Pivot (melt) to long format
        df_long = df2.melt(
            id_vars=["job_identifier", "callsign", "vehicle_type"],
            value_vars=time_cols + ["total_duration"],
            var_name="name",
            value_name="value",
        )

        # print(df_long[df_long.job_identifier == 1])

        # 5. Drop any rows where callsign or vehicle_type is missing
        df_long = df_long.dropna(subset=["callsign", "vehicle_type"])
        df_long_sorted = df_long.sort_values("job_identifier").reset_index(drop=True)

        # 6. Write out to CSV
        df_long_sorted.to_csv(
            "historical_data/historical_job_durations_breakdown.csv", index=False
        )

    # ========== ARCHIVED CODE - v1 of calcualte_availability_row ======================== #
    # def calculate_availability_row(self, row, rota_df, callsign_lookup_df, period_start, period_end):
    #     """
    #     Compute downtime overlap, rota-based scheduled time, and proportion for a given row.
    #     Returns data tagged with bin, quarter, and downtime reason.
    #     """

    #     registration = row['aircraft'].lower()
    #     downtime_start = pd.to_datetime(row['offline'], utc=True)
    #     downtime_end = pd.to_datetime(row['online'], utc=True)
    #     reason = row.get('reason', None)

    #     hour = downtime_start.hour
    #     if 0 <= hour <= 5:
    #         six_hour_bin = '00-05'
    #     elif 6 <= hour <= 11:
    #         six_hour_bin = '06-11'
    #     elif 12 <= hour <= 17:
    #         six_hour_bin = '12-17'
    #     else:
    #         six_hour_bin = '18-23'

    #     quarter = downtime_start.quarter

    #     # Match callsign
    #     match = callsign_lookup_df[callsign_lookup_df['registration'].str.lower() == registration]
    #     if match.empty:
    #         return {
    #             'registration': registration,
    #             'offline': downtime_start,
    #             'online': downtime_end,
    #             'six_hour_bin': six_hour_bin,
    #             'quarter': quarter,
    #             'total_offline': None,
    #             'scheduled_minutes': None,
    #             'reason': reason,
    #             'proportion': None
    #         }
    #     callsign = match.iloc[0]['callsign']

    #     rota_rows = rota_df[rota_df['callsign'] == callsign]
    #     if rota_rows.empty:
    #         return {
    #             'registration': registration,
    #             'offline': downtime_start,
    #             'online': downtime_end,
    #             'six_hour_bin': six_hour_bin,
    #             'quarter': quarter,
    #             'total_offline': None,
    #             'scheduled_minutes': None,
    #             'reason': reason,
    #             'proportion': None
    #         }

    #     # Clip evaluation window to downtime period
    #     eval_start = max(downtime_start.normalize(), pd.to_datetime(period_start, utc=True))
    #     eval_end = min(downtime_end.normalize(), pd.to_datetime(period_end, utc=True))

    #     total_scheduled_minutes = 0
    #     total_overlap_minutes = 0

    #     current_day = eval_start
    #     while current_day <= eval_end:
    #         month = current_day.month
    #         season = 'summer' if month in [4, 5, 6, 7, 8, 9] else 'winter'

    #         for _, rota in rota_rows.iterrows():
    #             start_hour = rota[f'{season}_start']
    #             end_hour = rota[f'{season}_end']

    #             rota_start = current_day + timedelta(hours=start_hour)
    #             rota_end = current_day + timedelta(hours=end_hour)
    #             if end_hour <= start_hour:
    #                 rota_end += timedelta(days=1)

    #             # Count scheduled time regardless of overlap
    #             scheduled_minutes = (rota_end - rota_start).total_seconds() / 60
    #             total_scheduled_minutes += scheduled_minutes

    #             # Count overlap only if intersecting with downtime
    #             overlap_start = max(downtime_start, rota_start)
    #             overlap_end = min(downtime_end, rota_end)
    #             if overlap_end > overlap_start:
    #                 overlap_minutes = (overlap_end - overlap_start).total_seconds() / 60
    #                 total_overlap_minutes += overlap_minutes

    #         current_day += timedelta(days=1)

    #     if total_scheduled_minutes == 0:
    #         proportion = None
    #     else:
    #         proportion = total_overlap_minutes / total_scheduled_minutes

    #     return {
    #         'registration': registration,
    #         'offline': downtime_start,
    #         'online': downtime_end,
    #         'six_hour_bin': six_hour_bin,
    #         'quarter': quarter,
    #         'total_offline': total_overlap_minutes,
    #         'scheduled_minutes': total_scheduled_minutes,
    #         'reason': reason,
    #         'proportion': proportion
    #     }

    # ============= ARCHIVED CODE - v2 of calculate_availability_row ======================= #
    # def calculate_availability_row(self, row, rota_df, callsign_lookup_df):
    #     """
    #         Compute downtime overlap, rota-based scheduled time, and proportion for a given row.
    #         Returns data tagged with bin, quarter, and downtime reason.
    #     """

    #     registration = row['aircraft'].lower()
    #     downtime_start = pd.to_datetime(row['offline'], utc=True)
    #     downtime_end = pd.to_datetime(row['online'], utc=True)
    #     reason = row.get('reason', None)

    #     hour = downtime_start.hour
    #     if 0 <= hour <= 5:
    #         six_hour_bin = '00-05'
    #     elif 6 <= hour <= 11:
    #         six_hour_bin = '06-11'
    #     elif 12 <= hour <= 17:
    #         six_hour_bin = '12-17'
    #     else:
    #         six_hour_bin = '18-23'

    #     quarter = downtime_start.quarter

    #     # Match callsign
    #     match = callsign_lookup_df[callsign_lookup_df['registration'].str.lower() == registration]
    #     if match.empty:
    #         return {
    #             'registration': registration,
    #             'offline': downtime_start,
    #             'online': downtime_end,
    #             'six_hour_bin': six_hour_bin,
    #             'quarter': quarter,
    #             'total_offline': None,
    #             'scheduled_minutes': None,
    #             'reason': reason,
    #             'proportion': None
    #         }
    #     callsign = match.iloc[0]['callsign']

    #     rota_rows = rota_df[rota_df['callsign'] == callsign]
    #     if rota_rows.empty:
    #         return {
    #             'registration': registration,
    #             'offline': downtime_start,
    #             'online': downtime_end,
    #             'six_hour_bin': six_hour_bin,
    #             'quarter': quarter,
    #             'total_offline': None,
    #             'scheduled_minutes': None,
    #             'reason': reason,
    #             'proportion': None
    #         }

    #     month = downtime_start.month
    #     season = 'summer' if month in [4, 5, 6, 7, 8, 9] else 'winter'

    #     total_scheduled_minutes = 0
    #     total_overlap_minutes = 0

    #     for _, rota in rota_rows.iterrows():
    #         start_hour = rota[f'{season}_start']
    #         end_hour = rota[f'{season}_end']

    #         for base_day in [downtime_start.normalize() - timedelta(days=1),
    #                         downtime_start.normalize(),
    #                         downtime_start.normalize() + timedelta(days=1)]:

    #             rota_start = base_day + timedelta(hours=start_hour)
    #             rota_end = base_day + timedelta(hours=end_hour)
    #             if end_hour <= start_hour:
    #                 rota_end += timedelta(days=1)

    #             overlap_start = max(downtime_start, rota_start)
    #             overlap_end = min(downtime_end, rota_end)

    #             if overlap_end > overlap_start:
    #                 scheduled_minutes = (rota_end - rota_start).total_seconds() / 60
    #                 overlap_minutes = (overlap_end - overlap_start).total_seconds() / 60

    #                 total_scheduled_minutes += scheduled_minutes
    #                 total_overlap_minutes += overlap_minutes

    #     if total_scheduled_minutes == 0:
    #         proportion = None
    #     else:
    #         proportion = total_overlap_minutes / total_scheduled_minutes

    #     return {
    #         'registration': registration,
    #         'offline': downtime_start,
    #         'online': downtime_end,
    #         'six_hour_bin': six_hour_bin,
    #         'quarter': quarter,
    #         'total_offline': total_overlap_minutes,
    #         'scheduled_minutes': total_scheduled_minutes,
    #         'reason': reason,
    #         'proportion': proportion
    #     }

    # ================ ARCHIVED CODE - ad-hoc unavailability calculation ================ #
    # def ad_hoc_unavailability(self, period_start, period_end, include_debugging_cols=False):
    #     """Process ad hoc unavailability records into a stratified probability table.

    #     Calculates the probability of ad hoc unavailability and availability based
    #     on historical data. The data is stratified by aircraft registration,
    #     six-hour time bins, and calendar quarters. It ensures all standard
    #     reasons ('available', 'crew', 'weather', 'aircraft') are present for
    #     each combination. It filters out any registration/quarter/bin pairings
    #     with little scheduled time, marking their probabilities as blank. It also adds
    #     a count of the ad-hoc unavailability events considered for each probability calculation
    #     and the total scheduled time for the resource in that quarter/time period that was part of
    #     the calculations.

    #     Returns
    #     -------
    #     None
    #         This function does not return a value but saves the calculated
    #         probabilities to 'distribution_data/ad_hoc_unavailability.csv'.
    #         The CSV file includes columns for registration, six_hour_bin,
    #         quarter, reason, probability, and count.

    #     Notes
    #     -----
    #     This function relies on the following input files:
    #     - 'external_data/ad_hoc.csv': Contains ad hoc unavailability records.
    #     - 'actual_data/HEMS_ROTA.csv': Contains rota information.
    #     - 'actual_data/callsign_registration_lookup.csv': Maps callsigns to registrations.
    #     It also depends on the 'calculate_availability_row' method within the
    #     same class. Ensure that the 'distribution_data' directory exists.
    #     """
    #     try:
    #         # Load data
    #         adhoc_df = pd.read_csv('external_data/ad_hoc.csv', parse_dates=['offline', 'online'])
    #         adhoc_df = adhoc_df[['aircraft', 'offline', 'online', 'reason']]
    #         rota_df = pd.read_csv("actual_data/HEMS_ROTA.csv")
    #         callsign_lookup_df = pd.read_csv("actual_data/callsign_registration_lookup.csv")

    #         # Process each ad hoc record
    #         results = adhoc_df.apply(
    #             lambda row: self.calculate_availability_row(row, rota_df, callsign_lookup_df, period_start, period_end),
    #             axis=1
    #         )
    #         final_df = pd.DataFrame(results.tolist())
    #         final_df.to_csv("external_data/ad_hoc_intermediate.csv")

    #         # Check if final_df is empty before proceeding
    #         if final_df.empty:
    #             print("No ad-hoc data processed, skipping file generation.")
    #             return

    #         # Define the full set of reasons expected
    #         all_reasons = ['available', 'crew', 'weather', 'aircraft']

    #         # --- Aggregate Data ---
    #         # Calculate job count per registration, quarter, AND bin
    #         unavailability_instance_counts = final_df.groupby(['registration', 'quarter', 'six_hour_bin']).size().reset_index(name='unavailability_instance_counts')

    #         # Downtime by bin + quarter + reason (only for unavailability reasons)
    #         grouped = final_df[final_df['reason'].isin(['crew', 'weather', 'aircraft'])]
    #         grouped = grouped.groupby(['registration', 'six_hour_bin', 'quarter', 'reason'])['total_offline'].sum().reset_index()

    #         # Scheduled time by bin + quarter
    #         scheduled_totals = final_df.groupby(['registration', 'six_hour_bin', 'quarter'])['scheduled_minutes'].sum().reset_index()
    #         scheduled_totals = scheduled_totals.rename(columns={'scheduled_minutes': 'total_scheduled'})

    #         # Merge job counts into scheduled totals
    #         scheduled_totals = pd.merge(scheduled_totals, unavailability_instance_counts, on=['registration', 'quarter', 'six_hour_bin'], how='left')

    #         # Calculate total downtime per bin + quarter (for 'available' calculation)
    #         downtime_totals = grouped.groupby(['registration','six_hour_bin', 'quarter'])['total_offline'].sum().reset_index()
    #         downtime_totals = downtime_totals.rename(columns={'total_offline': 'total_downtime'})

    #         # --- Create Full Grid ---
    #         # Get all unique combinations of registration, quarter, and bin
    #         unique_bins = scheduled_totals[['registration', 'quarter', 'six_hour_bin']].drop_duplicates()

    #         # Check for empty unique_bins
    #         if unique_bins.empty:
    #             print("No valid unique bins found, skipping file generation.")
    #             return

    #         # Create the full grid by crossing unique bins with all reasons
    #         full_grid = unique_bins.assign(key=1).merge(pd.DataFrame({'reason': all_reasons, 'key': 1}), on='key').drop('key', axis=1)

    #         # --- Merge Data into Full Grid ---
    #         full_grid = pd.merge(full_grid, scheduled_totals, on=['registration', 'quarter', 'six_hour_bin'], how='left')
    #         full_grid = pd.merge(full_grid, grouped, on=['registration', 'six_hour_bin', 'quarter', 'reason'], how='left')
    #         full_grid = pd.merge(full_grid, downtime_totals, on=['registration', 'six_hour_bin', 'quarter'], how='left')

    #         # Fill NaNs created during merges
    #         full_grid['total_offline'] = full_grid['total_offline'].fillna(0)
    #         full_grid['total_downtime'] = full_grid['total_downtime'].fillna(0)
    #         full_grid['unavailability_instance_counts'] = full_grid['unavailability_instance_counts'].fillna(0) # Fill job count with 0 for bins that might exist but have no jobs

    #         # --- Calculate Probabilities ---
    #         # Suppress division by zero warnings - we handle these next
    #         with np.errstate(divide='ignore', invalid='ignore'):
    #             prob_avail = (full_grid['total_scheduled'] - full_grid['total_downtime']) / full_grid['total_scheduled']
    #             prob_unavail = full_grid['total_offline'] / full_grid['total_scheduled']

    #             full_grid['probability'] = np.where(
    #                 full_grid['reason'] == 'available',
    #                 prob_avail,
    #                 prob_unavail
    #             )

    #         # Handle NaN/Inf from division by zero, set them to 0.0 for now.
    #         full_grid['probability'] = full_grid['probability'].replace([np.inf, -np.inf], np.nan).fillna(0.0)

    #         # --- Apply Threshold and Blanking ---
    #         # Condition for setting probability to blank
    #         condition_for_blank = (full_grid['total_scheduled'] < 60 * 2 * 30 * 3 ) # If less than 2 hours per day in time period rotad, exclude

    #         # Convert probability to object to allow blanks, then apply the condition
    #         full_grid['probability'] = full_grid['probability'].astype(object)
    #         full_grid.loc[condition_for_blank, 'probability'] = ''

    #         # --- Finalize and Save ---
    #         # Select and rename columns
    #         if include_debugging_cols:
    #             final_prob_df = full_grid[['registration', 'six_hour_bin', 'quarter', 'reason', 'probability', 'unavailability_instance_counts', 'total_offline', 'total_scheduled']]
    #             final_prob_df.rename(columns={"total_offine":"total_minutes_offline_for_reason", "total_scheduled": "total_minutes_rotad_availability_in_quarter_and_time_period"})
    #         else:
    #             final_prob_df = full_grid[['registration', 'six_hour_bin', 'quarter', 'reason', 'probability']]
    #         # final_prob_df = final_prob_df.rename(columns={'job_count': 'count'})

    #         # Sort and save
    #         final_prob_df = final_prob_df.sort_values(by=['registration', 'quarter', 'six_hour_bin', 'reason']).reset_index(drop=True)
    #         final_prob_df.to_csv("distribution_data/ad_hoc_unavailability.csv", index=False)

    #         print("Ad-hoc unavailability probability table generated successfully.")

    #     except FileNotFoundError:
    #         print("Couldn't generate ad-hoc unavailability due to missing file(s). "
    #             "Please ensure 'external_data/ad_hoc.csv', "
    #             "'actual_data/HEMS_ROTA.csv', and "
    #             "'actual_data/callsign_registration_lookup.csv' exist.")
    #         pass
    #     except Exception as e:
    #         print(f"An error occurred during ad-hoc unavailability processing: {e}")
    #         pass

    def ad_hoc_unavailability(self, period_start, period_end):
        """
        Calculate aircraft availability and unavailability probabilities based on scheduled rotas and ad-hoc downtime.

        Args:
            period_start (str/datetime): Start date for analysis period
            period_end (str/datetime): End date for analysis period
            include_debugging_cols (bool): Whether to include debugging columns in output (currently unused)

        Returns:
            None (saves results to CSV file)
        """
        # Load and prepare ad-hoc downtime data
        adhoc_df = pd.read_csv(
            "external_data/ad_hoc.csv", parse_dates=["offline", "online"]
        )
        adhoc_df = adhoc_df[["aircraft", "offline", "online", "reason"]]
        # Load rota and callsign lookup data
        rota_df = pd.read_csv("actual_data/HEMS_ROTA.csv")
        callsign_lookup_df = pd.read_csv("actual_data/callsign_registration_lookup.csv")
        # Merge rota with callsign lookup to get registration numbers to allow matching with
        # ad-hoc data, which uses registrations
        full_rota_df = rota_df.merge(callsign_lookup_df, on="callsign")

        # Define the hour bands mapping
        HOUR_BANDS = {
            "00-05": (0, 6),  # 00:00 to 05:59
            "06-11": (6, 12),  # 06:00 to 11:59
            "12-17": (12, 18),  # 12:00 to 17:59
            "18-23": (18, 24),  # 18:00 to 23:59
        }

        # Create list of 6-hour bins
        bins = ["00-05", "06-11", "12-17", "18-23"]

        def is_summer(date_obj, summer_start_month=4, summer_end_month=9):
            """
            Determine if a date falls in summer months (April-September).

            Args:
                date_obj: Date object to check

            Returns:
                bool: True if date is in summer months
            """
            return date_obj.month in [
                i for i in range(summer_start_month, summer_end_month + 1)
            ]

        def check_month_is_summer(month, summer_start_month=4, summer_end_month=9):
            """
            Determine if a date falls in summer months (April-September).

            Args:
                month: Integer month

            Returns:
                str: 'summer' is in summer months, 'winter' if in winter months
            """
            return (
                "summer"
                if month in [i for i in range(summer_start_month, summer_end_month + 1)]
                else "winter"
            )

        def get_band_for_hour(hour):
            """
            Return the 6-hour band name for a given hour (0-23).

            Args:
                hour (int): Hour of day (0-23)

            Returns:
                str or None: Band name or None if hour is invalid
            """
            for band_name, (start, end) in HOUR_BANDS.items():
                if start <= hour < end:
                    return band_name
            return None

        def calculate_minutes_in_band(shift_start, shift_end, band_start, band_end):
            """
            Calculate how many minutes of a shift fall within a specific time band.

            Args:
                shift_start (float): Shift start time in hours (0-23.99)
                shift_end (float): Shift end time in hours (0-23.99)
                band_start (int): Band start hour
                band_end (int): Band end hour

            Returns:
                int: Minutes of overlap between shift and band
            """
            # Find the overlap between shift and band
            overlap_start = max(shift_start, band_start)
            overlap_end = min(shift_end, band_end)

            if overlap_start < overlap_end:
                return int((overlap_end - overlap_start) * 60)  # Convert to minutes
            return 0

        # Create date range for analysis period
        date_range = pd.date_range(start=period_start, end=period_end, freq="D")
        daily_df = pd.DataFrame({"date": date_range})
        daily_df["date"] = pd.to_datetime(daily_df["date"])

        # Create MultiIndex from date and bins combinations to get all date/time combinations
        multi_index = pd.MultiIndex.from_product(
            [daily_df["date"], bins], names=["date", "six_hour_bin"]
        )

        # Convert MultiIndex to DataFrame and reset index
        daily_df = multi_index.to_frame(index=False).reset_index(drop=True)
        # Add quarter information for seasonal analysis
        daily_df["quarter"] = daily_df.date.dt.quarter

        # Initialize availability columns for each aircraft registration
        # Each column will store minutes of scheduled time per time band
        for registration in full_rota_df["registration"].unique():
            daily_df[registration] = 0  # Initialize with 0 minutes

        # Calculate scheduled availability for each date/time band combination
        for _, row in daily_df.iterrows():
            current_date = row["date"]
            current_band = row["six_hour_bin"]
            band_start, band_end = HOUR_BANDS[current_band]

            is_current_date_summer = is_summer(current_date)

            # Get the row index for updating the dataframe
            row_idx = daily_df[
                (daily_df["date"] == current_date)
                & (daily_df["six_hour_bin"] == current_band)
            ].index[0]

            # Iterate through each resource's rota entry
            for _, rota_entry in full_rota_df.iterrows():
                registration = rota_entry["registration"]
                # Select appropriate start/end times based on season
                start_hour_col = (
                    "summer_start" if is_current_date_summer else "winter_start"
                )
                end_hour_col = "summer_end" if is_current_date_summer else "winter_end"
                start_hour = rota_entry[start_hour_col]
                end_hour = rota_entry[end_hour_col]

                total_minutes_for_band = 0

                if start_hour < end_hour:
                    # Shift within same day
                    total_minutes_for_band = calculate_minutes_in_band(
                        start_hour, end_hour, band_start, band_end
                    )
                elif start_hour > end_hour:
                    # Shift spans midnight - check both parts

                    # Part 1: Today from start_hour to midnight
                    if band_end <= 24:  # This band is today
                        total_minutes_for_band += calculate_minutes_in_band(
                            start_hour, 24, band_start, band_end
                        )

                    # Part 2: Tomorrow from midnight to end_hour
                    # Need to check if this band is for tomorrow
                    tomorrow = current_date + pd.Timedelta(days=1)
                    tomorrow_rows = daily_df[daily_df["date"] == tomorrow]

                    if (
                        not tomorrow_rows.empty
                        and current_band in tomorrow_rows["six_hour_bin"].values
                    ):
                        total_minutes_for_band += calculate_minutes_in_band(
                            0, end_hour, band_start, band_end
                        )

                # Update the scheduled time for this aircraft in this time band
                daily_df.loc[row_idx, registration] += total_minutes_for_band

        # Aggregate scheduled availability by quarter, time band, and registration
        available_time = (
            daily_df.melt(
                id_vars=["date", "six_hour_bin", "quarter"],
                value_name="rota_time",
                var_name="registration",
            )
            .groupby(["quarter", "six_hour_bin", "registration"])[["rota_time"]]
            .sum()
            .reset_index()
        )

        def calculate_availability_row(row, rota_df, callsign_lookup_df):
            """
            Calculate downtime overlap with scheduled rota for a single downtime event.

            Args:
                row: DataFrame row containing downtime information
                rota_df: DataFrame with rota schedules
                callsign_lookup_df: DataFrame mapping callsigns to registrations

            Returns:
                dict: Dictionary containing processed availability data
            """
            # Extract downtime information
            registration = row["aircraft"].lower()
            downtime_start = pd.to_datetime(row["offline"], utc=True)
            downtime_end = pd.to_datetime(row["online"], utc=True)
            reason = row.get("reason", None)

            # Determine which 6-hour bin this downtime starts in
            hour = downtime_start.hour
            if 0 <= hour <= 5:
                six_hour_bin = "00-05"
            elif 6 <= hour <= 11:
                six_hour_bin = "06-11"
            elif 12 <= hour <= 17:
                six_hour_bin = "12-17"
            else:
                six_hour_bin = "18-23"

            quarter = downtime_start.quarter

            # Find the callsign for this registration
            match = callsign_lookup_df[
                callsign_lookup_df["registration"].str.lower() == registration
            ]

            # No matching callsign found
            if match.empty:
                return {
                    "registration": registration,
                    "offline": downtime_start,
                    "online": downtime_end,
                    "six_hour_bin": six_hour_bin,
                    "quarter": quarter,
                    "total_offline": None,
                    "reason": reason,
                }

            callsign = match.iloc[0]["callsign"]
            # Find rota entries for this callsign
            rota_rows = rota_df[rota_df["callsign"] == callsign]
            if rota_rows.empty:
                return {
                    "registration": registration,
                    "offline": downtime_start,
                    "online": downtime_end,
                    "six_hour_bin": six_hour_bin,
                    "quarter": quarter,
                    "total_offline": None,
                    "reason": reason,
                }

            # Determine season for appropriate rota times
            month = downtime_start.month
            season = check_month_is_summer(month)

            total_overlap_minutes = 0

            # Calculate overlap between downtime and scheduled rota times
            for _, rota in rota_rows.iterrows():
                start_hour = rota[f"{season}_start"]
                end_hour = rota[f"{season}_end"]

                # Check overlap across multiple days (yesterday, today, tomorrow)
                # This handles shifts that span midnight
                for base_day in [
                    downtime_start.normalize() - timedelta(days=1),
                    downtime_start.normalize(),
                    downtime_start.normalize() + timedelta(days=1),
                ]:
                    rota_start = base_day + timedelta(hours=start_hour)
                    rota_end = base_day + timedelta(hours=end_hour)

                    # Handle shifts that cross midnight
                    if end_hour <= start_hour:
                        rota_end += timedelta(days=1)
                    # Calculate overlap between downtime and this rota period
                    overlap_start = max(downtime_start, rota_start)
                    overlap_end = min(downtime_end, rota_end)

                    if overlap_end > overlap_start:
                        overlap_minutes = (
                            overlap_end - overlap_start
                        ).total_seconds() / 60

                        total_overlap_minutes += overlap_minutes

            return {
                "registration": registration,
                "offline": downtime_start,
                "online": downtime_end,
                "six_hour_bin": six_hour_bin,
                "quarter": quarter,
                "total_offline": total_overlap_minutes,
                "reason": reason,
            }

        # Process all ad-hoc downtime events
        results = adhoc_df.apply(
            lambda row: calculate_availability_row(row, rota_df, callsign_lookup_df),
            axis=1,
        )

        # Convert results to DataFrame and select relevant columns
        unavailability_minutes_df = pd.DataFrame(results.tolist())[
            ["registration", "six_hour_bin", "quarter", "total_offline", "reason"]
        ]

        # Aggregate offline time by registration, time band, quarter, and reason
        offline_durations = (
            unavailability_minutes_df.groupby(
                ["registration", "six_hour_bin", "quarter", "reason"]
            )[["total_offline"]]
            .sum()
            .reset_index()
        )

        # Create complete combinations to ensure all possible categories are represented
        # This prevents missing data issues in the final output
        registrations = offline_durations["registration"].unique()
        six_hour_bins = offline_durations["six_hour_bin"].unique()
        quarters = offline_durations["quarter"].unique()
        reasons = offline_durations["reason"].unique()

        # Generate all possible combinations
        all_combinations = list(
            itertools.product(registrations, six_hour_bins, quarters, reasons)
        )

        # Create complete dataframe with all combinations
        complete_df = pd.DataFrame(
            all_combinations,
            columns=["registration", "six_hour_bin", "quarter", "reason"],
        )

        # Merge with original data to preserve existing values, fill missing with 0
        offline_durations = complete_df.merge(
            offline_durations,
            on=["registration", "six_hour_bin", "quarter", "reason"],
            how="left",
        )

        # Fill NaN values with 0 (no downtime for those combinations)
        offline_durations["total_offline"] = offline_durations["total_offline"].fillna(
            0.0
        )

        # Sort for better readability
        offline_durations = offline_durations.sort_values(
            ["registration", "six_hour_bin", "quarter", "reason"]
        ).reset_index(drop=True)

        # Ensure consistent case for registration names
        available_time["registration"] = available_time["registration"].str.lower()
        offline_durations["registration"] = offline_durations[
            "registration"
        ].str.lower()

        # Merge scheduled time with downtime data
        ad_hoc = available_time.merge(
            offline_durations, on=["registration", "quarter", "six_hour_bin"]
        )
        ad_hoc["probability"] = ad_hoc["total_offline"] / ad_hoc["rota_time"]

        # Calculate availability probability (1 - sum of all unavailability probabilities)
        available_prop_df = (
            ad_hoc.groupby(["quarter", "six_hour_bin", "registration", "rota_time"])[
                ["probability"]
            ]
            .sum()
            .reset_index()
        )

        available_prop_df["reason"] = "available"
        available_prop_df["probability"] = 1 - available_prop_df["probability"]

        # Combine unavailability and availability data
        final_ad_hoc_df = pd.concat([ad_hoc, available_prop_df]).sort_values(
            ["quarter", "six_hour_bin", "registration", "reason"]
        )

        # Handle cases where there's no scheduled time (set probability to NaN)
        final_ad_hoc_df["probability"] = final_ad_hoc_df.apply(
            lambda x: np.nan if x["rota_time"] == 0 else x["probability"], axis=1
        )

        final_ad_hoc_df["probability"] = final_ad_hoc_df["probability"].round(5)

        # Save results to CSV
        final_ad_hoc_df[
            ["registration", "six_hour_bin", "quarter", "reason", "probability"]
        ].to_csv("distribution_data/ad_hoc_unavailability.csv", index=False)

    def run_sim_on_historical_params(self):
        # Ensure all rotas are using default values
        rota = pd.read_csv("tests/rotas_historic/HISTORIC_HEMS_ROTA.csv")
        rota.to_csv("actual_data/HEMS_ROTA.csv", index=False)

        callsign_reg_lookup = pd.read_csv(
            "tests/rotas_historic/HISTORIC_callsign_registration_lookup.csv"
        )
        callsign_reg_lookup.to_csv(
            "actual_data/callsign_registration_lookup.csv", index=False
        )

        service_history = pd.read_csv(
            "tests/rotas_historic/HISTORIC_service_history.csv"
        )
        service_history.to_csv("actual_data/service_history.csv", index=False)

        service_sched = pd.read_csv(
            "tests/rotas_historic/HISTORIC_service_schedules_by_model.csv"
        )
        service_sched.to_csv("actual_data/service_schedules_by_model.csv", index=False)

        print("Generating simulation results...")
        removeExistingResults()

        total_runs = 30
        sim_years = 2
        sim_duration = 60 * 24 * 7 * 52 * sim_years

        parallelProcessJoblib(
            total_runs=total_runs,
            sim_duration=sim_duration,
            warm_up_time=0,
            sim_start_date=datetime.strptime(
                "2023-01-01 05:00:00", "%Y-%m-%d %H:%M:%S"
            ),
            amb_data=False,
            print_debug_messages=True,
        )

        collateRunResults()

        try:
            results_all_runs = pd.read_csv("data/run_results.csv")
            # results_all_runs.to_csv("historical_data/calculated/SIM_hist_params.csv", index=False)

            # save parameters used
            # TODO: Add saving of parameters used

            # save data of counts of suboptimal care category sent
            counts_df = (
                results_all_runs[results_all_runs["event_type"] == "resource_use"][
                    ["run_number", "hems_res_category", "care_cat"]
                ]
                .value_counts()
                .reset_index()
            )

            counts_df_summary = (
                counts_df.groupby(["hems_res_category", "care_cat"])["count"]
                .agg(["mean", "min", "max"])
                .reset_index()
            )

            counts_df_summary.to_csv(
                "historical_data/calculated/SIM_hist_params_suboptimal_care_cat_sent_summary.csv"
            )

            # save data of counts of suboptimal vehicle type sent
            counts_df = (
                results_all_runs[results_all_runs["event_type"] == "resource_use"][
                    ["run_number", "vehicle_type", "heli_benefit"]
                ]
                .value_counts()
                .reset_index()
            )

            counts_df_summary = (
                counts_df.groupby(["vehicle_type", "heli_benefit"])["count"]
                .agg(["mean", "min", "max"])
                .reset_index()
            )

            counts_df_summary.to_csv(
                "historical_data/calculated/SIM_hist_params_suboptimal_vehicle_type_sent_summary.csv"
            )

            # # Also run the model to get some base-case outputs
            # resource_requests = (
            #     results_all_runs[results_all_runs["event_type"] == "resource_request_outcome"]
            #     .copy()
            #     )

            # resource_requests["care_cat"] = (
            #     resource_requests.apply(lambda x: "REG - Helicopter Benefit" if x["heli_benefit"]=="y"
            #                             and x["care_cat"]=="REG" else x["care_cat"],
            #                             axis=1)
            #                             )

            # missed_jobs_care_cat_summary = (
            #     resource_requests[["care_cat", "time_type"]].value_counts().reset_index(name="jobs")
            #     .sort_values(["care_cat", "time_type"])
            #     .copy()
            #     )

            # missed_jobs_care_cat_summary["jobs_average"] = (
            #     missed_jobs_care_cat_summary["jobs"]/
            #     total_runs
            #     )

            # missed_jobs_care_cat_summary["jobs_per_year_average"] = (
            #     (missed_jobs_care_cat_summary["jobs_average"] / float(sim_years*365)*365)
            #     ).round(0)

            missed_jobs_care_cat_summary = _job_outcome_calculation.get_missed_call_df(
                results_all_runs=results_all_runs,
                run_length_days=float(sim_years * 365),
                what="summary",
            )

            missed_jobs_care_cat_summary.to_csv(
                "historical_data/calculated/SIM_hist_params_missed_jobs_care_cat_summary.csv"
            )

            missed_jobs_care_cat_breakdown = (
                _job_outcome_calculation.get_missed_call_df(
                    results_all_runs=results_all_runs,
                    run_length_days=float(sim_years * 365),
                    what="breakdown",
                )
            )

            missed_jobs_care_cat_breakdown.to_csv(
                "historical_data/calculated/SIM_hist_params_missed_jobs_care_cat_breakdown.csv"
            )

        except FileNotFoundError:
            pass
****************************************

****************************************
air_ambulance_des\utils.py
****************************************
from datetime import datetime, time, timedelta
import json
import random
import numpy as np
import pandas as pd
import ast
import scipy
import calendar
from streamlit_extras.stylable_container import stylable_container
from numpy.random import SeedSequence, default_rng
from scipy.stats import (
    poisson,
    bernoulli,
    triang,
    erlang,
    weibull_min,
    exponweib,
    betabinom,
    pearson3,
    cauchy,
    chi2,
    expon,
    exponpow,
    gamma,
    lognorm,
    norm,
    powerlaw,
    rayleigh,
    uniform,
)
import logging
from pathlib import Path

PACKAGE_ROOT = Path(__file__).resolve().parent  # points to air_ambulance_des/
REPO_ROOT = PACKAGE_ROOT.parent  # points to repo root


class Utils:
    RESULTS_FOLDER = REPO_ROOT / "data"
    ACTUAL_DATA_FOLDER = REPO_ROOT / "actual_data"
    HISTORICAL_FOLDER = REPO_ROOT / "historical_data"
    DISTRIBUTION_FOLDER = REPO_ROOT / "distribution_data"

    ALL_RESULTS_CSV = RESULTS_FOLDER / "all_results.csv"
    RUN_RESULTS_CSV = RESULTS_FOLDER / "run_results.csv"

    # External file containing details of resources
    # hours of operation and servicing schedules
    HEMS_ROTA_DEFAULT = pd.read_csv(
        REPO_ROOT / ACTUAL_DATA_FOLDER / "HEMS_ROTA_DEFAULT.csv"
    )
    HEMS_ROTA = pd.read_csv(REPO_ROOT / ACTUAL_DATA_FOLDER / "HEMS_ROTA.csv")

    SERVICING_SCHEDULES_BY_MODEL = pd.read_csv(
        REPO_ROOT / ACTUAL_DATA_FOLDER / "service_schedules_by_model.csv"
    )

    TIME_TYPES = [
        "call start",
        "mobile",
        "at scene",
        "leaving scene",
        "at hospital",
        "handover",
        "clear",
        "stand down",
    ]

    def __init__(self, master_seed=SeedSequence(42), print_debug_messages=False):
        # Record the primary master seed sequence passed in to the sequence
        self.master_seed_sequence = master_seed

        ###############################
        # Set up remaining attributes #
        ###############################
        self.print_debug_messages = print_debug_messages

        # Load in mean inter_arrival_times
        self.hourly_arrival_by_qtr_probs_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER / "hourly_arrival_by_qtr_probs.csv"
        )
        self.hour_by_ampds_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER / "hour_by_ampds_card_probs.csv"
        )
        self.sex_by_ampds_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER / "sex_by_ampds_card_probs.csv"
        )
        self.care_cat_by_ampds_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER
            / "enhanced_or_critical_care_by_ampds_card_probs.csv"
        )
        # self.callsign_by_care_category_df = pd.read_csv('distribution_data/callsign_group_by_care_category_probs.csv')
        self.callsign_group_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER / "callsign_group_probs.csv"
        )
        # New addition without stratification by month
        self.vehicle_type_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER / "vehicle_type_probs.csv"
        )
        self.vehicle_type_quarter_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER / "vehicle_type_by_quarter_probs.csv"
        )

        # ========= ARCHIVED CODE ================== ##
        # self.vehicle_type_by_month_df = pd.read_csv('distribution_data/vehicle_type_by_month_probs.csv')
        # self.inter_arrival_rate_df = pd.read_csv('distribution_data/inter_arrival_times.csv')
        # self.callsign_by_ampds_and_hour_df = pd.read_csv('distribution_data/callsign_group_by_ampds_card_and_hour_probs.csv')
        # self.callsign_by_ampds_df = pd.read_csv('distribution_data/callsign_group_by_ampds_card_probs.csv')
        # self.hems_result_by_callsign_group_and_vehicle_type_df = pd.read_csv('distribution_data/hems_result_by_callsign_group_and_vehicle_type_probs.csv')
        # self.hems_result_by_care_category_and_helicopter_benefit_df = pd.read_csv('distribution_data/hems_result_by_care_cat_and_helicopter_benefit_probs.csv')
        # self.pt_outcome_by_hems_result_and_care_category_df = pd.read_csv('distribution_data/pt_outcome_by_hems_result_and_care_category_probs.csv')
        # ========= END ARCHIVED CODE ============== #

        # NEW PATIENT OUTCOME AND HEMS RESULT DATA FRAMES
        self.patient_outcome_by_care_category_and_quarter_probs_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER
            / "patient_outcome_by_care_category_and_quarter_probs.csv"
        )
        # self.hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_probs_df = pd.read_csv('distribution_data/hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_probs.csv')
        self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df = pd.read_csv(
            self.DISTRIBUTION_FOLDER
            / "hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs.csv"
        )

        # Import maximum call duration times
        self.min_max_values_df = pd.read_csv(
            self.ACTUAL_DATA_FOLDER / "upper_allowable_time_bounds.csv"
        )

        # Load ad hoc unavailability probability table
        try:
            # This file may not exist depending on when utility is called e.g. fitting
            self.ad_hoc_probs = pd.read_csv(
                self.DISTRIBUTION_FOLDER / "ad_hoc_unavailability.csv"
            )
            # Remove ad-hoc probs for any times outside of historical
        except FileNotFoundError:
            self.debug("ad_hoc spreadsheet does not exist yet")

        # Read in age distribution data into a dictionary
        age_data = []
        with open(self.DISTRIBUTION_FOLDER / "age_distributions.txt", "r") as inFile:
            age_data = ast.literal_eval(inFile.read())
        inFile.close()
        self.age_distr = age_data

        # Read in activity time distribution data into a dictionary
        activity_time_data = []
        with open(
            self.DISTRIBUTION_FOLDER / "activity_time_distributions.txt", "r"
        ) as inFile:
            activity_time_data = ast.literal_eval(inFile.read())
        inFile.close()
        self.activity_time_distr = activity_time_data

        # Read in incident per day distribution data into a dictionary
        inc_per_day_data = []
        with open(
            self.DISTRIBUTION_FOLDER / "inc_per_day_distributions.txt", "r"
        ) as inFile:
            inc_per_day_data = ast.literal_eval(inFile.read())
        inFile.close()
        self.inc_per_day_distr = inc_per_day_data

        inc_per_day_per_qtr_data = []
        with open(
            self.DISTRIBUTION_FOLDER / "inc_per_day_qtr_distributions.txt", "r"
        ) as inFile:
            inc_per_day_per_qtr_data = ast.literal_eval(inFile.read())
        inFile.close()
        self.inc_per_day_per_qtr_distr = inc_per_day_per_qtr_data

        # Incident per day samples
        with open(self.DISTRIBUTION_FOLDER / "inc_per_day_samples.json", "r") as f:
            self.incident_per_day_samples = json.load(f)

        # Turn the min-max activity times data into a format that supports easier/faster
        # lookups
        self.min_max_cache = {
            row["time"]: (row["min_value_mins"], row["max_value_mins"])
            for _, row in self.min_max_values_df.iterrows()
        }

    def setup_seeds(self):
        #########################################################
        # Control generation of required random number streams  #
        #########################################################

        # Spawn substreams for major simulation modules
        module_keys = [
            "activity_times",
            "ampds_code_selection",
            "care_category_selection",
            "callsign_group_selection",
            "vehicle_type_selection",
            "hems_result_by_callsign_group_and_vehicle_type_selection",
            "hems_result_by_care_category_and_helicopter_benefit_selection",
            "hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_selection",
            "pt_outcome_selection",
            "sex_selection",
            "age_sampling",
            "calls_per_day",
            "calls_per_hour",
            "predetermine_call_arrival",
            "call_iat",
            "helicopter_benefit_from_reg",
            "hems_case",
            "ad_hoc_reason_selection",
            "know_heli_benefit",
            "helicopter_benefit_from_cc",
            "helicopter_benefit_from_ec",
            "know_cc_ec_benefit",
        ]
        # Efficiently spawn substreams
        spawned = self.master_seed_sequence.spawn(len(module_keys))

        # Map substreams and RNGs to keys
        self.seed_substreams = dict(zip(module_keys, spawned))
        self.rngs = {key: default_rng(ss) for key, ss in self.seed_substreams.items()}

        self.build_seeded_distributions(self.master_seed_sequence)

    def debug(self, message: str):
        if self.print_debug_messages:
            logging.debug(message)

    def current_time() -> str:
        """
        Return the current time as a string in the format hh:mm:ss
        """
        now = datetime.now()
        return now.strftime("%H:%M:%S")

    def date_time_of_call(
        self, start_dt: str, elapsed_time: int
    ) -> list[int, int, str, int, pd.Timestamp]:
        """
        Calculate a range of time-based parameters given a specific date-time

        **Returns:**
            list(
                `dow`             : int
                `current_hour`    : int
                `weekday`         : str
                `current_month`   : int
                `current_quarter` : int
                `current_dt`      : datetime
            )

        """
        # Elapsed_time = time in minutes since simulation started

        start_dt = pd.to_datetime(start_dt)

        current_dt = start_dt + pd.Timedelta(elapsed_time, unit="min")

        dow = current_dt.strftime("%a")
        # 0 = Monday, 6 = Sunday
        weekday = "weekday" if current_dt.dayofweek < 5 else "weekend"

        current_hour = current_dt.hour

        current_month = current_dt.month

        current_quarter = current_dt.quarter

        return [dow, current_hour, weekday, current_month, current_quarter, current_dt]

    # SR 2025-04-17: Commenting out as I believe this is no longer used due to changes
    # in the way inter-arrival times for calls are managed
    # def inter_arrival_rate(self, hour: int, quarter: int) -> float:
    #     """
    #         This function will return the current mean inter_arrival rate in minutes
    #         for the provided hour of day and yearly quarter

    #         NOTE: not used with NSPPThinning
    #     """

    #     #print(f"IA with values hour {hour} and quarter {quarter}")
    #     df = self.inter_arrival_rate_df
    #     mean_ia = df[(df['hour'] == hour) & (df['quarter'] == quarter)]['mean_iat']

    #     # Currently have issue in that if hour and quarter not in data e.g. 0200 in quarter 3
    #     # then iloc value broken. Set default to 120 in that case.

    #     return 120 if len(mean_ia) == 0 else mean_ia.iloc[0]
    #     #return mean_ia.iloc[0]

    def ampds_code_selection(self, hour: int) -> int:
        """
        This function will allocate and return an AMPDS card category
        based on the hour of day
        """

        df = self.hour_by_ampds_df[self.hour_by_ampds_df["hour"] == hour]

        return pd.Series.sample(
            df["ampds_card"],
            weights=df["proportion"],
            random_state=self.rngs["ampds_code_selection"],
        ).iloc[0]

    def is_time_in_range(self, current: int, start: int, end: int) -> bool:
        """
        Function to check if a given time is within a range of start and end times on a 24-hour clock.

        Parameters:
        - current (datetime.time): The time to check.
        - start (datetime.time): The start time.
        - end (datetime.time): The end time.

        """

        current = time(current, 0)
        start = time(start, 0)
        end = time(end, 0)

        if start <= end:
            # Range does not cross midnight
            return start <= current < end
        else:
            # Range crosses midnight
            return current >= start or current < end

    def care_category_selection(self, ampds_card: str) -> str:
        """
        This function will allocate and return an care category
        based on AMPDS card
        """

        # print(f"Callsign group selection with {hour} and {ampds_card}")

        df = self.care_cat_by_ampds_df[
            (self.care_cat_by_ampds_df["ampds_card"] == ampds_card)
        ]

        return pd.Series.sample(
            df["care_category"],
            weights=df["proportion"],
            random_state=self.rngs["care_category_selection"],
        ).iloc[0]

    # def LEGACY_callsign_group_selection(self, ampds_card: str) -> int:
    #     """
    #         This function will allocate and return an callsign group
    #         based on AMPDS card
    #     """

    #     #print(f"Callsign group selection with {hour} and {ampds_card}")

    #     df = self.callsign_by_ampds_df[
    #         (self.callsign_by_ampds_df['ampds_card'] == ampds_card)
    #     ]

    #     return pd.Series.sample(df['callsign_group'], weights = df['proportion'],
    #                             random_state=self.rngs["callsign_group_selection"]).iloc[0]

    # def callsign_group_selection(self, care_category: str) -> int:
    #     """
    #         This function will allocate and return an callsign group
    #         based on the care category
    #     """

    #     #print(f"Callsign group selection with {hour} and {ampds_card}")

    #     df = self.callsign_by_care_category_df[
    #         (self.callsign_by_care_category_df['care_category'] == care_category)
    #     ]

    #     return pd.Series.sample(df['callsign_group'], weights = df['proportion'],
    #                             random_state=self.rngs["callsign_group_selection"]).iloc[0]

    def callsign_group_selection(self) -> int:
        """
        This function will allocate and return an callsign group
        based on the care category
        """

        # print(f"Callsign group selection with {hour} and {ampds_card}")

        return pd.Series.sample(
            self.callsign_group_df["callsign_group"],
            weights=self.callsign_group_df["proportion"],
            random_state=self.rngs["callsign_group_selection"],
        ).iloc[0]

    # def vehicle_type_selection(self, callsign_group: str) -> int:
    #     """
    #         This function will allocate and return a vehicle type
    #         based callsign group
    #     """

    #     df = self.vehicle_type_df[
    #         (self.vehicle_type_df['callsign_group'] == int(callsign_group)) # Cater for Other
    #     ]

    #     return pd.Series.sample(df['vehicle_type'], weights = df['proportion'],
    #                             random_state=self.rngs["vehicle_type_selection"]).iloc[0]

    def vehicle_type_selection_qtr(self, callsign_group: str, qtr: int) -> int:
        """
        This function will allocate and return a vehicle type
        based callsign group
        """

        df = self.vehicle_type_quarter_df[
            (
                self.vehicle_type_quarter_df["callsign_group"] == int(callsign_group)
            )  # Cater for Other
            & (self.vehicle_type_quarter_df["quarter"] == int(qtr))
        ]

        return pd.Series.sample(
            df["vehicle_type"],
            weights=df["proportion"],
            random_state=self.rngs["vehicle_type_selection"],
        ).iloc[0]

    # def hems_result_by_callsign_group_and_vehicle_type_selection(self, callsign_group: str, vehicle_type: str) -> str:
    #     """
    #         This function will allocate a HEMS result based on callsign group and vehicle type
    #     """

    #     df = self.hems_result_by_callsign_group_and_vehicle_type_df[
    #         (self.hems_result_by_callsign_group_and_vehicle_type_df['callsign_group'] == int(callsign_group)) &
    #         (self.hems_result_by_callsign_group_and_vehicle_type_df['vehicle_type'] == vehicle_type)
    #     ]

    #     return pd.Series.sample(df['hems_result'], weights = df['proportion'],
    #                             random_state=self.rngs["hems_result_by_callsign_group_and_vehicle_type_selection"]).iloc[0]

    # def hems_result_by_care_category_and_helicopter_benefit_selection(self, care_category: str, helicopter_benefit: str) -> str:
    #     """
    #         This function will allocate a HEMS result based on care category and helicopter benefit
    #     """

    #     df = self.hems_result_by_care_category_and_helicopter_benefit_df[
    #         (self.hems_result_by_care_category_and_helicopter_benefit_df['care_cat'] == care_category) &
    #         (self.hems_result_by_care_category_and_helicopter_benefit_df['helicopter_benefit'] == helicopter_benefit)
    #     ]

    #     return pd.Series.sample(df['hems_result'], weights = df['proportion'],
    #                             random_state=self.rngs["hems_result_by_care_category_and_helicopter_benefit_selection"]).iloc[0]

    def hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs(
        self,
        pt_outcome: str,
        quarter: int,
        vehicle_type: str,
        callsign_group: int,
        hour: int,
    ):
        """
        This function will allocate a HEMS result based on patient outcome, yearly quarter and HEMS deets.
        """

        if (hour >= 7) and (hour <= 18):
            time_of_day = "day"
        else:
            time_of_day = "night"

        self.debug(f"Hour is {hour}: for hems_result sampling, this is {time_of_day}")

        # (self.hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_probs_df.head())

        df = self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df[
            (
                self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df[
                    "pt_outcome"
                ]
                == pt_outcome
            )
            & (
                self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df[
                    "quarter"
                ]
                == quarter
            )
            & (
                self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df[
                    "vehicle_type"
                ]
                == vehicle_type
            )
            & (
                self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df[
                    "callsign_group"
                ]
                == callsign_group
            )
            & (
                self.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs_df[
                    "time_of_day"
                ]
                == time_of_day
            )
        ]

        # print(df.head())

        return pd.Series.sample(
            df["hems_result"],
            weights=df["proportion"],
            random_state=self.rngs[
                "hems_results_by_patient_outcome_and_quarter_and_vehicle_type_and_callsign_group_selection"
            ],
        ).iloc[0]

    def pt_outcome_selection(self, care_category: str, quarter: int) -> int:
        """
        This function will allocate and return an patient outcome
        based on the HEMS result
        """

        df = self.patient_outcome_by_care_category_and_quarter_probs_df[
            (
                self.patient_outcome_by_care_category_and_quarter_probs_df[
                    "care_category"
                ]
                == care_category
            )
            & (
                self.patient_outcome_by_care_category_and_quarter_probs_df["quarter"]
                == quarter
            )
        ]

        # print(df)
        return pd.Series.sample(
            df["pt_outcome"],
            weights=df["proportion"],
            random_state=self.rngs["pt_outcome_selection"],
        ).iloc[0]

    # TODO: RANDOM SEED SETTING
    def sex_selection(self, ampds_card: int) -> str:
        """
        This function will allocate and return the patient sex
        based on allocated AMPDS card category
        """

        prob_female = self.sex_by_ampds_df[
            self.sex_by_ampds_df["ampds_card"] == ampds_card
        ]["proportion"]

        return (
            "Female"
            if (self.rngs["sex_selection"].uniform(0, 1) < prob_female.iloc[0])
            else "Male"
        )

    # TODO: RANDOM SEED SETTING
    def age_sampling(self, ampds_card: int, max_age: int) -> float:
        """
        This function will return the patient's age based
        on sampling from the distribution that matches the allocated AMPDS card
        """

        distribution = {}

        # print(self.age_distr)

        for i in self.age_distr:
            # print(i)
            if i["ampds_card"] == str(ampds_card):
                # print('Match')
                distribution = i["best_fit"]

        # print(f"Getting age for {ampds_card}")
        # print(distribution)

        age = 100000
        while age > max_age:
            age = self.sample_from_distribution(
                distribution, rng=self.rngs["age_sampling"]
            )

        return age

    def activity_time(self, vehicle_type: str, time_type: str) -> float:
        """
        This function will return a dictionary containing
        the distribution and parameters for the distribution
        that match the provided HEMS vehicle type and time type

        """

        dist = self.activity_time_distr.get((vehicle_type, time_type))
        # print(dist)

        if dist is None:
            raise ValueError(f"No distribution found for ({vehicle_type}, {time_type})")

        try:
            min_time, max_time = self.min_max_cache[time_type]
            # print(f"Min time {time_type}: {min_time}")
            # print(f"Max time {time_type}: {max_time}")
        except KeyError:
            raise ValueError(f"Min/max bounds not found for time_type='{time_type}'")

        sampled_time = -1
        while not (min_time <= sampled_time <= max_time):
            sampled_time = dist.sample()

        # print(sampled_time)

        return sampled_time

    def inc_per_day(self, quarter: int, quarter_or_season: str = "season") -> float:
        """
        This function will return the number of incidents for a given
        day

        """

        season = "summer" if quarter in [2, 3] else "winter"

        distribution = {}
        max_n = 0
        min_n = 0

        distr_data = (
            self.inc_per_day_distr
            if quarter_or_season == "season"
            else self.inc_per_day_per_qtr_distr
        )

        for i in distr_data:
            # print(i)
            if quarter_or_season == "season":
                if i["season"] == season:
                    # print('Match')
                    distribution = i["best_fit"]
                    max_n = i["max_n_per_day"]
                    min_n = i["min_n_per_day"]
            else:
                if i["quarter"] == quarter:
                    # print('Match')
                    distribution = i["best_fit"]
                    max_n = i["max_n_per_day"]
                    min_n = i["min_n_per_day"]

        sampled_inc_per_day = -1

        while not (sampled_inc_per_day >= min_n and sampled_inc_per_day <= max_n):
            sampled_inc_per_day = self.sample_from_distribution(
                distribution, rng=self.rngs["calls_per_day"]
            )

        return sampled_inc_per_day

    def inc_per_day_samples(
        self, quarter: int, quarter_or_season: str = "season"
    ) -> float:
        """
        This function will return a dictionary containing
        the distribution and parameters for the distribution
        that match the provided HEMS vehicle type and time type

        """

        season = "summer" if quarter in [2, 3] else "winter"

        if quarter_or_season == "season":
            return self.rngs["calls_per_day"].choice(
                self.incident_per_day_samples[season]
            )
        else:
            return self.rngs["calls_per_day"].choice(
                self.incident_per_day_samples[f"Q{quarter}"]
            )

    def sample_from_distribution(self, distr: dict, rng: np.random.Generator) -> float:
        """
        Sample a single float value from a seeded scipy distribution.

        Parameters
        ----------
        distr : dict
            A dictionary with one key (the distribution name) and value as the parameters.
        rng : np.random.Generator
            A seeded RNG from the simulation's RNG stream pool.

        Returns
        -------
        float
            A positive sampled value from the specified distribution.
        """
        if len(distr) != 1:
            raise ValueError("Expected one distribution name in distr dictionary")

        dist_name, params = list(distr.items())[0]
        sci_distr = getattr(scipy.stats, dist_name)

        while True:
            sampled_value = np.floor(sci_distr.rvs(random_state=rng, **params))
            if sampled_value > 0:
                return sampled_value

    def get_nth_weekday(self, year: int, month: int, weekday: int, n: int):
        """
        Calculate  date of the nth occurrence of a weekday in a given month and year.
        """

        first_day = datetime(year, month, 1)
        first_weekday = first_day.weekday()
        days_until_weekday = (weekday - first_weekday + 7) % 7
        first_occurrence = first_day + timedelta(days=days_until_weekday)

        return first_occurrence + timedelta(weeks=n - 1)

    def get_last_weekday(self, year, month, weekday):
        """
        Return the date of the last occurrence of a weekday in a given month and year.
        """

        last_day = datetime(year, month, calendar.monthrange(year, month)[1])
        last_weekday = last_day.weekday()
        days_since_weekday = (last_weekday - weekday + 7) % 7

        return last_day - timedelta(days=days_since_weekday)

    def calculate_term_holidays(self, year):
        holidays = []

        # Jan to Easter
        start_date = datetime(year, 1, 1)

        first_mon_of_year = self.get_nth_weekday(year, 1, calendar.MONDAY, 1)

        jan_term_start = first_mon_of_year

        if start_date.weekday() in [0, 6]:
            # print(f"1st jan is {start_date.weekday()}")
            # 1st Jan is a a weekday
            jan_term_start += timedelta(days=1)

        # print(f"Year {year} - start_date: {start_date} and term_start is {jan_term_start}")

        holidays.append(
            {
                "year": int(year),
                "start_date": start_date,
                "end_date": jan_term_start - timedelta(days=1),
            }
        )

        # Spring half-term

        spring_half_term_start = self.get_nth_weekday(year, 2, calendar.MONDAY, 2)
        spring_half_term_end = spring_half_term_start + timedelta(days=4)

        holidays.append(
            {
                "year": int(year),
                "start_date": spring_half_term_start,
                "end_date": spring_half_term_end,
            }
        )

        # Easter hols

        # Calculate Good Friday
        easter_sunday = self.calculate_easter(year)

        # print(f"Easter Sunday is {easter_sunday}")
        good_friday = easter_sunday - timedelta(days=2)

        # If ES is in March, 1st two weeks of Apri
        # Otherwise, Monday of ES week + 1 week
        start_date = easter_sunday - timedelta(days=6)
        end_date = start_date + timedelta(days=13)

        if easter_sunday.month == 3 or (
            easter_sunday.month == 4
            and easter_sunday >= self.get_nth_weekday(year, 4, calendar.SUNDAY, 2)
        ):
            start_date = self.get_nth_weekday(year, 4, calendar.MONDAY, 1)
            if easter_sunday.month == 4 and easter_sunday >= self.get_nth_weekday(
                year, 4, calendar.SUNDAY, 2
            ):
                # Will also likely be a late Easter Monday
                end_date = start_date + timedelta(days=14)
            else:
                end_date = start_date + timedelta(days=13)

        holidays.append(
            {"year": int(year), "start_date": start_date, "end_date": end_date}
        )

        # Summer half-term

        summer_half_term_start = self.get_last_weekday(year, 5, calendar.MONDAY)
        summer_half_term_end = summer_half_term_start + timedelta(days=6)

        holidays.append(
            {
                "year": int(year),
                "start_date": summer_half_term_start,
                "end_date": summer_half_term_end,
            }
        )

        # Summer Holidays

        summer_start = self.get_last_weekday(year, 7, calendar.MONDAY)
        summer_end = self.get_nth_weekday(year, 9, calendar.MONDAY, 1)

        holidays.append(
            {"year": int(year), "start_date": summer_start, "end_date": summer_end}
        )

        # Autumn Term

        autumn_half_term_start = summer_end + timedelta(weeks=8)

        if summer_end.day >= 4:
            autumn_half_term_start = summer_end + timedelta(weeks=7)

        autumn_half_term_end = autumn_half_term_start + timedelta(days=6)

        holidays.append(
            {
                "year": int(year),
                "start_date": autumn_half_term_start,
                "end_date": autumn_half_term_end,
            }
        )

        # Christmas Hols

        start_date = self.get_last_weekday(year, 12, calendar.MONDAY) - timedelta(
            days=7
        )

        holidays.append(
            {
                "year": int(year),
                "start_date": start_date,
                "end_date": datetime(year, 12, 31),
            }
        )

        return pd.DataFrame(holidays)

    def calculate_easter(self, year):
        """
        Calculate the date of Easter Sunday for a given year using the Anonymous Gregorian algorithm.
        Converted to Python from this SO answer: https://stackoverflow.com/a/49558298/3650230
        Really interesting rabbit hole to go down about this in the whole thread: https://stackoverflow.com/questions/2192533/function-to-return-date-of-easter-for-the-given-year
        """

        a = year % 19
        b = year // 100
        c = year % 100
        d = b // 4
        e = b % 4
        f = (b + 8) // 25
        g = (b - f + 1) // 3
        h = (19 * a + b - d - g + 15) % 30
        k = c % 4
        i = (c - k) // 4
        l = (32 + 2 * e + 2 * i - h - k) % 7
        m = (a + 11 * h + 22 * l) // 451
        month = (h + l - 7 * m + 114) // 31
        day = ((h + l - 7 * m + 114) % 31) + 1

        return datetime(year, month, day)

    def years_between(self, start_date: datetime, end_date: datetime) -> list[int]:
        return list(range(start_date.year, end_date.year + 1))

    def biased_mean(series: pd.Series, bias: float = 0.5) -> float:
        """

        Compute a weighted mean, favoring the larger value since demand
        likely to only increase with time

        """

        if len(series) == 1:
            return series.iloc[0]  # Return the only value if there's just one

        sorted_vals = np.sort(series)  # Ensure values are sorted
        weights = np.linspace(
            1, bias * 2, len(series)
        )  # Increasing weights with larger values
        return np.average(sorted_vals, weights=weights)

    def build_seeded_distributions(self, seed_seq):
        """
        Build a dictionary of seeded distributions keyed by (vehicle_type, time_type)

        Returns
        -------
        dict of (vehicle_type, time_type) -> SeededDistribution

        e.g. {('helicopter', 'time_allocation'): <utils.SeededDistribution object at 0x000001521627C750>,
        ('car', 'time_allocation'): <utils.SeededDistribution object at 0x000001521627D410>,
        ('helicopter', 'time_mobile'): <utils.SeededDistribution object at 0x0000015216267E90>}
        """
        n = len(self.activity_time_distr)
        rngs = [default_rng(s) for s in seed_seq.spawn(n)]

        dist_map = {
            "poisson": poisson,
            "bernoulli": bernoulli,
            "triang": triang,
            "erlang": erlang,
            "weibull_min": weibull_min,
            "expon_weib": exponweib,
            "betabinom": betabinom,
            "pearson3": pearson3,
            "cauchy": cauchy,
            "chi2": chi2,
            "expon": expon,
            "exponential": expon,  # alias for consistency
            "exponpow": exponpow,
            "gamma": gamma,
            "lognorm": lognorm,
            "norm": norm,
            "normal": norm,  # alias for consistency
            "powerlaw": powerlaw,
            "rayleigh": rayleigh,
            "uniform": uniform,
        }

        seeded_distributions = {}

        # print(activity_time_distr)
        # print(len(self.activity_time_distr))
        i = 0
        for entry, rng in zip(self.activity_time_distr, rngs):
            i += 1
            vt = entry["vehicle_type"]
            tt = entry["time_type"]
            best_fit = entry["best_fit"]

            # dist_name = best_fit['dist'].lower()
            dist_name = list(best_fit.keys())[0]
            dist_cls = dist_map.get(dist_name)
            self.debug(
                f"{i}/{len(self.activity_time_distr)} for {vt} {tt} set up {dist_name} {dist_cls} with params: {best_fit[dist_name]}"
            )

            if dist_cls is None:
                raise ValueError(f"Unsupported distribution type: {dist_name}")

            params = best_fit[dist_name]

            seeded_distributions[(vt, tt)] = SeededDistribution(dist_cls, rng, **params)

        self.activity_time_distr = seeded_distributions

        return seeded_distributions

    def sample_ad_hoc_reason(self, hour: int, quarter: int, registration: str) -> bool:
        """
        Sample from ad hoc unavailability probability table based on time bin and quarter.
        Returns a reason string (e.g. 'available', 'crew', 'weather', etc.).
        """
        # Determine time bin
        if 0 <= hour <= 5:
            bin_label = "00-05"
        elif 6 <= hour <= 11:
            bin_label = "06-11"
        elif 12 <= hour <= 17:
            bin_label = "12-17"
        else:
            bin_label = "18-23"

        # Main subset for registration, bin, and quarter
        subset = self.ad_hoc_probs[
            (self.ad_hoc_probs["registration"] == registration)
            & (self.ad_hoc_probs["six_hour_bin"] == bin_label)
            & (self.ad_hoc_probs["quarter"] == quarter)
        ]

        if subset.empty:
            self.debug(
                f"[AD HOC] No data for {registration} in bin {bin_label}, Q{quarter}. Falling back to all registrations."
            )
            subset = self.ad_hoc_probs[
                (self.ad_hoc_probs["six_hour_bin"] == bin_label)
                & (self.ad_hoc_probs["quarter"] == quarter)
            ]
            if subset.empty:
                self.debug(
                    f"[AD HOC] No data at all for bin {bin_label}, Q{quarter}. Defaulting to 'available'."
                )
                return "available"

        filled_subset = subset.copy()

        for idx, row in filled_subset.iterrows():
            if pd.isna(row["probability"]):
                reason = row["reason"]
                # Try registration-wide average across all bins in this quarter
                reg_avg = self.ad_hoc_probs[
                    (self.ad_hoc_probs["registration"] == row["registration"])
                    & (self.ad_hoc_probs["quarter"] == row["quarter"])
                    & (self.ad_hoc_probs["reason"] == reason)
                    & (self.ad_hoc_probs["probability"].notna())
                ]["probability"].mean()

                if pd.isna(reg_avg):
                    # Fall back: average for this bin/quarter across all registrations
                    fallback_avg = self.ad_hoc_probs[
                        (self.ad_hoc_probs["six_hour_bin"] == row["six_hour_bin"])
                        & (self.ad_hoc_probs["quarter"] == row["quarter"])
                        & (self.ad_hoc_probs["reason"] == reason)
                        & (self.ad_hoc_probs["probability"].notna())
                    ]["probability"].mean()

                    if pd.notna(fallback_avg):
                        self.debug(
                            f"[AD HOC] Missing prob for {registration}, {bin_label}, Q{quarter}, reason '{reason}'. Using fallback avg across registrations: {fallback_avg:.4f}"
                        )
                        filled_subset.at[idx, "probability"] = fallback_avg
                    else:
                        self.debug(
                            f"[AD HOC] Missing prob for {registration}, {bin_label}, Q{quarter}, reason '{reason}'. No fallback avg found. Defaulting to 'available'."
                        )
                        return "available"
                else:
                    self.debug(
                        f"[AD HOC] Missing prob for {registration}, {bin_label}, Q{quarter}, reason '{reason}'. Using reg-wide avg: {reg_avg:.4f}"
                    )
                    filled_subset.at[idx, "probability"] = reg_avg

        if filled_subset["probability"].isna().any():
            self.debug(
                f"[AD HOC] Still missing probabilities after imputation for {registration}, {bin_label}, Q{quarter}. Defaulting to 'available'."
            )
            return "available"

        total_prob = filled_subset["probability"].sum()
        if total_prob == 0 or pd.isna(total_prob):
            self.debug(
                f"[AD HOC] Total probability is zero or NaN for {registration}, {bin_label}, Q{quarter}. Defaulting to 'available'."
            )
            return "available"

        norm_probs = filled_subset["probability"] / total_prob

        sampled_reason = self.rngs["ad_hoc_reason_selection"].choice(
            filled_subset["reason"].tolist(), p=norm_probs.tolist()
        )

        return sampled_reason


class SeededDistribution:
    def __init__(self, dist, rng, **kwargs):
        self.dist = dist(**kwargs)
        self.rng = rng

    def sample(self, size=None):
        return self.dist.rvs(size=size, random_state=self.rng)


def format_diff(value):
    if value > 0:
        return f"**:red[+{value:.0f}]** from historical"
    elif value < 0:
        return f"**:green[{value:.0f}]** from historical"
    else:
        return "**:gray[no difference]** from historical"


# Mapping months to numbers
MONTH_MAPPING = {
    "January": 1,
    "February": 2,
    "March": 3,
    "April": 4,
    "May": 5,
    "June": 6,
    "July": 7,
    "August": 8,
    "September": 9,
    "October": 10,
    "November": 11,
    "December": 12,
}

REVERSE_MONTH_MAPPING = {v: k for k, v in MONTH_MAPPING.items()}


def get_rota_month_strings(start_month, end_month):
    # Convert selected months to numbers
    start_month_num = MONTH_MAPPING[start_month]
    end_month_num = MONTH_MAPPING[end_month]

    # Summer rota
    summer_start_date = f"1st {start_month}"
    summer_end_day = calendar.monthrange(2024, end_month_num)[
        1
    ]  # Assume leap year for Feb
    summer_end_date = f"{summer_end_day}th {end_month}"

    # Winter rota
    winter_start_num = (end_month_num % 12) + 1  # month after summer end
    winter_end_num = (
        (start_month_num - 1) if start_month_num > 1 else 12
    )  # month before summer start

    winter_start_date = f"1st {REVERSE_MONTH_MAPPING[winter_start_num]}"
    winter_end_day = calendar.monthrange(2024, winter_end_num)[
        1
    ]  # same leap year assumption
    winter_end_date = f"{winter_end_day}th {REVERSE_MONTH_MAPPING[winter_end_num]}"

    return (
        start_month_num,
        end_month_num,
        summer_start_date,
        summer_end_date,
        summer_end_day,
        winter_start_date,
        winter_end_date,
        winter_end_day,
    )


def format_sigfigs(x, sigfigs=4):
    from math import log10, floor

    try:
        if x == 0:
            return "0.0"
        elif x < 1e-10:
            return "<1e-10"
        else:
            digits = sigfigs - 1 - floor(log10(abs(x)))
            return f"{x:.{digits}f}"
    except (ValueError, TypeError):
        return str(x)


COLORSCHEME = {
    "red": "#D50032",
    "navy": "#00205B",
    "blue": "#1D428A",
    "teal": "#00B0B9",
    "lightblue": "#C0F0F2",
    "green": "#56E39F",
    "orange": "#FFA400",
    "yellow": "#F8C630",
    "darkgreen": "#264027",
    "verylightblue": "#D5F5F6",
    "lightgrey": "#CCCCCC",
    "darkgrey": "#4D4D4D",
    "charcoal": "#1F1F1F",
}


# 99th Percentile
def q99(x):
    return x.quantile(0.99)


# 95th Percentile
def q95(x):
    return x.quantile(0.95)


# 90th Percentile
def q90(x):
    return x.quantile(0.9)


# 10th Percentile
def q10(x):
    return x.quantile(0.1)


# 75th Percentile
def q75(x):
    return x.quantile(0.75)


# 25th Percentile
def q25(x):
    return x.quantile(0.25)


def to_military_time(hour: int) -> str:
    return f"{hour:02d}00"


def iconMetricContainer(
    key, icon_unicode, css_style=None, icon_color="grey", family="filled", type="icons"
):
    """Function that returns a CSS styled container for adding a Material Icon to a Streamlit st.metric value

    CREDIT for starter version of this code: https://discuss.streamlit.io/t/adding-an-icon-to-a-st-metric-easily/59140?u=sammi1

    Args:
        key (str): Unique key for the component
        iconUnicode (str): Code point for a Material Icon, you can find them here https://fonts.google.com/icons. Sample \e8b6
        css_style(str, optional): Additional CSS to apply
        icon_color (str, optional): HTML Hex color value for the icon. Defaults to 'grey'.
        family(str, optional): "filled" or "outline". Only works with type = "icons"
        type(str, optional): "icons" or "symbols"

    Returns:
        DeltaGenerator: A container object. Elements can be added to this container using either the 'with'
        notation or by calling methods directly on the returned object.
    """

    if (family == "filled") and (type == "icons"):
        font_family = "Material Icons"
    elif (family == "outline") and (type == "icons"):
        font_family = "Material Icons Outlined"
    # elif (family == "filled") and (type=="symbols"):
    #     font_family = "Material Symbols"
    elif type == "symbols":
        font_family = "Material Symbols Outlined"
    else:
        print("ERROR - Check Params for iconMetricContainer")
        font_family = "Material Icons"

    css_style_icon = f"""
                    div[data-testid="stMetricValue"]>div::before
                    {{
                        font-family: {font_family};
                        content: "\{icon_unicode}";
                        vertical-align: -20%;
                        color: {icon_color};
                    }}
                    """

    if css_style is not None:
        css_style_icon += """

        """

        css_style_icon += css_style

    iconMetric = stylable_container(key=key, css_styles=css_style_icon)
    return iconMetric
****************************************

****************************************
air_ambulance_des\_processing_functions.py
****************************************
import pandas as pd
import re
import functools
import traceback
import streamlit as st


def graceful(fn):
    """Decorator that handles errors gracefully in or out of Streamlit."""

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            tb = traceback.format_exc()

            # Detect Streamlit runtime safely
            in_streamlit = False
            try:
                import streamlit.runtime

                in_streamlit = streamlit.runtime.exists()
            except Exception:
                pass

            if in_streamlit:
                import streamlit as st

                st.error(f"⚠️ Failed in `{fn.__name__}`: {e}")
                st.code(tb)
            else:
                print(f"⚠️ Failed in `{fn.__name__}`: {e}")
                print(tb)
            return None

    return wrapper


def graceful_methods(cls):
    """Wrap all methods in a class with context-aware error handling:
    - If running in Streamlit, shows st.error("Failed to generate graph.")
    - Otherwise, logs the error to console.
    """
    for name, method in cls.__dict__.items():
        if callable(method) and not name.startswith("__"):

            def make_wrapper(fn):
                def wrapper(self, *args, **kwargs):
                    try:
                        return fn(self, *args, **kwargs)
                    except Exception as e:
                        # Detect Streamlit
                        try:
                            import streamlit.runtime

                            in_streamlit = streamlit.runtime.exists()
                        except Exception:
                            in_streamlit = False

                        if in_streamlit:
                            import streamlit as st

                            st.error(
                                f"⚠️ Failed to generate output from `{fn.__name__}`: {e}."
                            )
                        else:
                            print(f"⚠️ Failed to run {fn.__name__}: {e}")
                        return None

                return wrapper

            setattr(cls, name, make_wrapper(method))
    return cls


# def make_callsign_column(dataframe):
#     dataframe["callsign"] = dataframe["vehicle_type"].str[0].str.upper() + dataframe[
#         "callsign_group"
#     ].fillna(0).astype(int).astype(str)
#     dataframe["callsign"] = dataframe["callsign"].apply(
#         lambda x: re.sub(r"^(?<!C)C(?=\d{2}$)", "CC", x)
#     )

#     return dataframe


def calculate_time_difference(df, col1, col2, unit="minutes"):
    """
    Calculate the time difference between two datetime columns in a Pandas DataFrame.
    *AI PRODUCED FUNCTION - CHECKED FOR CORRECT FUNCTIONING*

    Args:
        df (pd.DataFrame): The DataFrame containing the datetime columns.
        col1 (str): Name of the first datetime column.
        col2 (str): Name of the second datetime column.
        unit (str): The unit for the time difference ('seconds', 'minutes', 'hours', 'days').

    Returns:
        pd.Series: A Pandas Series with the time differences in the specified unit.
    """
    # Convert columns to datetime format
    df[col1] = pd.to_datetime(df[col1], format="ISO8601")
    df[col2] = pd.to_datetime(df[col2], format="ISO8601")

    # Compute time difference
    time_diff = df[col2] - df[col1]

    # Convert to specified unit
    if unit == "seconds":
        return time_diff.dt.total_seconds()
    elif unit == "minutes":
        return time_diff.dt.total_seconds() / 60
    elif unit == "hours":
        return time_diff.dt.total_seconds() / 3600
    elif unit == "days":
        return time_diff.dt.total_seconds() / 86400
    else:
        raise ValueError(
            "Invalid unit. Choose from 'seconds', 'minutes', 'hours', or 'days'."
        )


def get_param(parameter, params_df):
    return params_df[params_df["parameter"] == parameter]["value"].values[0]


def fill_missing_values(df, column, value):
    """
    Replaces missing (NaN) values in a specified column with a given value.

    Args:
        df (pd.DataFrame): The DataFrame containing the column.
        column (str): The name of the column to process.
        value (any): The value to replace NaN values with.

    Returns:
        pd.DataFrame: The modified DataFrame with NaNs replaced.
    """
    df[column] = df[column].fillna(value)
    return df
****************************************

****************************************
air_ambulance_des_docs\.gitignore
****************************************
/.quarto/
/reference/

# Ignore any files that are automatically generated by the Quarto pre-render script
# This minimizes the risk of them silently going out of sync with the documents they are
# generated from.
code_of_conduct.qmd
licence.qmd
readme.qmd
STARS.qmd
STRESS_DES.qmd

_sidebar.yml
objects.json
****************************************

****************************************
air_ambulance_des_docs\citation_details.qmd
****************************************
# Citing this work

If you use or build on this work, please use the following citation:

```markdown
{{< include ../CITATION.cff >}}
```
****************************************

****************************************
air_ambulance_des_docs\code_of_conduct.qmd
****************************************
# Code of Conduct

## Our Pledge
We are committed to creating a welcoming and inclusive environment for everyone who participates in this project.
We pledge to make participation in our community a harassment-free experience for all, regardless of:

- Age
- Body size
- Disability
- Ethnicity
- Gender identity and expression
- Level of experience
- Nationality
- Personal appearance
- Race
- Religion
- Sexual identity and orientation

## Our Standards
Examples of behavior that contributes to a positive environment include:

- Being respectful and considerate in all interactions
- Offering constructive feedback
- Using inclusive language
- Focusing on what is best for the community
- Showing empathy toward other contributors

Examples of unacceptable behavior include:

- Harassment or discrimination of any kind
- Personal attacks, trolling, or insulting comments
- Public or private harassment
- Publishing others’ private information without consent
- Any conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities
Project maintainers are responsible for clarifying standards of acceptable behavior and are expected to take appropriate action in response to any unacceptable behavior.
They have the right and responsibility to remove, edit, or reject contributions that do not align with this Code of Conduct.

## Scope
This Code of Conduct applies both within project spaces (e.g., GitHub issues, pull requests, discussions) and in public spaces when someone is representing the project or its community.

## Enforcement
Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the maintainers via GitHub Issues or by direct contact.
All complaints will be reviewed and investigated promptly and fairly.

## Attribution
This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/), version 2.1.
****************************************

****************************************
air_ambulance_des_docs\index.qmd
****************************************
---
format:
    html:
        toc: true
        toc-expand: 3
---

# Air Ambulance Discrete Event Simulation

Welcome!
****************************************

****************************************
air_ambulance_des_docs\licence.qmd
****************************************
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.
****************************************

****************************************
air_ambulance_des_docs\objects.json
****************************************
{"project": "air_ambulance_des", "version": "0.0.9999", "count": 86, "items": [{"name": "air_ambulance_des.class_ambulance.Ambulance", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_ambulance.Ambulance.html#air_ambulance_des.class_ambulance.Ambulance", "dispname": "-"}, {"name": "air_ambulance_des.class_hems.HEMS.hems_resource_on_shift", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems.HEMS.html#air_ambulance_des.class_hems.HEMS.hems_resource_on_shift", "dispname": "-"}, {"name": "air_ambulance_des.class_hems.HEMS.unavailable_due_to_service", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems.HEMS.html#air_ambulance_des.class_hems.HEMS.unavailable_due_to_service", "dispname": "-"}, {"name": "air_ambulance_des.class_hems.HEMS", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_hems.HEMS.html#air_ambulance_des.class_hems.HEMS", "dispname": "-"}, {"name": "air_ambulance_des.class_patient.Patient", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_patient.Patient.html#air_ambulance_des.class_patient.Patient", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.ResourceAllocationReason", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_hems_availability.ResourceAllocationReason.html#air_ambulance_des.class_hems_availability.ResourceAllocationReason", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.add_hems", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.add_hems", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.allocate_regular_resource", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.allocate_regular_resource", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.allocate_resource", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.allocate_resource", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.current_store_status", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.current_store_status", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.daily_servicing_check", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.daily_servicing_check", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.do_ranges_overlap", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.do_ranges_overlap", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.find_next_service_date", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.find_next_service_date", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.is_during_school_holidays", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.is_during_school_holidays", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.is_other_resource_being_serviced", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.is_other_resource_being_serviced", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.populate_store", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.populate_store", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.preferred_regular_group_available", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.preferred_regular_group_available", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.preferred_resource_available", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.preferred_resource_available", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.prep_HEMS_resources", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.prep_HEMS_resources", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.return_resource", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.return_resource", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability.years_between", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability.years_between", "dispname": "-"}, {"name": "air_ambulance_des.class_hems_availability.HEMSAvailability", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_hems_availability.HEMSAvailability.html#air_ambulance_des.class_hems_availability.HEMSAvailability", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.activity_time_distributions", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.activity_time_distributions", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.ad_hoc_unavailability", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.ad_hoc_unavailability", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.age_distributions", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.age_distributions", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.callsign_group", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.callsign_group", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.callsign_group_by_ampds_card_and_hour_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.callsign_group_by_ampds_card_and_hour_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.callsign_group_by_ampds_card_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.callsign_group_by_ampds_card_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.enhanced_or_critical_care_by_ampds_card_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.enhanced_or_critical_care_by_ampds_card_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.getBestFit", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.getBestFit", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hems_result_by_callsign_group_and_vehicle_type_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hems_result_by_callsign_group_and_vehicle_type_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hems_results_by_patient_outcome_and_time_of_day_and_quarter_and_vehicle_type_and_callsign_group_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_care_cat_counts", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_care_cat_counts", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_median_time_of_activities_by_month_and_resource_type", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_median_time_of_activities_by_month_and_resource_type", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_resource_utilisation", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_resource_utilisation", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_all_calls", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_all_calls", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_by_callsign", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_by_callsign", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_by_day_of_week", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_by_day_of_week", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_by_hour_of_day", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.historical_monthly_totals_by_hour_of_day", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hour_by_ampds_card_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hour_by_ampds_card_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hourly_arrival_by_qtr_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.hourly_arrival_by_qtr_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.import_and_wrangle", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.import_and_wrangle", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.incidents_per_day", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.incidents_per_day", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.incidents_per_day_samples", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.incidents_per_day_samples", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.patient_outcome_by_care_category_and_quarter_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.patient_outcome_by_care_category_and_quarter_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.removeExistingResults", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.removeExistingResults", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.school_holidays", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.school_holidays", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.sex_by_ampds_card_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.sex_by_ampds_card_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.upper_allowable_time_bounds", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.upper_allowable_time_bounds", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.vehicle_type_by_quarter_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.vehicle_type_by_quarter_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils.vehicle_type_probs", "domain": "py", "role": "function", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils.vehicle_type_probs", "dispname": "-"}, {"name": "air_ambulance_des.distribution_fit_utils.DistributionFitUtils", "domain": "py", "role": "class", "priority": "1", "uri": "reference/distribution_fit_utils.DistributionFitUtils.html#air_ambulance_des.distribution_fit_utils.DistributionFitUtils", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.add_patient_result_row", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.add_patient_result_row", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.calls_per_hour", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.calls_per_hour", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.generate_calls", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.generate_calls", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.patient_journey", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.patient_journey", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.predetermine_call_arrival", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.predetermine_call_arrival", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.run", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.run", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.store_patient_results", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.store_patient_results", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.write_all_results", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.write_all_results", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS.write_run_results", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS.write_run_results", "dispname": "-"}, {"name": "air_ambulance_des.des_hems.DES_HEMS", "domain": "py", "role": "class", "priority": "1", "uri": "reference/des_hems.DES_HEMS.html#air_ambulance_des.des_hems.DES_HEMS", "dispname": "-"}, {"name": "air_ambulance_des.des_parallel_process.runSim", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_parallel_process.runSim.html#air_ambulance_des.des_parallel_process.runSim", "dispname": "-"}, {"name": "air_ambulance_des.des_parallel_process.parallelProcessJoblib", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_parallel_process.parallelProcessJoblib.html#air_ambulance_des.des_parallel_process.parallelProcessJoblib", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_inputs.SimulationInputs", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_simulation_inputs.SimulationInputs.html#air_ambulance_des.class_simulation_inputs.SimulationInputs", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.PLOT_SIMULATION_utilisation_summary", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.PLOT_SIMULATION_utilisation_summary", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.PLOT_hourly_call_counts", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.PLOT_hourly_call_counts", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.calculate_available_hours", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.calculate_available_hours", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.compute_average_calls", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.compute_average_calls", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.count_weekdays_in_month", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.count_weekdays_in_month", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.display_UNTATTENDED_calls_per_run", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.display_UNTATTENDED_calls_per_run", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.display_resource_use_exploration", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.display_resource_use_exploration", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.display_vehicle_utilisation_metric", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.display_vehicle_utilisation_metric", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.get_AVERAGE_UNATTENDED_calls_per_run", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.get_AVERAGE_UNATTENDED_calls_per_run", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.get_AVERAGE_calls_per_run", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.get_AVERAGE_calls_per_run", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.get_UNATTENDED_calls_per_run", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.get_UNATTENDED_calls_per_run", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.get_calls_per_run", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.get_calls_per_run", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.get_perc_unattended_string", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.get_perc_unattended_string", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.get_perc_unattended_string_normalised", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.get_perc_unattended_string_normalised", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.make_SIMULATION_utilisation_headline_figure", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.make_SIMULATION_utilisation_headline_figure", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.make_SIMULATION_utilisation_variation_plot", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.make_SIMULATION_utilisation_variation_plot", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults.make_job_count_df", "domain": "py", "role": "function", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults.make_job_count_df", "dispname": "-"}, {"name": "air_ambulance_des.class_simulation_trial_results.TrialResults", "domain": "py", "role": "class", "priority": "1", "uri": "reference/class_simulation_trial_results.TrialResults.html#air_ambulance_des.class_simulation_trial_results.TrialResults", "dispname": "-"}, {"name": "air_ambulance_des.des_parallel_process.write_run_params", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_parallel_process.write_run_params.html#air_ambulance_des.des_parallel_process.write_run_params", "dispname": "-"}, {"name": "air_ambulance_des.des_parallel_process.collateRunResults", "domain": "py", "role": "function", "priority": "1", "uri": "reference/des_parallel_process.collateRunResults.html#air_ambulance_des.des_parallel_process.collateRunResults", "dispname": "-"}]}
****************************************

****************************************
air_ambulance_des_docs\readme.qmd
****************************************
---
format:
    html:
        toc: true
        toc-expand: 3
---

# Devon Air Ambulance Discrete Event Simulation

This repository contains all model and web app code for the Devon Air Ambulance Simulation modelling project.


## DES model logic

The model creates patient episodes and associated outcomes based on the following sequence:

1.  Obtain AMPDS card category based on hour of day
2.  Choose a callsign based on activation criteria, which helicopter (if any) is available, whether helicopter can currently fly (servicing or weather impacts),
3.  Based on callsign, determine the HEMS result (Stand Down Before Mobile, Stand Down En Route, Landed but no patient contact, Patient Treated (Not Conveyed), Patient Conveyed)
4.  Based on the HEMS result determine the patient outcome (Airlifted, Conveyed by land with DAA, Conveyed by land without DAA, Deceased, Unknown)

A full breakdown of the model logic can be found in **reference/daa_des_model_logic**

![](reference/daa_des_model_logic.png)

## Issue Tracking and Roadmap

Project issues are tracked using the Github issues system and can be accessed [here](https://github.com/RichardPilbery/DAA_DES/issues).

Project milestones can be found [here](https://github.com/RichardPilbery/DAA_DES/milestones?direction=desc&sort=title&state=open).

These project milestones currently supersede information contained in the `roadmap.md` file.

Tickets actively being worked on by contributors can be found on the [project board](https://github.com/users/RichardPilbery/projects/1).


## API Documentation

The function and class documentation can be accessed at the following link: [https://richardpilbery.github.io/DAA_DES/](https://richardpilbery.github.io/DAA_DES/)

This documentation is automatically generated using [pdoc](https://pdoc.dev/) and will be regenerated when the code is updated on the main branch on Github. If you are a contributor and wish to see

It is also available as part of the enhanced documentation offering available at [bergam0t.quarto.pub/air-ambulance-simulation/](https://bergam0t.quarto.pub/air-ambulance-simulation/).

## Data output items

The model generates a CSV file containing raw data, which can then be wrangled and presented separately, either following a model run(s) or at any time after the model has been run.

**All** runs of the model get added to the **all_results.csv** file.

The last run - whether run by the functions in can be found in the **run_results.csv** file.

The table below identifies the column headers and provides a brief description

| Column name               | Description |
| --------------------------| ------------------------------------------------------------------------------ |
| P_ID                      | Patient ID    |
| run_number                | Run number in cases where the model is run repeatedly (e.g. 100x) to enable calculations of confidence intervals etc.|
| time_type                 | Category that elapsed time represents e.g. 'call start', 'on scene', 'leave scene', 'arrive hospital', 'handover', 'time clear'|
| timestamp                 | Elapsed time in seconds since model started running |
| day                       | Day of the week as a string ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun') |
| hour                      | Hour of call as integer between 0–23 |
| weekday                   | String identifying whether the call started on a weekday or weekend |
| month                     | Integer representing the month of the year (1–12) |
| qtr                       | Integer representing the yearly quarter of the call (1–4) |
| callsign                  | String representing callsign of resource (either HEMS or Ambulance service) |
| triage_code               | Call triage outcome represented as one of the AMPDS 'golden' codes (or OTHER) |
| age                       | Integer representing patient age in years |
| sex                       | String representing patient sex ('male', 'female') |
| time_to_first_response    | Integer representing time in minutes to first response (ambulance service or HEMS) |
| time_to_cc                | Integer representing time in minutes to first HEMS response |
| cc_conveyed               | Integer indicating whether HEMS conveyed patient (1 or 0) |
| cc_flown                  | Integer indicating whether HEMS conveyed the patient by air (1 or 0) |
| cc_travelled_with         | Integer indicating whether patient was conveyed by ambulance but HEMS personnel travelled with the patient to hospital (1 or 0) |
| hems                      | Integer indicating whether HEMS attended the incident (1 or 0) |
| cc_desk                   | Integer indicating whether HEMS activation was due to critical care desk dispatch |
| dispatcher_intevention    | Integer indicating whether the ambulance service dispatcher activated HEMS for a called which did not meet auto-dispatch criteria (1 or 0) |

## Environment Setup

### Installing the project locally

Clone this repository to your machine.

The core environment is provided in the `requirements.txt` file in the root folder of the repository.

This has been used in conjunction with Python 3.11.9.

Using your preferred virtual environment manager, install the requirements specified in `requirements.txt`.

When you have completed this, you will need to undertake one additional step; installing the air ambulance simulation code.

This is achieved by running `pip install -e .`

> [!IMPORTANT]
> Be sure to run this command from the root of the repository (the folder containing pyproject.toml and requirements.txt).

This installs the air ambulance simulation code as an *editable package*. This means that any changes to the classes and functions in the folder `air_ambulance_des` will automatically be recognised in the web app or anywhere else you call the code from while using this environment, without needing to reinstall the package.

This additional step is necessary because the simulation code is organised as a Python package (air_ambulance_des) rather than a set of standalone scripts. This supports long term reusability, testing and documentation workflows.

### Remote hosting

A devcontainer.json file has also been provided in the .devcontainer folder; this allows VSCode to access or create a container with the appropriate versions of Python and all requirements, including automatically installing the air ambulance simulation code as an editable package (so you will **not** need to set up a virtual environment or run `pip install -e .`).

Alternatively, you could open up the development environment in Github Codespaces, which will achieve the same purpose without you having to clone the repository and set up an environment on your local machine. To access this, ensure you are logged into GitHub, then look for the green 'Code' button at the top of this repository. Click on this and select 'Create codespace on main'.

![](readme_assets/2025-10-22-16-12-57.png)

You can find out more about codespaces at [github.com/features/codespaces](https://github.com/features/codespaces).

## Web App

Assuming you have installed the environment as above, either locally or in Github Codespaces, the web app can be run using the command

`streamlit run app/app.py`

The app will attempt to use multiple cores of the user's computer to undertake several runs of the model simultaneously.

The web app can also be accessed without needing to install anything at [daa-des-demo.streamlit.app/](https://daa-des-demo.streamlit.app/). However, not that this will run more slowly and will complete fewer runs by default due to limitations of the hosting platform (which prevents multi-core running); it is recommended to download the app and run it locally if you are not just looking to get an idea of the app's capabilities.

### Quarto

If you wish to be able to download the output from the web app as a Quarto file, you will need to also install Quarto.

Quarto can be downloaded at [https://quarto.org/docs/get-started/](https://quarto.org/docs/get-started/).

- It is recommended that, when asked by the installer, you add Quarto to your PATH variable.
- It is important to note that while a [Python package for quarto exists](https://pypi.org/project/quarto/), this is not the full Quarto command line utility, which will need to be installed separately.

Note that if you are using the .devcontainer, Quarto will be available within the container without undertaking any additional steps.

### Hosted version of web app

The Streamlit app is also available at the following link: [https://daa-des-demo.streamlit.app/](https://daa-des-demo.streamlit.app/)

Note that for permissions reasons, the hosted version of the app runs off a fork ((https://github.com/Bergam0t/DAA_DES)[https://github.com/Bergam0t/DAA_DES]) instead of this main repository.
Therefore, the hosted version may not always be the most up to date version of the app.

You can determine if the fork is currently up to date with this repository by navigating to the fork using the link above and looking at the following message.

![](readme_assets/2025-03-07-16-40-19.png)

Note that due to limitations of the hosting platform, each run of the model is undertaken sequentially, not in parallel, so execution times are much longer.

## Estimated Run Times

As of March 2025, the app has been tested on a pc with the following specs

- OS: Windows 11 (Version 10.0.22631 Build 22631)
- Processor: 12th Gen Intel(R) Core(TM) i7-12700H, 2300 Mhz, 14 Core(s), 20 Logical Processor(s)
- Installed Physical Memory (RAM): 32.0 GB

When executing the model through the streamlit interface, running locally so parallel processing can be used, the following run times were observed.

**2 years simulated time, default demand, default parameters (2 helicopters, 2 backup cars, 1 additional car)**

*including generation of all plots via web interface*

- **12 runs, 730 days:** ~2 minutes (~90 seconds for initial model running)
- **100 runs, 730 days:** ~8 minutes (~4 minutes for initial model running)


## Details for Contributors

Please refer to our code of conduct here: [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)

### Repository Structure

#### Root

#### actual_data

This folder contains key input data that is called upon by the model, pertaining to areas including rotas and servicing.

#### air_ambulance_des

The model logic is structured as a package in this folder.

**class_ambulance.py**:
This file defines the core resource class.

**class_hems_availability.py**:
This file contains classes controlling the logic around the allocation of resources to jobs.

**class_hems.py**:
This file defines the HEMS class as a child of the core resource class defined in class_ambulance.

**class_historic_results.py**
This file defines a class that allows for generation of metrics and plots relating to the historical data. This allows for comparison of the model with historic data in a number of ways.

**class_patient.py**:
This file defines the core patient class. The model uses a single patient class.

**class_simulation_inputs.py**:
This manages the data files that are handed to the simulation, ensuring they are made available to the historical data and simulation data classes to facilitate calculations.

**class_simulation_trial_results.py**:
This file defines a class that allows for generation of metrics and plots relating to the data generated by running the simulation.

**des_hems.py**:
This file is the key file controlling call generation and call journeys, and is where most of the logic relating to the patient journey is held.

**des_parallel_process.py**:
This file contains functions to perform a single run of the model, or multiple runs using parallel processing. It also handles the collation of results files and associated filespace cleanup.

*There are also several supporting files.*

**distribution_fit_utils.py**: This file contains functions and classes for fitting distributions from raw data. Note that instructions are currently not given for the required data format for using this on your own data. However, the distributions generated by these functions for the site the model is being developed for can be found in the *distribution_data* folder, allowing other parts of the app to be run in the absence of raw job-level data.

**utils.py**:
This file contains various supporting functions used across other files.

#### air_ambulance_des_docs

This folder contains the

#### app

This folder contains code relating to the generation of the interactive web app interface.

#### distribution_data

This folder contains the distributions that are generated from the call-level data and the module **air_ambulance_des.distribution_fit_utils**.

#### docs

This folder contains the pdoc documentation. Note that this is usually generated as part of an automated GitHub workflow, so this folder will usually be empty apart from a `.gitignore` file.

#### docs_quarto

This folder contains the rendered quarto documentation.

#### historical_data

This folder contains aggregated historical data.

This data is currently used to visualise the historical range of data and compare it with the simulated results.

The data will also be used as part of the pytest testing suite to ensure models run with the parameters in use at the time these simulations were run successfully mimic the observed real-world patterns.

#### readme_assets

This folder contains images used within this readme.

#### reference

This folder contains

If editing the files, you should use the .drawio versions, which can be opened with the free tool diagrams.net.

It is recommended that you edit and export the diagrams in light mode, and export using a 50 pixel border, to ensure a consistent appearance is maintained.

#### tests

This folder contains files that will be picked up by the pytest framework. They are planned to contain various tests including validation/verification tests, and unit tests.

To run all tests, use the command `pytest` in a terminal, running from the root folder of the project.

To run a subset of tests, you can choose subsets of tests. For example, to run all resource-related tests based on their tag (known as a marker), run `pytest -m resources`. More options can be seen [here](https://docs.pytest.org/en/stable/example/markers.html).

Markers available include:

```
    warmup: Tests relating to the warm-up period
    resources: Tests relating to the correct logic around resource allocation and unavailability
    calls: Tests relating to the number of calls/jobs generated
    jobdurations: Tests relating to the duration of jobs
    quick: Tests to quickly check the sim is functioning at a basic level
    reproducibility: Tests to check that random seeds behave as expected
    performance: Tests to check that basic metrics behave as expected when core parameters change
```

See `pytest.ini` for the most up-to-date list of markers.

You can also use `pytest -k some_string` to match tests with a name containing a particular string (replacing `some_string` in the command with your own search term).

Alternatively, see the [pytest documentation](https://docs.pytest.org/en/stable/how-to/usage.html) for additional ways to manage groups of tests.

To generate an interactive html report, run `pytest --html=report.html`

To generate a code coverage report, run `pytest --cov=. --cov-report=html`.

### Regenerating documentation

#### pdoc (basic class/function documentation)

In general, you will not to generate the pdoc documentation as this is controlled by a Github actions workflow (.github/docs.yml).

However, you may wish to do so for testing purposes or to preview the documentation without pushing changes - though note that your rendered/previewed version will be ignored by GitHub.

To regenerate pdoc documentation, ensure you are in the root of the repo (folder 'DAA_DES')

Then run `pdoc air_ambulance_des -o docs/`

This will output the pdoc documentation to the `docs/` folder.

To preview the documentation, navigate to this folder and open the file `index.html`.

The pdoc documentation is hosted on GitHub pages at [richardpilbery.github.io/DAA_DES/](https://richardpilbery.github.io/DAA_DES/)

#### quarto/quartodoc (basic class/function documentation + additional enhanced documentation)

To regenerate the quartodoc/quarto documentation, first make sure you have the quarto CLI installed and the quartodoc package installed in your environment (which should happen automatically if you have set up the environment from requirements.txt)

First, move into the folder `air_ambulance_des_docs`. If you are in the root of the repository, you can do this by running `cd air_ambulance_des_docs`.

Next, run `quartodoc build`. This will rebuild the sidebar and the autogenerated class and function documentation.

Then, run `quarto render`. This will fully render both the autogenerated quartodoc pages and all additional pages that are included.

If you wish to preview the finished documentation, run `quarto preview`. This will open a preview server. Depending on your IDE settings, this may open in a panel in the IDE, or it may open as a tab in your default web browser.

The quarto/quartodoc documentation is hosted on quartopub at [bergam0t.quarto.pub/air-ambulance-simulation/](https://bergam0t.quarto.pub/air-ambulance-simulation/).
****************************************
